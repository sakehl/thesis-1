%
% analysis, example programs
%

\chapter{Results}
\label{ch:results}
% \epigraph{Dionysus [is] the Master of Illusions, who could make a vine grow
% out grow out of a ship's plank, and in general enable his votaries to see the
% world as the world's not.}%
% {\textsc{---e.\ r.\ dodds}\\\textit{The Greeks and the Irrational}}

% \epigraph{If the immutable appears recast, then you yourself have been
% transformed.}%
% {\textsc{---r.\ scott bakker}\\\textit{The Judging Eye}}

\epigraph{Logic merely enables one to be wrong with authority}%
{\textsc{---the doctor}} %\\\textit{The Wheel in Space}}

% \begin{itemize}
%     \item performance \& analysis
%     \begin{itemize}
%       \item What is an optimisation? Describe the metrics considered.
%         \begin{itemize}
%           \item wall-clock time, parallel speedup??
%           \item instruction count
%           \item memory traffic (load/store/coalescing)
%           \item memory size (heap, register count, shared memory)
%           \item program size (number of parallel steps)
%           \item code size (generated binary, compilation time c.f. code complexity)
%         \end{itemize}
%     \end{itemize}
% 
%     \item expressiveness / example programs
%     \item difficulties
%     \begin{itemize}
%         \item code generation
%         \item unintentional nesting
%     \end{itemize}
% \end{itemize}

The previous chapters discussed the design, implementation and optimisation of
the Accelerate language and CUDA backend. This chapter analyses the performance
of the implementation. Benchmarks were conducted on a single Tesla T10 processor
(compute capability 1.3, 30 multiprocessors = 240 cores at 1.3GHz, 4GB RAM)
backed by two quad core Xeon E5405 CPUs (64-bit, 2GHz, 8GB RAM) running
GNU/Linux (Ubuntu 12.04 LTS, CUDA 5.0).


\section{Runtime Overheads}

Runtime program optimisation, code generation, kernel loading, data transfer,
and so on can contribute significant overhead to short lived GPU computations.
Accelerate mitigates these overheads via caching and memoisation. For example,
the first time a particular expression is executed it is compiled to CUDA code,
which is reused for subsequent invocations. This section analyses those
overheads, so that they can be factored out later when we discuss the effects of
the program optimisations on kernel runtimes of the benchmark programs.


\subsection{Program Optimisation}

% tk: performance and benchmarks for the fusion pass
%   - actual runtime
%   - tick counters for the final version and old one with extend/cunctation
%     separation

\subsection{Data Transfer}

% tk: some graphs of memory transfer times or bandwith vs. array size
% tk: importance of using weak pointers, so data transfers ``escape'' the black
%     box of evaluating the AST --> hashcat.
% tk: importance of the nursery for reducing calls to malloc/free

\subsection{Code Generation \& Compilation}

% tk: measure \& tabulate some code generation / compilation times
% tk: persistent cache, perhaps just a mention?


\section{Dot product}

Dot product, or scalar product, is an algebraic operation that takes two
equal-length sequences of numbers and returns a single number by computing the
sum of the products of the corresponding entries in the two sequences. Dot
product can be defined in Accelerate using the code below, which was also seen
in section~\ref{sec:producer_consumer_fusion}.
%
\begin{lstlisting}[style=haskell
    ,label=lst:dotp
    ,caption={Vector dot-product in Accelerate}]
dotp :: Acc (Vector Float) -> Acc (Vector Float) -> Acc (Scalar Float)
dotp xs ys = A.fold (+) 0                       -- sum result of\ldots
           $ A.zipWith (*) xs ys                -- \ldots element-wise multiplying inputs
\end{lstlisting}
%
Figure~\ref{fig:dotp} show the result of running this code, compared to several
other sequential and parallel implementations of the same operation. Without
fusion the intermediate array produced by \code{zipWith} is created in GPU
memory before being read back by \code{fold}. As this is a simple memory bound
benchmark, fusion roughly doubles the performance of the unfused Accelerate
program.

The \texttt{Data.Vector} baseline is sequential code produced by
\index{fusion!stream}stream fusion~\cite{Coutts:2007kp}, running on the host
CPU. The \texttt{Repa} version runs in parallel on all eight cores of the host
CPU, using the fusion method of \index{fusion!delayed arrays}delayed
arrays~\cite{Keller:2010er}. The \texttt{NDP2GPU}~\cite{Bergstrom:2012bi}
version compiles NESL code~\cite{Blelloch:1995ut} down to CUDA. The performance
of this version suffers because the \texttt{NDP2GPU} compiler uses the legacy
NESL compiler for the front-end, which introduces redundant administrative
operations that are not strictly needed when evaluating a dot product.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/dotp/dotp}
    \end{center}
    \caption[Vector dot product kernel benchmarks]{Kernel runtimes for vector
        dot product, in Accelerate with and without optimisations, compared to
        other parallel GPU and CPU implementations. Note the log-log scale.}
    \label{fig:dotp}
\end{figure}

While the fused Accelerate version is about $30\times$ faster than the
sequential version executed on the CPU, it is still approximately $20\%$ slower
than the hand-written CUDA version. Dot product is a computationally simple
task, so any additional overheads in the implementation will be significant.
We first discuss how reductions are implemented in CUDA, as a basis for
understanding these sources of overhead.


\subsection{Parallel Reductions in CUDA}
\label{sec:parallel_reduction}

\note{tk: probably move this to the implementation chapter}

Figure~\ref{fig:tree_reduction} illustrates the basic strategy of a tree
reduction, a common and important data-parallel primitive. Within each thread
block a tree-based approach is used to reduce elements to a single value, and
multiple thread blocks each process a portion of the array in parallel. There is
no global synchronisation primitive in CUDA, because (a) it would be expensive
to build in hardware for large numbers of multiprocessors, and (b) would be
difficult for programmers to use without introducing sources of deadlock.
Instead, kernel launches serve as a global synchronisation point, so thread
blocks instead communicate their partial results by committing them to memory,
and the kernel is launched recursively until the final reduction is computed.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{images/sec-6/tree-reduction}
    \end{center}
    \caption[A parallel tree reduction]{Illustration of a tree reduction,
        performed in two steps. In the first step 8 thread blocks in parallel
        reduce 8 elements to a single value. The thread blocks synchronise by
        writing their result to memory, and the kernel is called recursively
        until the final result is computed.}
    \label{fig:tree_reduction}
\end{figure}

For a fused dot product kernel, only the first line of reductions at level zero
perform the element wise multiplication. All subsequent reductions in that
kernel, as well as the recursive steps at level one and beyond, are pure
reductions. Thus, the fused operation requires compilation of two separate
kernels. % We focus on the level zero kernel, which is more interesting.

\subsubsection{Parallel Reduction Complexity}
\label{sec:parallel_complexity}

If each thread combines two elements at a time, then a vector of length $N$ will
be reduced in $\mathcal{O}\left( \log N \right)$ parallel steps. Each step $S$
does $\sfrac{N}{S^2}$ independent operations, so the \emph{step
complexity}\index{complexity!step} of the algorithm is:
\[
\mathcal{O}\left( \log N \right)
\]
For $N=2^{D}$, the algorithm thus performs $\sum_{S=1}^{D}2^{D-S} = N - 1$
operations. This means that the \emph{work complexity}\index{complexity!work} of
the algorithm is:
\[
\mathcal{O}\left( N \right)
\]
and so does not perform more work than a sequential algorithm. For $P$ threads
running physically in parallel on $P$ processors, the \emph{time
complexity}\index{complexity!time} is $\mathcal{O}\left( \sfrac{N}{P} + \log N
\right)$. In a thread block $N = P$, so the time complexity is:
\[
\mathcal{O}\left( \log N \right)
\]
Compare this to a sequential reduction, which has a time complexity of
$\mathcal{O}\left( N \right)$.

\subsubsection{Algorithm Cascading}
\label{sec:algorithm_cascading}

The \emph{cost} of a parallel algorithm is the number of processors $\times$
time complexity. This implies that the cost of the algorithm is
$\mathcal{O}\left( N \log N \right)$, which is \emph{not} cost efficient.

Brent's theorem~\cite{Chatterjee:2009vh} suggests that instead of each thread
summing two elements, \emph{algorithm cascading} can be used to combine a
sequential and parallel reduction. Each thread does $\mathcal{O}\left( \log N
\right)$ sequential work, which reduces the cost of the algorithm to
$\mathcal{O}\left( \sfrac{N}{\log N} \log N \right)$, or rather:
\[
\mathcal{O}\left( N \right)
\]
while keeping the work complexity $\mathcal{O}\left( N \right)$ and step
complexity $\mathcal{O}\left( \log N \right)$.

This suggests, for example, that a block of 256 threads should sum a total of
2048 elements. In practice it is beneficial to do even more sequential work per
thread, since this reduces the number of levels in the recursive tree reduction
and provides better latency hiding.


\subsection{Fused Dot Product}

The fused version of dot product embeds a function of type \code{(sh -> a)} into
the reduction, represented here as the second argument to the constructor
\code{Delayed}. This scalar function does the work of element wise multiplying
the two input vectors:
%
\begin{lstlisting}[style=haskell]
let a0 = use (Array ...) in
let a1 = use (Array ...) in
fold (\x0 x1 -> x0 + x1) 0.0
   (Delayed (intersect (shape a0) (shape a1))   -- extent of the input array
            (\x0 -> (a0!x0) * (a1!x0)))         -- function to generate a value at each index
\end{lstlisting}
%
To see why this version performs slightly slower than the CUBLAS version, we
must inspect the generated code.

\subsubsection{Duplicate loop counters}

The fused dot product operation will only perform the element wise
multiplication of the two input arrays in the first step of the tree reduction
(\S\ref{sec:parallel_reduction}). This occurs in the phase of the cascaded
algorithm when individual threads sequentially sum multiple elements
(\S\ref{sec:algorithm_cascading}). After embedding the fused producer, the
following CUDA code is generated for the inner loop of this step:
%
% pookie is the bestest
% bubbaboo ish silly
% kekekee
% i can't put smiley faces in cause weird things happen
% hehe^{this is my thesis
% please like it
% i worked really hard
% and writed a lots
% }<++>
%
%
\begin{lstlisting}[style=cuda
    ,firstnumber=18
    ,label=lst:dotp_cuda
    ,caption={Generated CUDA code for the first step of fused dot product}]
for (ix += gridSize; ix < shapeSize; ix += gridSize) {
    const Int64 v2 = ix;
    const int v3 = toIndex(shIn0, shape(v2));
    const int v4 = toIndex(shIn1, shape(v2));

    x0 = arrIn0_a0[v3] * arrIn1_a0[v4];
    y0 = x0 + y0;
}
\end{lstlisting}
%
We have four loop counters: \code{ix}, \code{v2}, \code{v3} and \code{v4} ---
two for the source arrays and two to convert between the multidimensional and
linear representations. These counters contain the same value and are
incremented in lockstep. In addition to the superfluous arithmetic, the
duplication of counters unnecessarily increases register pressure.

\marginnote{this was unexpected}
The corresponding section of PTX~\cite{NVIDIA:2012vj} code is for this loop is
shown below. To retrieve the data from the first input array, the input array
pointer is retrieved (line~\ref{lst:dotp_ptx_ldparam}), the offset stored in
register \code{rd16} added to it (line~\ref{lst:dotp_ptx_add}), and then the
value read from global memory at this address
(line~\ref{lst:dotp_ptx_ldglobal}). Happily, we note that retrieving the second
input element \emph{also} uses the offset stored in register \code{rd16}
(line~\ref{lst:dotp_ptx_ldglobal2}).
%
\begin{lstlisting}[style=ptx
    ,firstnumber=103
    ,label=lst:dotp_ptx
    ,caption={Corresponding PTX code for the first step of fused dot product}]
 //  18          for (ix += gridSize; ix < shapeSize; ix += gridSize) {
        cvt.u32.u16     %r10, %nctaid.x;
        mul.lo.u32      %r11, %r10, %r1;
        add.s32         %r12, %r11, %r7;
        mov.s32         %r13, %r12;
        setp.le.s32     %p2, %r8, %r12;
        @%p2 bra        $Lt_0_13058;
        cvt.s64.s32     %rd12, %r12;
        cvt.s64.u32     %rd13, %r11;
$Lt_0_14082:
 //<loop> Loop body line 18, nesting depth: 1, estimated iterations: unknown
        .loc    16      24      0
 //  20              const int v3 = toIndex(shIn0, shape(v2));
 //  21              const int v4 = toIndex(shIn1, shape(v2));
 //  22
 //  23              x0 = arrIn0\_a0[v3] * arrIn1\_a0[v4];
 //  24              y0 = x0 + y0;
        cvt.s32.s64     %rd14, %rd12;                           (@* \label{lst:dotp_ptx_cvt1} *@)
        cvt.s32.s64     %r14, %rd14;
        cvt.s64.s32     %rd15, %r14;                            (@* \label{lst:dotp_ptx_cvt3} *@)
        mul.wide.s32    %rd16, %r14, 4;
        .loc    16      17      0
        ld.param.u64    %rd9, [__cudaparm_foldAll_arrIn0_a0];   (@* \label{lst:dotp_ptx_ldparam} *@)
        .loc    16      24      0
        add.u64         %rd17, %rd16, %rd9;                     (@* \label{lst:dotp_ptx_add} *@)
        ld.global.f32   %f4, [%rd17+0];                         (@* \label{lst:dotp_ptx_ldglobal} *@)
        .loc    16      17      0
        ld.param.u64    %rd8, [__cudaparm_foldAll_arrIn1_a0];
        .loc    16      24      0
        add.u64         %rd18, %rd16, %rd8;
        ld.global.f32   %f5, [%rd18+0];                         (@* \label{lst:dotp_ptx_ldglobal2} *@)
        mad.f32         %f3, %f4, %f5, %f3;
        add.s32         %r13, %r13, %r11;
        add.s64         %rd12, %rd12, %rd13;
        setp.gt.s32     %p3, %r8, %r13;
        @%p3 bra        $Lt_0_14082;
        bra.uni         $Lt_0_13058;
$Lt_0_13314:
        mov.f32         %f3, %f6;
$Lt_0_13058:
        .loc    16      27      0
 //  25          }
\end{lstlisting}

In this case the CUDA compiler was able to coalesce our four counters into a
single counter. This is because our definition of \code{toIndex} for
one-dimensional indices does \emph{not} do bounds checking:
%
\begin{lstlisting}[style=cuda]
template <>
static __inline__ __device__ Ix toIndex(const DIM1 sh, const DIM1 ix)
{
    return ix;
}
\end{lstlisting}
%
Since we do not check that the current index \code{ix} is within bounds of the
array shape \code{sh}, the compiler can see that the definitions of \code{v3}
and \code{v4} in Listing~\ref{lst:dotp_cuda} are identical. Similarly
one-dimensional indices are just integers, and so the one-dimensional instance
of \code{shape} is the identity function. For higher dimensional shapes,
however, this is not the case and \code{toIndex} and \code{shape} do real work
to their arguments. As the shape \code{shIn0} and \code{shIn1} are inputs
arguments to the kernel, the compiler will not be able to determine they are
equivalent and so the counters would not be combined.

Why don't we perform bounds checks in \code{toIndex}? Implementing exceptions in
a massively parallel architecture is difficult, and support for throwing
exceptions from kernel functions was only recently added for devices of compute
capability 2.0 and later~\cite{NVIDIA:2012wf}. % [\SB.15]
It would be a straightforward matter of adding runtime bounds checks for
supported devices, however.


\subsubsection{64-bit Arithmetic}

CUDA devices are at their core 32-bit processors and thus are optimised for
32-bit arithmetic. For example, our Tesla GPU with compute capability 1.3 has a
throughput of eight 32-bit floating-point add, multiply, or multiply-add
operations per clock cycle per multiprocessor, but only a single operation per
cycle of the 64-bit equivalents of these~\cite{NVIDIA:2012wf}. % [\S5.4.1]

However, the host architecture that the Haskell program executes on is likely to
be a 64-bit processor. This means that an \code{Int} value from Haskell ---
which may appear marshalled as array data or shape values --- will generate code
to manipulate 64-bit integers, as well as conversions between 32- and 64-bit
types. Lines~\ref{lst:dotp_ptx_cvt1}--\ref{lst:dotp_ptx_cvt3} of
Listing~\ref{lst:dotp_ptx} contain such type conversions, which on our compute
capability 1.3 device have a throughput of only a single operation per cycle per
multiprocessor. These conversions would not be necessary if we were not
cross-compiling from a 64-bit Haskell host, or if the programmer working
directly with CUDA only used the \code{int} type, which would be interpreted by
the CUDA compiler as a 32-bit wide integer.


\subsubsection{Non-neutral starting elements}

For convenience, and to avoid possibly unexpected behaviour, Accelerate's
\code{fold*} family of functions do not require the combination function and
starting element to form a moniod. We still require the function to be
associative, so that the reduction can be implemented efficiently in parallel as
a tree-reduction, but the initial element does not need to be a \emph{neutral}
element.\marginnote{semigroup?} For example, \code{fold (+) 10} is valid in
Accelerate and evaluates to the correct answer.

However, this means that evaluation of the reduction is more complex. In
particular, we can not simply initialise all threads to the initial value.
In the first phase of the reduction, threads must be sure to initialise
their local sum \code{y0} with values from the input array before beginning the
sequential loop show in Listing~\ref{lst:dotp_cuda}:
%
% tk: hacks in comment style below: check if changing the basic language style
\begin{lstlisting}[style=cuda,firstnumber=12]
if (ix < shapeSize) {
    const Int64 v2 = ix;
    const int v3 = toIndex(shIn0, shape(v2));
    const int v4 = toIndex(shIn1, shape(v2));

    y0 = arrIn0_a0[v3] * arrIn1_a0[v4];
    for (ix += gridSize; ix < shapeSize; ix += gridSize) {
\end{lstlisting}

Similarly, in the second parallel tree reduction phase threads may only read
values from shared memory that were properly initialised. This requires
additional bounds checks at every step:
%
\begin{lstlisting}[style=cuda,firstnumber=27]
sdata0[threadIdx.x] = y0;
__syncthreads();
ix = min(shapeSize - blockIdx.x * blockDim.x, blockDim.x);
if (threadIdx.x + 512 < ix) {
    x0 = sdata0[threadIdx.x + 512];
    y0 = y0 + x0;
    sdata0[threadIdx.x] = y0;
}
__syncthreads();
if (threadIdx.x + 256 < ix) {
    x0 = sdata0[threadIdx.x + 256];
    y0 = y0 + x0;
    sdata0[threadIdx.x] = y0;
}
__syncthreads();
// etc...
\end{lstlisting}

These requirements increase overhead from ancillary instructions that are not
loads, stores, or arithmetic for the core computation. A summation reduction has
low arithmetic intensity to begin with, and is the limiting factor in the
performance of this kernel, so additional bounds checks further reduce
performance.

\subsubsection{Kernel Specialisation}

To further reduce instruction overhead, it is possible to completely unroll the
reduction by specialising the kernel for a specific block size. In standard CUDA
this can be achieved through the use of C++ templates. Branches referring to the
template parameter will be evaluated at compile time, resulting in a very
efficient inner loop.
%
\begin{lstlisting}[style=cuda]
template <unsigned int blockSize> __global__ void reduce(...) {
    ...
    if (blockSize > 512) {
    }
    if (blockSize > 256) {
    }
    ...
\end{lstlisting}

This technique has shown to produce significant gains in
practice~\cite{Harris:2007te}, although requires compiling a separate kernel for
each thread block size we wish to specialise for. Accelerate can achieve this
kind of kernel specialisation because the CUDA code is generated at program
runtime, but this extra compilation adds significant additional overhead. Since
compiled kernels are cached and reused (\S tk), if we know that the reduction
will be computed many times the extra compilation overhead can be amortised by
the improved kernel performance, and may thus be worthwhile. Such
specialisations are left for future work.

