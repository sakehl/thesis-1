%
% conclusions, related work, future work
%
%
% aim:
%   implement programming language targeting parallel hardware / GPUs
%
% why:
%   higher level of abstraction, still competitive with hand-written / lower
%   level programs.
%
% how:
%   (a) purely functional language of parallel array operations
%   (b) runtime code generationn
%   (c) runtime system for efficiently managing stuff
%   (d) sharing and fusion optimisations
%
% result:
%   * results show that we have achieved this for a range of benchmark programs.
%     significant future work will be the support expressing nested data
%     parallel programs.
%
%   * Some things remain difficult/impossible to express (general recursion,
%     nested parallelism) or are less efficient..?
%
% last: overall we can see that i should get a floopy hat


\chapter{Conclusion}
\label{ch:conclusion}

\epigraph{A new scientific truth does not triumph by convincing its opponents
and making them see the light, but rather because its opponents eventually die,
and a new generation grows up that is familiar with it.}%
{\textsc{---max planck}}

% \epigraph{The best way to predict the future is to implement it.}
% {\textsc{---david heinemeier hansson}}

% Purely functional programming is a good model for programming massively parallel
% SIMD processors such as graphics processing units. Indeed, all current GPU
% programming models, including CUDA, an imperative C-like programming language,
% implicitly emphasise a purely functional style for writing kernels, where global
% effects must be tightly controlled by the user. Indeed, functional programming
% may be \emph{the} main virtue of GPU programming, even if this fact is not
% widely recognised.

% Bubbabear worked thuuuuuuuuper hard to write the thesises. Pwease give him his
% doctor hat now pweaaaaaaaaaaaaaaaase~<3

The present dissertation argues that purely functional embedded languages
represent a good programming model for massively parallel SIMD hardware. Indeed,
all current GPU programming models, including CUDA --- the standard, imperative,
C-like programming language for NVIDIA GPUs --- implicitly emphasise a purely
functional style of writing kernels, where global side-effects must be tightly
controlled by the user.

To this end, this thesis has developed a purely functional language, compiler,
and runtime system for executing flat data-parallel array computations targeting
graphics processing units. When implementing such an embedded language, the
na\"ive compilation of such programs quickly leads to both code explosion and
excessive intermediate data structures. The resulting slowdown is not acceptable
on target hardware that is usually chosen to achieve high performance. This
thesis demonstrates that our embedding in Haskell, combined with the program
optimisations and runtime system that I have implemented in this thesis, are a
successful approach to implementing an expressive array programming language
targeting bulk-parallel SIMD hardware. Moreover, I demonstrate that the
Accelerate language and CUDA backend that I developed offers a higher level of
abstraction while often approaching the performance of traditional low-level
methods for programming GPUs.

Chapter~\ref{ch:accelerate} introduces Accelerate, an embedded language of
parameterised, collective array operations. Accelerate is our approach to
reducing the complexity of programming massively parallel multicore processors
such as GPUs. The choice of language operations is informed by the scan-vector
model, which has been shown to be suitable for a wide range of algorithms and
can be efficiently implemented on the GPU\@. We extend this model with
parameterised, rank-polymorphic operations over multidimensional arrays, making
Accelerate more expressive than previous high-level GPU programming language
proposals.

Chapter~\ref{ch:implementation} details how I implemented the Accelerate backend
which targets programmable graphics processing units. While the current
implementation targets CUDA, the same approach works for other target languages
such as OpenCL and LLVM. In the implementation of Accelerate's CUDA backend, I
regard the collective operations of the Accelerate language as algorithmic
skeletons, where a parallel program is composed of one or more parameterised
skeletons, or templates, encapsulating specific parallel behaviour. The
dynamically generated code is specialised to the capabilities of the current
hardware, and to minimise the overhead of online compilation, kernels are
compiled in parallel and asynchronously with other operations, and previously
compiled kernels are cached and reused. The runtime system automatically manages
data on the GPU, minimising allocations and data transfers and overlapping
host-to-device transfers with other operations. The execution engine optimises
kernel launches for the current hardware to maximise thread occupancy, and
kernels are scheduled so that independent operations are executed concurrently,
for devices that support this capability. All this and more is done to ensure
that the Accelerate runtime system executes programs as efficiently as possible.

Chapter~\ref{ch:optimising} describes our solution to what we found as the two
most pressing performance limitations for executing embedded array computations:
sharing recovery and array fusion. In particular, Chapter~\ref{ch:optimising}
discusses my novel approach to type-safe array fusion for compilers targeting
bulk-parallel SIMD hardware. Fusion for a massively data-parallel embedded
language instrumented via algorithmic skeletons requires a few uncommon
considerations compared to existing shortcut fusion methods. This problem was
tacked by classifying array operations into two categories: producers are
operations where individual elements are computed independently, while consumers
need to know exactly how the input elements depend on each other in order to
implement the operation efficiently in parallel. The problem with existing
fusion systems is that this information is obfuscated, and as a result the
parallel interpretation of the program is lost. Program fusion is realised in
two stages. First, sequences of producer operations are fused using
type-preserving source-to-source term rewrites. In the second stage, the fused
producer representation is embedded directly into the consumer skeleton during
code generation. This phase distinction is necessary in order to support the
different properties of producers and consumers.

Following these two main chapters, Chapter~\ref{ch:results} presents a series of
benchmarks comparing the performance of the Accelerate program to hand-optimised
reference implementations. The benchmarks illustrate the positive effect of the
optimisations discussed in this thesis, and the accompanying analyses discuss
future work that can be undertaken in cases where the performance of the
Accelerate program is lower than that of the reference solution. Overall, I have
demonstrated that the Accelerate language is suitable for expressing a wide
range of programs, and that the runtime system and program optimisations
described in this thesis result in programs that are often competitive with low
level, hand optimised solutions.


\section{Research contribution}

This thesis presents two main research contributions. First, it demonstrates how
to implement a high-performance embedded language and runtime system. Second, it
establishes a novel method of array fusion for languages targeting bulk-parallel
SIMD hardware. Together, these contributions provide an embedded language of
array operations, which achieves performance comparable to traditional
imperative language approaches, but at a much higher level of abstraction.

\subsection{A high-performance embedded language}

While the current generation of graphics processing units are massively parallel
processors with peak arithmetic and memory throughput rates much higher than
traditional CPUs, programming them remains a work intensive exercise that
requires a substantial degree of expert knowledge. While several researchers
have proposed ameliorate the status quo, by either using a library to compose
GPU code or by compiling a subset of a high-level language to low-level GPU
code, no high-level embedded language has addressed either the breath of
operations supported by Accelerate, nor the underlying runtime system necessary
to achieve competitive performance.

The implementation of Accelerate's CUDA backend, covered in
Chapter~\ref{ch:implementation}, is original in that it successfully addresses
these problems. While the current implementation targets CUDA, the approach to
code generation works for other target languages, and the runtime system is
applicable to any deeply embedded language which coordinates with a foreign
language or library in order to evaluate computations.


\subsection{A fusion system for parallel array languages}

Although the topic of fusion has received significant prior attention in
the context functional programming languages, none of the existing techniques
are adequate for type-preserving embedded language compilers targeting massively
parallel SIMD hardware, such as GPUs.

The implementation of fusion for array languages implemented via algorithmic
skeletons, as developed in Chapter~\ref{ch:optimising}, is original. In
particular, by retaining the structure of the program as a sequence of
collective operations, we are able to generate efficient parallel
implementations of fused operations that are able to exploit all levels of the
processor thread and memory hierarchy, which is of crucial importance to
achieving high performance.


\section{Related work}

Previous chapters have already covered related work of special relevance to the
topics presented in their respective chapters. I refer readers to those
chapters, and simply make some general remarks here.


\section{Future work}

Based on the results of this thesis, a wide range of interesting future work is
possible. The previous chapters and the analysis of the benchmark results in
particular have already given suggestions for future work; these are not
repeated here, but some further topics are discussed.

\subsection{Nested data parallelism}

This thesis has demonstrated a high level array programming language and runtime
environment that is expressive and has performance competitive to much lower
level imperative program languages. However, the Accelerate language is
restricted to programs which express only \emph{flat} data parallelism, in which
the computation of each data element must itself be sequential. In practice,
this severely limits the applications of data-parallel computing, especially for
sparse and irregular problems~\cite{Prins:1999}.

Blelloch's pioneering work on NESL showed that it was possible to combine the
more flexible model of \emph{nested} data parallelism, where the computation of
each element may spawn further parallel computations, with a flat data parallel
execution model~\cite{Blelloch:1988iu}. Recent work has extend the flatting
transformation to support richer data types and higher order
functions~\cite{Jones:2008uu}.

Extending the Accelerate language to support the expression nested parallelism,
and implementing the vectorisation transformation to convert this nested
parallelism into a flat data parallel program which can be executed on the
compiler and runtime system presented in this thesis, is worthwhile future work.
This would allow Accelerate to support programs that are very difficult to
parallelise in a flat data parallel setting, such as the Barnes-Hut algorithm
for $N$-body simulation~\cite{Barnes:1986}.


\subsection{Type-preserving code generation}

The Accelerate language is richly typed, maintaining full type information of
the embedded language in the term tree and during the transformations described
in this thesis. Since Accelerate is an embedded language implemented via online
compilation, compilation time of the Accelerate program corresponds to Haskell
program \emph{runtime}. By encoding type information into the Accelerate terms
that are checked by the Haskell type checker at Haskell compile time, we ensure
that only well formed Accelerate programs will be compiled, reducing the number
of possible runtime errors.

Although we ensure the source Accelerate program is well typed, the
implementation of code generation presented in this thesis does not reflect the
types of the target language into the Haskell type system. Thus, any errors in
translating the Accelerate program into CUDA source code will not be detected at
Haskell compilation time, and will instead raise a runtime error when the
program is executed. Adding a fully type preserving compiler that translates the
well-typed Accelerate program into a verifiable target code is important future
work.


\subsection{Flexibility in the optimisation pipeline}

Chapter~\ref{ch:results} demonstrated the positive effects of the optimisations
that have been presented in this thesis. However, the optimisation pipeline is
completely fixed, so changing or extending the set of optimisations requires
changing the compiler itself.

Useful future work would be the ability to control the existing optimisation
process, for example by specifying the eagerness with which producer operations
should be fused. This would allow the user to override the default behaviour of
always fusing expressions that are used exactly once in subsequent computations.

Orthogonal to this work would be the ability to extend the optimisation process
through the implementation of a system of extensible rewrite
rules~\cite{Jones:2001wm}. Although our host language Haskell compiler supports
rewrite rules, since Accelerate programs can be generated at program runtime, we
require rewrite rules at program runtime as well. In addition to allowing the
compiler to generate rules internally to propagate information obtained from
automated analyses, this would allow library authors to express domain specific
knowledge that the compiler can not discover for itself. For example, the author
of a library of linear algebra routines might like to express the equivalence
that $x^{-1}x \mapsto 1$, or the less obvious $\left( 1 - x y \right)^{1}x
\mapsto x \left( 1 - y x \right)^{-1}$. While a programmer may be unlikely to
write such expressions themselves, they can easily appear when aggressive
inlining brings together code that was written separately. Adding the ability to
add rewrite rules to the program could be a very powerful feature, but raises
questions such as verifying whether the programmer-specified rules are
consistent with the underlying function definitions that they purport to
describe, or that the set of rules are confluent or even terminating.


% \begin{itemize}
% \item conclusion / summary discussion (2 days)
%
% \item future work
%     \begin{itemize}
%     \item frontend
%         \begin{itemize}
%             \item fusion cost analysis (1 day)
%             \item user controllable fusion (e.g. inline/noinline annotations) (1 day)
%             \item used definable rewrite rules (1 day)
%         \end{itemize}
%     \item backend
%         \begin{itemize}
%             \item type-preserving code generation (1 day)
%         \end{itemize}
%     \item language
%         \begin{itemize}
%             \item first order flattening: naturally nested algorithms
%                 (barns-hutt) as well as convenience of expression
%                 (matrix-vector) (2 days)
%         \end{itemize}
%     \end{itemize}
% \end{itemize}

