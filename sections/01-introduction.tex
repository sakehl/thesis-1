
\chapter{Introduction}
\label{ch:introduction}

\epigraph{%
Unless someone like you cares a whole awful lot,\\
nothing is going to get better. It's not.}%
{\textsc{---dr. seuss}\\\textit{The Lorax}}

% \epigraph{I hear you say ``Why?'' Always ``Why?'' You see things; and you say, ``Why?''\\
% But I dream things that never were; and I say, ``Why not?''}%
% {\textsc{---george bernard shaw}\\\textit{Back To Methuselah}}

% \epigraph{First things first, but not necessarily in that order.}
% {\textsc{---the doctor}}

%\epigraph{Come then, and let us pass a leisure hour in
%storytelling,\\and our story shall be the education of our heroes.}%
%{\textsc{---plato}\\\textit{Republic}, \textsc{book ii}}

The beginning of the twenty first century has seen an interesting trend in
computing emerge. General purpose CPUs, such as those provided by Intel, IBM and
AMD, have increased performance substantially, but with nowhere near the
increases seen in the late 1980's and early 1990's. To a large extent, single
threaded performance increases have tapered off, due to the low level of
inter-process communication in general purpose workloads, and the physical
limitations of power dissipation for integrated circuits. The additional
millions and billions of transistors afforded by Moore's rule\footnote{Moore's
actual prediction in 1965 referred only to the number of devices that could be
fabricated on a single die, and that this quantity would increase by fifty
percent each year \cite{Moore:1965wc}. I refrain from using the commonly held
name of ``Moore's Law'' as this trend is not a law in the sense defined by the
physical sciences, such as the Laws of Gravity and Thermodynamics --- it is
simply a description of observed facts, rather than a principle driven by some
fundamental controlling influence.} are simply not very productive in increasing
the performance of single--threaded code.

At the same time, the commodity \emph{graphics processing unit} (GPU)\gpu{} has
been able to use this ever increasing transistor budget effectively,
geometrically increasing rendering performance, as \indext{rasterisation} is
an inherently parallel operation.
%
% Historically fixed function pipelines, graphics architectures are
% evolving to become increasingly flexible as well as powerful, consisting of an
% expressive set of general purpose computation resources, with some fixed
% function units for graphics operations on the side.
%
Modern GPUs are massively parallel multicore processors, offering instruction
throughput and memory bandwidth rates much higher than those found on
traditional CPUs~\cite{NVIDIA:2012wf}. However, despite the advertised potential
of $100\times$ speedups, development of high-performance \emph{general-purpose
GPU} (GPGPU)\gpgpu{} programs requires highly idiomatic programs, whose
development is work intensive and requires a substantial degree of expert
knowledge.

Several researchers have proposed methods to ameliorate the status quo by either
using a library to compose GPU\gpu{} code or by compiling a subset of a
high-level language to low-level GPU
code~\cite{McCool:2004,Bond:2010bd,ThrustAParallelT:ub,Catanzaro:2011cn,Mainland:2010vj,CLyther:EvXSiruK,Muranushi:2012eh}.
This thesis is in the same spirit: we propose a domain-specific high-level
language of array computations, called \emph{Accelerate}, that captures
appropriate GPGPU\gpgpu{} programming idioms in the form of parameterised,
collective array operations. Our choice of operations was informed primarily by
the \emph{scan-vector model}~\cite{Chatterjee:1990vj}, which is suitable for a
wide range of algorithms and can be efficiently implemented on modern
GPUs~\cite{Sengupta:2007tc}.

The beginning of the Accelerate project predates this thesis, and several
researchers have contributed to it in that time. The following summarises the
main areas I have contributed to, and I have explicitly noted which are my own
individual work:
%
\begin{itemize}
    \item We have developed an embedded language, \emph{Accelerate}, of
        parameterised collective array computations that is more expressive than
        previous GPGPU\gpgpu{} proposals (Section~\ref{sec:accelerate}).

    \item I implement a dynamic code generator based on CUDA\cuda{}
        skeletons\skeleton{} of collective array operations that are
        instantiated at runtime (Sections \ref{sec:code_generation} \&
        \ref{sec:instantiating_skeletons}).

    \item I implement an execution engine that caches previously compiled
        skeleton instances and host-to-device data transfers, as well as
        parallelises code generation, data transfer, and GPU\gpu{} kernel
        loading, configuration, and execution
        (Sections~\ref{sec:dynamic_compilation},
        \ref{sec:memory_management}, \&
        \ref{sec:executing_programs}).

    \item We introduce a novel sharing recovery algorithm for type-safe abstract
        syntax trees, preserving the structure of the deeply embedded program
        (Section~\ref{sec:sharing_recovery}).

    \item I introduce type preserving optimisations of embedded language array
        programs, including a novel approach to array fusion
        % as well as scalar simplifications
        (Sections~\ref{sec:manipulating_embedded_programs},
        \ref{sec:array_fusion}, \&
        \ref{sec:simplification}).

    \item Benchmarks assessing runtime code generation overheads and kernel
        performance for a range of programs (Chapter~\ref{ch:results}).
\end{itemize}

Although we motivate and evaluate our work in the context of the Accelerate
language, our contributions are not limited to this context. Specifically, our
sharing recovery algorithm applies to any embedded language based on the typed
lambda calculus, and my approach to array fusion applies to any dynamic compiler
targeting bulk-parallel SIMD hardware. The current implementation targets
CUDA\cuda{}, but the same approach works for OpenCL, as well as traditional CPU
programming languages such as C and LLVM.

Our results show that Accelerate programs can be competitive with programs
hand-written in CUDA\cuda{}, while having a much higher level of abstraction.
Because of the work undertaken in this thesis, we have seen Accelerate gain
traction within academia and industry, including:

\begin{itemize}
    \item Peter Braam of Parallel Scientific is building a high level DSL in
        Haskell based on Accelerate, for image reconstruction computations for
        the Square Kilometer Array, the largest radio telescope in the world.

    \item Andy Gill has recently been awarded an NSF grant to research
        resource-aware DSLs for high performance computing applications, and is
        building on top of Accelerate.\footnote{\url{http://www.nsf.gov/awardsearch/showAward?AWD_ID=1350901}}

    \item Flowbox\footnote{\url{http://www.flowbox.io}} is a startup developing
        tools for composition and visual effects for images and video that is
        built on top of Accelerate. Flowbox is generating interest amongst
        several Hollywood studios as a promising new technology.

%    \item Matt Sottile of Galois is using Accelerate for computations in a
%        research project that attempts to reconstruct the brain activity of a
%        simple worm.

    \item We commonly see research papers on array-based computations and
        languages, particularly in a functional programming context, include a
        comparison to Accelerate in their own benchmarks.\footnote{However, I
        modestly refrain from going so far as to claim that my work in this
        thesis has set a new standard by which functional array-based
        programming languages are measured.}

\end{itemize}

We will discuss related work as each topic arises. The source code for
Accelerate including all benchmark code is available from
\url{https://github.com/AccelerateHS}.


% Parallel programming models fall into two broad categories. Task parallelism
% emphasises a distributed model for independent processes, whereas data
% parallelism specifies a single computation which is applied simultaneously
% across a large number of data elements. While task parallelism typically does not
% scale well with the size of the problem, data parallelism provides one of the
% most promising approaches to making efficient use of parallel hardware.
%
% Despite their attractiveness for computationally intensive operations, accessing
% this performance has remained elusive for all but a few applications. The
% arithmetic power of the GPU is a result of a highly specialised and constrained
% architecture, resulting in programs which rely heavily on the CPU to handle
% difficult parts of control and data flow. However, the CPU--GPU link is
% relatively slow, engendering high communication latencies which radically impair
% performance. To be practicable, the GPU programming model must efficiently
% support messy boundary conditions, irregular data and control flow patterns, and
% provide an elegant mapping from the application to the underling architecture,
% without forcing the programmer to recast their algorithm. Support for nested
% data parallelism \cite{Blelloch:1994vc,Blelloch:1996jx} (NDP) would provide a
% first step towards a viable GPU programming model, unshackling vast
% computational resources from the small application niche for which they were
% originally designed.

