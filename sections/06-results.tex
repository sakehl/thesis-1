%
% analysis, example programs
%

\chapter{Results}
\label{ch:results}
% \epigraph{Dionysus [is] the Master of Illusions, who could make a vine grow out
% of a ship's plank, and in general enable his votaries to see the world as the
% world's not.}%
% {\textsc{---e.\ r.\ dodds}\\\textit{The Greeks and the Irrational}}

% \epigraph{If the immutable appears recast, then you yourself have been
% transformed.}%
% {\textsc{---r.\ scott bakker}\\\textit{The Judging Eye}}

\epigraph{Logic merely enables one to be wrong with authority}%
{\textsc{---the doctor}} %\\\textit{The Wheel in Space}}


The previous chapters discussed the design, implementation and optimisation of
the Accelerate language and CUDA backend. This chapter analyses the performance
of the implementation. Benchmarks were conducted on a single Tesla T10 processor
(compute capability 1.3, 30 multiprocessors = 240 cores at 1.3GHz, 4GB RAM)
backed by two quad core Xeon E5405 CPUs (64-bit, 2GHz, 8GB RAM) running
GNU/Linux (Ubuntu 12.04 LTS, CUDA 5.0).


\section{Runtime Overheads}

Runtime program optimisation, code generation, kernel loading, data transfer,
and so on can contribute significant overhead to short lived GPU computations.
Accelerate mitigates these overheads via caching and memoisation. For example,
the first time a particular expression is executed it is compiled to CUDA code,
which is reused for subsequent invocations. This section analyses those
overheads, so that they can be factored out later when we discuss the effects of
the program optimisations on kernel runtimes of the benchmark programs.


\subsection{Program Optimisation}

% tk: performance and benchmarks for the fusion pass
%   - actual runtime
%   - tick counters for the final version and old one with extend/cunctation
%     separation

\subsection{Data Transfer}

% tk: some graphs of memory transfer times or bandwith vs. array size
% tk: importance of using weak pointers, so data transfers ``escape'' the black
%     box of evaluating the AST --> hashcat.
% tk: importance of the nursery for reducing calls to malloc/free

\subsection{Code Generation \& Compilation}

% tk: measure \& tabulate some code generation / compilation times
% tk: persistent cache, perhaps just a mention?


\section{Dot product}
\label{sec:dotp}

Dot product, or scalar product, is an algebraic operation that takes two
equal-length sequences of numbers and returns a single number by computing the
sum of the products of the corresponding entries in the two sequences. Dot
product can be defined in Accelerate using the code below, which was also seen
in section~\ref{sec:producer_consumer_fusion}.
%
\begin{lstlisting}[style=haskell
    ,label=lst:dotp
    ,caption={Vector dot-product in Accelerate}]
dotp :: Acc (Vector Float) -> Acc (Vector Float) -> Acc (Scalar Float)
dotp xs ys = A.fold (+) 0                       -- sum result of\ldots
           $ A.zipWith (*) xs ys                -- \ldots element-wise multiplying inputs
\end{lstlisting}
%
Figure~\ref{fig:dotp} show the result of running this code, compared to several
other sequential and parallel implementations. The \texttt{Data.Vector} baseline
is sequential code produced by \index{fusion!stream}stream
fusion~\cite{Coutts:2007kp}, running on the host CPU. The \texttt{Repa} version
runs in parallel on all eight cores of the host CPU, using the fusion method of
\index{fusion!delayed arrays}delayed arrays~\cite{Keller:2010er}. The
\texttt{NDP2GPU}~\cite{Bergstrom:2012bi} version compiles NESL
code~\cite{Blelloch:1995ut} down to CUDA. The performance of this version
suffers because the \texttt{NDP2GPU} compiler uses the legacy NESL compiler for
the front-end, which introduces redundant administrative operations that are not
strictly needed when evaluating a dot product.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/dotp/dotp}
    \end{center}
    \caption[Vector dot product kernel benchmarks]{Kernel runtimes for vector
        dot product, in Accelerate with and without optimisations, compared to
        other parallel GPU and CPU implementations. Note the log-log scale.}
    \label{fig:dotp}
\end{figure}

Without optimisations the Accelerate version executes in approximately twice the
time of the CUBLAS version. Since the individual aggregate operations consist of
only a single arithmetic operation each, the problem can not be a lack of
sharing.


\subsection{Too many kernels}

The slow-down in the unoptimised version is due to Accelerate generating one GPU
kernel function for each aggregate operation in the source program. The use of
two separate kernels requires the intermediate array produced by @zipWith@
to be constructed in GPU memory before being immediately read back by
@fold@. In contrast the CUBLAS version uses only a single kernel. As this
is a simple memory bound benchmark, lack of fusion roughly doubles the runtime.

The fused version of dot product combines the aggregate computations by
embedding a function of type @(sh -> a)@ into the reduction, represented
here as the second argument to the constructor @Delayed@. This scalar
function does the work of element wise multiplying the two input vectors, and is
performed on-the-fly as part of the reduction kernel instead of requiring an
intermediate array:
%
\begin{lstlisting}[style=haskell]
let a0 = use (Array ...) in
let a1 = use (Array ...) in
fold (\x0 x1 -> x0 + x1) 0.0
   (Delayed (intersect (shape a0) (shape a1))   -- extent of the input array
            (\x0 -> (a0!x0) * (a1!x0)))         -- function to generate a value at each index
\end{lstlisting}

While the fused Accelerate version is $30\times$ faster than the sequential
version executed on the CPU, it is still approximately $20\%$ slower than the
hand-written CUBLAS version. As dot product is a computationally simple task,
any additional overheads in the implementation will be significant. To see why
this version is still slightly slower, we analyse the generated code to inspect
it for sources of overhead.


\subsection{Duplicate loop counters}

The fused dot product operation will only perform the element wise
multiplication of the two input arrays in the first step of the tree reduction
(\S\ref{sec:parallel_reduction}). This occurs in the phase of the cascaded
algorithm when individual threads sequentially sum multiple elements. After
embedding the fused producer, the following CUDA code is generated for the inner
loop of this step:
%
% pookie is the bestest
% bubbaboo ish silly
% kekekee
% i can't put smiley faces in cause weird things happen
% hehe^{this is my thesis
% please like it
% i worked really hard
% and writed a lots
% }<++>
%
%
\begin{lstlisting}[style=cuda
    ,firstnumber=18
    ,label=lst:dotp_cuda
    ,caption={Generated CUDA code for the first step of fused dot product}]
for (ix += gridSize; ix < shapeSize; ix += gridSize) {
    const Int64 v2 = ix;
    const int v3 = toIndex(shIn0, shape(v2));
    const int v4 = toIndex(shIn1, shape(v2));

    x0 = arrIn0_a0[v3] * arrIn1_a0[v4];
    y0 = x0 + y0;
}
\end{lstlisting}
%
We have four loop counters: @ix@, @v2@, @v3@ and @v4@ ---
two for the source arrays and two to convert between the multidimensional and
linear representations. These counters contain the same value and are
incremented in lockstep. In addition to the superfluous arithmetic, the
duplication of counters unnecessarily increases register pressure.

\marginnote{this was unexpected}
The corresponding section of PTX~\cite{NVIDIA:2012vj} code is for this loop is
shown in Listing~\ref{lst:dotp_ptx}. To retrieve the data from the first input
array, the input array pointer is retrieved (line~\ref{lst:dotp_ptx_ldparam}),
the offset stored in register @rd16@ added to it
(line~\ref{lst:dotp_ptx_add}), and then the value read from global memory at
this address (line~\ref{lst:dotp_ptx_ldglobal}). Happily, we note that
retrieving the second input element \emph{also} uses the offset stored in
register @rd16@ (line~\ref{lst:dotp_ptx_ldglobal2}).
%
\begin{lstlisting}[style=ptx
    ,float
    ,firstnumber=103
    ,label=lst:dotp_ptx
    ,caption={[Corresponding PTX code for the first step of fused dot product]
        Corresponding PTX code for the first step of fused dot product (sm13)}]
 //  18          for (ix += gridSize; ix < shapeSize; ix += gridSize) {
        cvt.u32.u16     %r10, %nctaid.x;
        mul.lo.u32      %r11, %r10, %r1;
        add.s32         %r12, %r11, %r7;
        mov.s32         %r13, %r12;
        setp.le.s32     %p2, %r8, %r12;
        @%p2 bra        $Lt_0_13058;
        cvt.s64.s32     %rd12, %r12;
        cvt.s64.u32     %rd13, %r11;
$Lt_0_14082:
 //<loop> Loop body line 18, nesting depth: 1, estimated iterations: unknown
        .loc    16      24      0
 //  20              const int v3 = toIndex(shIn0, shape(v2));
 //  21              const int v4 = toIndex(shIn1, shape(v2));
 //  22
 //  23              x0 = arrIn0\_a0[v3] * arrIn1\_a0[v4];
 //  24              y0 = x0 + y0;
        cvt.s32.s64     %rd14, %rd12;                           (@* \label{lst:dotp_ptx_cvt1} *@)
        cvt.s32.s64     %r14, %rd14;
        cvt.s64.s32     %rd15, %r14;                            (@* \label{lst:dotp_ptx_cvt3} *@)
        mul.wide.s32    %rd16, %r14, 4;
        .loc    16      17      0
        ld.param.u64    %rd9, [__cudaparm_foldAll_arrIn0_a0];   (@* \label{lst:dotp_ptx_ldparam} *@)
        .loc    16      24      0
        add.u64         %rd17, %rd16, %rd9;                     (@* \label{lst:dotp_ptx_add} *@)
        ld.global.f32   %f4, [%rd17+0];                         (@* \label{lst:dotp_ptx_ldglobal} *@)
        .loc    16      17      0
        ld.param.u64    %rd8, [__cudaparm_foldAll_arrIn1_a0];
        .loc    16      24      0
        add.u64         %rd18, %rd16, %rd8;
        ld.global.f32   %f5, [%rd18+0];                         (@* \label{lst:dotp_ptx_ldglobal2} *@)
        mad.f32         %f3, %f4, %f5, %f3;
        add.s32         %r13, %r13, %r11;
        add.s64         %rd12, %rd12, %rd13;
        setp.gt.s32     %p3, %r8, %r13;
        @%p3 bra        $Lt_0_14082;
        bra.uni         $Lt_0_13058;
$Lt_0_13314:
        mov.f32         %f3, %f6;
$Lt_0_13058:
        .loc    16      27      0
 //  25          }
\end{lstlisting}

In this case the CUDA compiler was able to coalesce our four counters into a
single counter. This is because our definition of @toIndex@ specialised for
one-dimensional indices @DIM1@ does \emph{not} do bounds checking:
%
\begin{lstlisting}[style=cuda]
template <>
static __inline__ __device__ Ix toIndex(const DIM1 sh, const DIM1 ix)
{
    return ix;
}
\end{lstlisting}
%
Since we do not check that the current index @ix@ is within bounds of the
array shape @sh@, the compiler can see that the definitions of @v3@
and @v4@ in Listing~\ref{lst:dotp_cuda} are identical. Similarly
one-dimensional indices are just integers, and so the one-dimensional instance
of @shape@ is the identity function. For higher dimensional shapes,
however, this is not the case and @toIndex@ and @shape@ do real work
to their arguments. As the shape @shIn0@ and @shIn1@ are inputs
arguments to the kernel, the compiler will not be able to determine they are
equivalent and so the counters would not be combined.

Why don't we perform bounds checks in @toIndex@? Implementing exceptions in
a massively parallel architecture is difficult, and support for throwing
exceptions from kernel functions was only recently added for devices of compute
capability 2.0 and later~\cite{NVIDIA:2012wf}. % [\SB.15]


\subsection{64-bit Arithmetic}

CUDA devices are at their core 32-bit processors and thus are optimised for
32-bit arithmetic. For example, our Tesla GPU with compute capability 1.3 has a
throughput of eight 32-bit floating-point add, multiply, or multiply-add
operations per clock cycle per multiprocessor, but only a single operation per
cycle of the 64-bit equivalents of these~\cite{NVIDIA:2012wf}. % [\S5.4.1]

However, the host architecture that the Haskell program executes on is likely to
be a 64-bit processor. This means that an @Int@ value from Haskell ---
which may appear marshalled as array data or shape values --- will generate code
to manipulate 64-bit integers, as well as conversions between 32- and 64-bit
types. Lines~\ref{lst:dotp_ptx_cvt1}--\ref{lst:dotp_ptx_cvt3} of
Listing~\ref{lst:dotp_ptx} contain such type conversions, which on our compute
capability 1.3 device have a throughput of only a single operation per cycle per
multiprocessor. These conversions would not be necessary if we were not
cross-compiling from a 64-bit Haskell host, or if the programmer working
directly with CUDA only used the @int@ type, which would be interpreted by
the CUDA compiler as a 32-bit wide integer.


\subsection{Non-neutral starting elements}
\label{sec:non-neutral_starting_elements}

For convenience, and to avoid possibly unexpected behaviour, Accelerate's
@fold*@ family of functions do not require the combination function and
starting element to form a moniod. We still require the function to be
associative, so that the reduction can be implemented efficiently in parallel as
a tree-reduction, but the initial element does not need to be a \emph{neutral}
element.\marginnote{semigroup?} For example, @fold (+) 10@ is valid in
Accelerate and evaluates to the correct answer.

However, this means that evaluation of the reduction is more complex. In
particular, we can not simply initialise all threads to the initial value.
In the first phase of the reduction, threads must be sure to initialise
their local sum @y0@ with values from the input array before beginning the
sequential loop show in Listing~\ref{lst:dotp_cuda}:
%
\begin{lstlisting}[style=cuda,firstnumber=12]
if (ix < shapeSize) {
    const Int64 v2 = ix;
    const int v3 = toIndex(shIn0, shape(v2));
    const int v4 = toIndex(shIn1, shape(v2));

    y0 = arrIn0_a0[v3] * arrIn1_a0[v4];
    for (ix += gridSize; ix < shapeSize; ix += gridSize) {
\end{lstlisting}

Similarly, in the second parallel tree reduction phase threads may only read
values from shared memory that were properly initialised. This requires
additional bounds checks at every step:
%
\begin{lstlisting}[style=cuda,firstnumber=27]
sdata0[threadIdx.x] = y0;
__syncthreads();
ix = min(shapeSize - blockIdx.x * blockDim.x, blockDim.x);
if (threadIdx.x + 512 < ix) {
    x0 = sdata0[threadIdx.x + 512];
    y0 = y0 + x0;
    sdata0[threadIdx.x] = y0;
}
__syncthreads();
if (threadIdx.x + 256 < ix) {
    x0 = sdata0[threadIdx.x + 256];
    y0 = y0 + x0;
    sdata0[threadIdx.x] = y0;
}
__syncthreads();
// etc...
\end{lstlisting}

These requirements increase overhead from ancillary instructions that are not
loads, stores, or arithmetic for the core computation. A summation reduction has
low arithmetic intensity to begin with, and is the limiting factor in the
performance of this kernel, so additional bounds checks further reduce
performance.

\subsection{Kernel specialisation}

To further reduce instruction overhead, it is possible to completely unroll the
reduction by specialising the kernel for a specific block size. In standard CUDA
this can be achieved through the use of C++ templates. Branches referring to the
template parameter will be evaluated at compile time, resulting in a very
efficient inner loop.
%
\begin{lstlisting}[style=cuda]
template <unsigned int blockSize> __global__ void reduce(...) {
    ...
    if (blockSize > 512) {
    }
    if (blockSize > 256) {
    }
    ...
\end{lstlisting}

This technique has shown to produce significant gains in
practice~\cite{Harris:2007te}, although requires compiling a separate kernel for
each thread block size we wish to specialise for. Accelerate can achieve this
kind of kernel specialisation because the CUDA code is generated at program
runtime, but this extra compilation adds significant additional overhead. Since
compiled kernels are cached and reused (\S tk), if we know that the reduction
will be computed many times the extra compilation overhead can be amortised by
the improved kernel performance, and may thus be worthwhile. Such
specialisations are left for future work.


\section{Black-Scholes option pricing}
\label{sec:blackscholes}

The Black-Scholes algorithm is a partial differential equation for modelling the
evolution of a European-style stock option price under certain assumptions. The
corresponding Accelerate program is shown in Listing~\ref{lst:blackscholes}.
Given a vector of triples of the underlying stock price, strike price, and time
to maturity in years, the Black-Scholes formula computes the price of a call and
put option. The function @callput@ evaluates the Black-Scholes formula for
a single triple, and @blackscholes@ maps it over a vector of triples such
that all individual applications of the formula are executed in parallel.
%
\begin{lstlisting}[style=haskell
    ,float
    ,label=lst:blackscholes
    ,caption={Black-Scholes option pricing in Accelerate}]
horner :: Num a => [a] -> a -> a
horner coeff x =
  let madd a b  = a + x*b
  in
  x * foldr1 madd coeff

cnd' :: Floating a => a -> a
cnd' d =
  let poly      = horner coeff
      coeff     = [0.31938153, -0.356563782, 1.781477937, -1.821255978, 1.330274429]
      rsqrt2pi  = 0.39894228040143267793994605993438
      k         = 1.0 / (1.0 + 0.2316419 * abs d)
  in
  rsqrt2pi * exp (-0.5*d*d) * poly k

blackscholes :: Acc (Vector (Float, Float, Float)) -> Acc (Vector (Float, Float))
blackscholes = A.map callput
  where
  callput x =
    let (price, strike, years) = A.unlift x
        r       = A.constant riskfree
        v       = A.constant volatility
        v_sqrtT = v * sqrt years
        d1      = (log (price / strike) + (r + 0.5 * v * v) * years) / v_sqrtT
        d2      = d1 - v_sqrtT
        cnd d   = let c = cnd' d in d >* 0 ? (1.0 - c, c)
        cndD1   = cnd d1
        cndD2   = cnd d2
        x_expRT = strike * exp (-r * years)
    in
    A.lift ( price * cndD1 - x_expRT * cndD2                    -- call price
           , x_expRT * (1.0 - cndD2) - price * (1.0 - cndD1))   -- put price
\end{lstlisting}

Figure~\ref{fig:blackscholes} shows the result of running this code compared to
the implementation that ships with the CUDA SDK. Without optimisations, the
Accelerate version is almost twenty times slower than the equivalent
implementation in CUDA C. As @blackscholes@ includes only one collective
array operation, the problem can not be a lack of fusion.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/black-scholes/black-scholes}
    \end{center}
    \caption[Black-Scholes kernel benchmarks]{Kernel runtimes for Black-Scholes
        options pricing, in Accelerate with and without optimisations, compared
        to a hand-written CUDA version. Note the log-log scale.}
    \label{fig:blackscholes}
\end{figure}

\subsection{Too little sharing}

The function @callput@ from Listing~\ref{lst:blackscholes} includes a
significant amount of sharing: the helper functions @cnd'@ and hence
@horner@ are used twice --- for @d1@ and @d2@ --- and its
argument @d@ is used multiple times in the body. Furthermore, the
conditional expression @d >* 0 ? (1 - c, c)@ results in a branch that,
without sharing, results in a growing number of predicated instructions that
leads to a large penalty on the SIMD architecture of the GPU.

Without sharing the generated code requires 2573 instructions and results in
\marginnote{check instruction count difference}
significant warp divergence which serialises portions of the execution. With
sharing recovery the generated code requires 501 instructions and is actually
slightly faster than the reference CUDA version because the latter contains a
common subexpression that is not spotted by the programmer and not eliminated by
the CUDA compiler. The common subexpression performs a single multiplication.


\section{Mandelbrot fractal}
\label{sec:mandelbrot}

The Mandelbrot set is generated by sampling values $c$ in the complex plane and
determining whether under iteration of the complex quadratic polynomial:
\[
z_{n+1} = z_{n}^{2} + c
\]
that the magnitude of $z$ (written $\left| z_{n} \right|$) remains bounded
however large $n$ gets. Images of the Mandelbrot set are created such that each
pixel corresponds to a point $c$ in the complex plane, and its colour depends on
the number of iterations $n$ before the relation diverges, with $z_{0} = c$. The
set of points forming the boundary of this relation forms the distinctive and
easily recognisable fractal shape, as was seen in Figure~\ref{fig:mandelbrot}.

\begin{table}
\centering
\small
\begin{tabu}{lrrrr}
\toprule

                        & \multicolumn{1}{c}{\bf Time}
                        & \multicolumn{1}{c}{\bf Bandwidth}
                        & \multicolumn{1}{c}{\bf Step}
                        & \multicolumn{1}{c}{\bf Cumulative} \\

\textbf{Benchmark}      & \multicolumn{1}{c}{\bf (ms)}
                        & \multicolumn{1}{c}{\bf (GB/s)}
                        & \multicolumn{1}{c}{\bf Speedup}
                        & \multicolumn{1}{c}{\bf Speedup} \\\midrule

Accelerate              &
                        &
                        &
                        & \\

Accelerate (+sharing)   &
                        &
                        &
                        & \\

Accelerate (+fusion)    &
                        &
                        &
                        & \\

Accelerate (+loop recovery)
                        &
                        &
                        &
                        & \\

CUDA (limit)            & 14.0
                        & 0.55
                        &
                        & \\

CUDA (avg)              & 5.0
                        & 1.53
                        & 2.79$\times$
                        & 2.79$\times$ \\

CPU (avg)               & 1389
                        & 0.006
                        &
                        & \\

\bottomrule
\end{tabu}
\caption[Mandelbrot fractal kernel benchmarks]{Mandelbrot fractal benchmarks in
    Accelerate with and without optimisations, compared to a hand written CUDA
    version. \note{tk: figure?}}
\label{tab:mandelbrot}
\end{table}

As expected, Table~\ref{tab:mandelbrot} shows that without fusion performance is
poor because storing each step of the iteration saturates the memory bus to the
point that reducing arithmetic intensity with sharing recovery
(\S\ref{sec:sharing_observation}) provides no improvement.


\subsection{Fixed unrolling}

In order to meet the restrictions of what can be efficiently executed on
specialised hardware such as GPUs, Accelerate does not directly support any form
of recursion. To implement the recurrence relation we instead define each step
as a collective operation and unfold the loop a fixed number of times.

Given $z_{n}$ and $c$, we can easily compute $z_{n+1}$, and for each point we
need to iterate this process until divergence, and then remember the number of
iterations at which divergence happened. This creates a small problem: GPUs are
designed to do the \emph{same thing} to lots of different data at the same time,
whereas we want to do something different depending on whether or not a
particular point has diverged or not.

Conditionals in GPU code should be avoided as much as possible because they can
lead to SIMD divergence. We can't avoid \emph{some} kind of conditional, but we
can make sure there is only a bounded amount of divergence by having just one
conditional per iteration, and a fixed number of iterations. The trick is to
keep a pair @(z,i)@ for every array element, where @i@ is the
iteration at which the point @z@ diverged. Each step of the iteration is
then:
%
\begin{itemize}
    \item Compute @z' = c `plus` (z `times` z)@
    \item If @z'@ is greater than 4, then the result is @(z, i)@
    \item Otherwise the result is @(z', i+1)@
\end{itemize}

\begin{lstlisting}[style=haskell
    ,float=tbh
    ,label=lst:mandelbrot
    ,caption={Mandelbrot set generator in Accelerate}]
type Complex = (Float, Float)                   -- not all GPUs can use \emph{Double}

mandelbrot :: Int -> Int -> Int -> Int
           -> Acc (Scalar View)
           -> Acc (Array DIM2 (Complex, Int))
mandelbrot screenX screenY depth view
  = P.foldr ($) zs0                             -- apply in sequence the\ldots  \label{lst:mandelbrot_foldr}
  $ P.take depth (repeat step)                  -- \ldots\emph{depth} copies of the function \emph{step}
  where
    cs  = genPlane screenX screenY view         -- the static complex plane $c$
    zs0 = mkinit cs                             -- initial array of $(c, 0)$

    step :: Acc (Array DIM2 (Complex, Int))     % \label{lst:mandelbrot_step} %
         -> Acc (Array DIM2 (Complex, Int))
    step = A.zipWith iter cs                    -- compute one iteration to progress to $z_{n+1}$

iter :: Exp Complex -> Exp (Complex, Int) -> Exp (Complex, Int)
iter c zi = f (A.fst zi) (A.snd zi)
 where
  f :: Exp Complex -> Exp Int -> Exp (Complex, Int)
  f z i =
    let z' = next c z
    in (dot z' >* 4) ? ( zi , lift (z', i+1) )

next :: Exp Complex -> Exp Complex -> Exp Complex
next c z = c `plus` (z `times` z)

dot :: Exp Complex -> Exp Float
dot = lift1 f
  where f :: (Exp Float, Exp Float) -> Exp Float
        f (x,y) = x*x + y*y
\end{lstlisting}

The core of the Mandelbrot routine is shown in Listing~\ref{lst:mandelbrot}. The
function @step@ (Line~\ref{lst:mandelbrot_step}) advances the entire
complex plane by one iteration of the recurrence relation. This function is
replicated @depth@ times and then the sequence combined by folding with the
function application operator @(\$)@ (Line~\ref{lst:mandelbrot_foldr}). This
results in a single fused kernel whole performance is greatly improved, but
still several times slower than the CUDA version.

The slowdown is because the fused code generates a completely unrolled loop,
with @depth = 255@ copies of the code for @step@. Unrolling loops not
only increases instruction count but can often increase register lifetimes
because of the scheduling of loads and stores. In particular, stores are pushed
down and loads are moved up in the instruction stream, which results in
temporary scalar lifetimes being longer. The unrolled code requires 63 registers
per thread (compute capability 3.0), resulting in a multiprocessor occupancy of
only 50\%. Since the Mandelbrot program is limited by computation, halving the
maximum number of resident threads halves the maximum performance.

% \begin{lstlisting}[style=cuda
%     ,firstnumber=24]
% const float v13 = v10 * v10 - v11 * v11;                // single application of \texttt{step}
% const float v14 = v10 * v11 + v11 * v10;
% const float v15 = v8 + v13;
% const float v16 = v9 + v14;
% const Word8 v17 = v15 * v15 + v16 * v16 > 4.0f;
% const float v18 = v17 ? v10 : v15;
% const float v19 = v17 ? v11 : v16;
% const Int64 v20 = v17 ? v12 : (Int64) 1;
%   // repeats 255 times\ldots
% \end{lstlisting}


\subsection{Loop recovery}

Sometimes unrolling loops can be beneficial. A CUDA programmer can control how
aggressively the compiler unrolls loops using the @#pragma unroll@
directive with an explicit unroll factor, where zero means to not unroll the
loop at all.

The problem is that the way we have fused the collective operations does not
preserve the iteration structure of the recurrence relation, so the loop is
completely unrolled in the single fused operation. We can re-introduce scalar
loops by analysing the fused expression to look for the following pattern:
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape]
%\bf$\langle$ loop introduction $\rangle$%
    let x =
        let y = e1
        in e2
    in e3
    $\mapsto$
    iterate[2] (\y -> e2) e1            %\rm if \texttt{e2} $\equiv$ \texttt{e3}%
\end{lstlisting}
%
The nested bindings are replaced with an explicit scalar value iteration
structure, where the expression @e2@ is repeated twice with the initial
value @e1@. Similarly, loops can be joined:
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape]
%\bf$\langle$ loop joining $\rangle$%
    let x = iterate[n] (\y -> e2) e1
    in e3
    $\mapsto$
    iterate[n+1] (\y -> e2) e1          %\rm if \texttt{e2} $\equiv$ \texttt{e3}%
\end{lstlisting}
%
Recovering scalar loops in this way enables a backend to generate explicit loops
in the target language, and the compiler makes the decision of whether or not to
unroll the loop. In future, it would be beneficial to augment it with related
loop optimisations such as loop invariant code motion in the frontend and
lower-level code generation optimisations in a backend.


\subsection{Unbalanced workload}

The final problem with the Mandelbrot program is due to the initial formulation
of the program using a fixed unrolling, as we must handle application of the
function to elements which have already diverged. As explained, this is to
ensure that all threads [in a warp] continue doing the same thing, which avoids
excessive SIMD divergence, but means that the computation always proceeds to the
iteration limit.

For the Mandelbrot program threads only diverge in the sense that they are
either still computing the recurrence relation, or the point is not in the set
and the computation has completed; threads do not diverge to follow separate
execution paths. The latter type of SIMD divergence is what we really need to
avoid, because this results in predicated execution: threads taking the
@true@ path of a branch execute while all other threads sit idle, then
execution switches to the @false@ branch while the first set of threads
idle.

The CUDA version is faster because threads exit the loop as soon as their point
in the complex plane diverges. Although once a thread completes it sits idle
until all threads in the group complete their calculation, if all the points
within a group of threads diverge quickly this can save a significant amount of
work. Table~\ref{tab:mandelbrot} shows the difference in execution time when
displaying the entire fractal (avg), where points diverge at varying iteration
depths in the sequence, compared to a region where all thread continue to the
iteration limit (limit). In order to manage this unbalanced workload, the CUDA
version additionally includes a custom thread block scheduler.

This test demonstrates that while loop recovery offered a significant
performance benefit for free, it would be beneficial to expose looping
constructs in the source language as well, rather than relying on potentially
fragile post-hoc transformations. Avoiding thread divergence in CUDA programs is
an active area of research~\cite{Zhang:2010jc}.

\section{N-body gravitational simulation}
\label{sec:nbody}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/nbody/nbody}
    \end{center}
    \caption[N-body gravitational simulation kernel benchmarks]{Kernel runtimes
        for the $n$-body gravitational simulation, in Accelerate with and
        without optimisations, compared to a hand-written CUDA implementation.
        Note the log-log scale.}
    \label{fig:nbody}
\end{figure}

The $n$-body example simulates the Newtonian gravitational forces on a set of
massive bodies in 3D space, using the na\"ive $\mathcal{O}\left( n^{2} \right)$
algorithm. In a data-parallel setting, the natural way to express this algorithm
is first to compute the forces between every pair of bodies, before adding the
forces applied to each body using a segmented sum. Without fusion this approach
also requires $\mathcal{O}\left( n^{2} \right)$ space for the intermediate array
of forces, which exhausts the memory of our device (4GB!) when using more than
about five thousand bodies. With fusion, the reduction operation consumes each
force value on-the-fly, so that the program only needs $\mathcal{O}\left( n
\right)$ space to store the final force values. The core of the $n$-body
simulation is shown in Listing~\ref{lst:nbody}, where @accel@ calculates
the acceleration between two particles, and @(.+.)@ component-wise sums the
acceleration of a particle along each $x$, $y$ and $z$ axis.

\begin{lstlisting}[style=haskell
    ,float
    ,label=lst:nbody
    ,caption={$N$-body gravitational simulation in Accelerate}]
calcAccels :: Exp Float -> Acc (Vector Body) -> Acc (Vector Accel)
calcAccels epsilon bodies
  = let n       = A.size bodies
        cols    = A.replicate (lift $ Z :. n :. All) bodies
        rows    = A.replicate (lift $ Z :. All :. n) bodies
    in
    A.fold (.+.) (vec 0) $ A.zipWith (accel epsilon) rows cols
\end{lstlisting}

\subsection{Use of Shared Memory}

Even with fusion the reference CUDA version is over 10x faster. In CUDA, the
memory hierarchy of the device is made explicit to the programmer, including a
small on-chip shared memory region threads can use to communicate results, that
is essentially a software managed cache (\S tk). The CUDA version uses this
memory region to share the particle mass and position data between a group of
threads, whereas Accelerate uses this region to share computation.

The CUDA version uses this low-latency memory region to reduce the bandwidth
requirements of the program by sharing the particle mass and position data
between the group of threads. This reduces the bandwidth requirement of the
program by a factor of the number of threads in a block (256) but requires each
thread to sum its particle interactions sequentially in $\mathcal{O}\left( n
\right)$ time. The Accelerate version uses shared memory to perform a
tree-reduction in $\mathcal{O}\left( \log n \right)$ time
(\S\ref{sec:parallel_reduction}) but requires all $n^{2}$ memory transfers. For
this bandwidth bound application, making the trade-off of parallelism for
bandwidth clearly wins. Automatic use of shared memory is a separate
consideration to the optimisations discussed in this work, and remains an open
research problem~\cite{Ma:2010ft}.


\section{Fluid Flow}
\label{sec:fluid}

The fluid flow example implements Jos Stam's stable fluid
algorithm~\cite{Stam:1999ey}, which is a fast approximate algorithm intended for
animation and games, rather than accurate engineering simulation. An example
sequence is shown in Figure~\ref{fig:fluid_steps}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.3\textwidth]{images/sec-6/fluid/fluid-10}
        \includegraphics[width=0.3\textwidth]{images/sec-6/fluid/fluid-50}
        \includegraphics[width=0.3\textwidth]{images/sec-6/fluid/fluid-75}
    \end{center}
    \caption[Example of the fluid flow simulation]{An example showing the
        progression of the fluid flow simulation for a set of initial conditions
        after 10 (left), 50 (centre) and 75 (right) steps.}
    \label{fig:fluid_steps}
\end{figure}

The core of the algorithm is a finite time step simulation on a grid,
implemented as a matrix relaxation involving the discrete Laplace operator
($\nabla^2$). This step, known as the linear solver, is used to diffuse the
density and velocity fields throughout the grid, as well as apply a projection
operator to the velocity field to ensure it conserves mass. The linear solver is
implemented in terms of a stencil convolution, repeatedly computing the
following for each grid element to allow the solution to converge:
\[
u_{i,j}^{''} = \left( u_{i,j} + a \cdot \left( u'_{i-1,j}+u'_{i+1,j}+u'_{i,j-1}+u'_{i,j+1} \right) \right) / c
\]
Here, $u$ is the grid in the previous time step, $u'$ the grid in the current
time step and previous relaxation iteration, and $u''$ the current time step and
iteration. The values $a$ and $c$ are constants chosen by the simulation
parameters.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/fluid/fluid}
    \end{center}
    \caption[Fluid flow simulation kernel benchmarks]{Kernel runtimes for the
        fluid flow simulation, in Accelerate with and without optimisations,
        compared to sequential and parallel CPU implementations. Running the
        Repa program with seven threads on the eight cores was found to be
        faster. Note the log-log scale.
        \note{TK: Check the fluid demo that is part of the CUDA SDK, it looks
        like it might implement something similar.}}
    \label{fig:fluid}
\end{figure}

Figure~\ref{fig:fluid} compares Accelerate to a parallel implementation written
in Repa and running on the host CPU \cite{Lippmeier:2012gx}. The program is
extremely memory intensive, performing approximately 160 convolutions of the
grid per time step. Given the nature of the computation, we find that the raw
bandwidth available on the CUDA device, which is much greater than that
available to the CPU, results in correspondingly shorter run times. Since the
program consists of a sequence of stencil operations, fusion does not apply to
this program. Sharing recovery has a surprisingly minimal impact because the
implementation of stencils always shares some parts of the computation: namely
access to the grid elements $u_{xy}$, precisely because these operations are so
expensive.

\subsection{Bounds checking}
\label{sec:stencil_bounds_checking}

When implementing stencil operations, an immediate concern is what to do when
the stencil ``falls off'' the edge of the array. For example,
Figure~\ref{fig:stencil3x3} shows the application of a $3\times3$ stencil
in this circumstance. The white squares indicate the \emph{internal} region
where the stencil is entirely within the array, and the grey squares indicate
the \emph{border} region, where part of the stencil falls lies outside of the
array boundary.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{images/sec-6/stencil3x3}
    \end{center}
    \caption[Application of a $3\times3$ stencil in the border region]
        {Application of a $3\times3$ stencil in the border region. Elements in
        the border region (grey), such as the indicated element at index
        \lstinline[style=inline,basicstyle=\sourcecodepro\footnotesize]{(Z:.2:.0)},
        have neighbouring elements which might fall outside the array bounds,
        whereas for elements in the internal region (white) the stencil lies
        entirely within the array.}
    \label{fig:stencil3x3}
\end{figure}

With the array sizes typical of GPU programs, the border region represents only
a tiny fraction of the entire array. A 512-pixel square image for example has
less than one percent of its pixels along the edge. This fact implies that for
optimal performance, we should avoid testing of the boundary each time we read
an element from the source array. For simple convolutions such as those used by
Canny (\S\ref{sec:canny}), this adds significant
overhead~\cite{Lippmeier:2011cd}. Separating computation of the border and
internal region is left for future work.

\note{tk: example showing performance of \code{map (+1)} versus the equivalent
of using a stencil: extra addressing arithmetic should be significant. For fluid
flow, the actual computation time (probably) greatly outweighs the extra
arithmetic.}


\subsection{Overlapping elements}

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{images/sec-6/stencil-sharing}
    \end{center}
    \caption[Overlapping elements in a $3\times3$ stencil]{Overlapping elements
        in a dense $3\times3$ stencil. Computation of each element requires nine
        reads from the source array. If the four elements cooperate to share the
        source data, instead of $4 \times 9 = 36$ loads, we require only 18.}
    \label{fig:stencil_sharing}
\end{figure}

Suppose we wish to apply a dense $3\times3$ stencil to a single internal point
in the array, and that every point in the array is utilised. Application of the
stencil requires at least nine elements of the array to be loaded, and one store
for the result. Since the computations of any point in the array do not depend
on any others, this is an embarrassingly parallel operation, and elements can be
computed in any order.

When evaluating on a GPU, however, for good absolute performance the thread
block should evaluate elements that are adjacent in memory.
Figure~\ref{fig:stencil_sharing} shows the evaluate of four horizontally
adjacent points. If we evaluate these points independently, we would need
$4 \times 9 = 36$ loads of the source array and four stores to the result.
However if the four threads cooperate and share data from the source array, this
requires only 18 loads, as well as the four stores.

Sharing of stencil elements can be achieved by having the thread block first
cooperatively read the stencil elements into shared memory (\S tk), and then
each thread computes its stencil function using these shared values. In
Accelerate, and the CUDA backend in particular, this is complicated by two
factors:
%
\begin{enumerate}
    \item Stencil patterns can be large: up to 9 elements in each dimension.
        Since the shared memory region on the GPU is usually very small, this
        can significantly limit the maximum thread block size. If the occupancy
        of the device is too low, there may be insufficient parallelism to hide
        global memory transfer latency.

    \item A stencil function does not need to use all elements defined in the
        pattern, so reading all elements into shared memory could waste more
        bandwidth than it saves. The pattern also does not need to be square nor
        symmetric, which complicates efficient cooperative loading of the
        common elements into shared memory.
\end{enumerate}

The implementation currently ignores this issue and does not explicitly share
elements between threads, instead relying on the cache. For older (compute 1.x)
devices that lack a traditional cache mechanism, we read the source array using
the texture cache (a holdover from the device's graphical heritage). It is left
to future work to implement sharing of stencil elements between threads, in
general or specialised for certain cases, such as those commonly used in image
convolutions.

\subsection{Stencil fusion}

\note{Fusion of \code{stencil s . map f} may be beneficial if \code{f} is
simple. Shares concerns with previous.}


\section{Canny edge detection}
\label{sec:canny}

The edge detection example applies the Canny algorithm~\cite{Canny:1986et} to
rectangular images of various sizes. The overall algorithm consists of seven
distinct phases, the first six of which are naturally data parallel and are
performed on the GPU. The last phase uses a recursive algorithm to ``connect''
the pixels that make up the output lines. In our implementation this phase is
performed on the CPU, which requires the image data to be transferred from the
GPU back to the CPU for processing. In the figure, this final phase accounts for
the non-linear slowdown visible with smaller image sizes. Also shown are the run
times for just the first six data parallel phases, which exhibit linear speedup.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.45\textwidth]{images/sec-6/canny/lena}
        \includegraphics[width=0.45\textwidth]{images/sec-6/canny/lena-edges}
    \end{center}
    \caption[Example of the Canny edge detection algorithm]{An example of the
        Canny algorithm applied to the Lena standard test image (left) which
        identifies edge pixels in the image (right).}
    \label{fig:lena}
\end{figure}

Neglecting the final phase, note that the data parallel phase is still slightly
slower than in OpenCV. As discussed in
section~\ref{sec:stencil_bounds_checking}, this is because the stencil kernels
in Accelerate currently make a test for every array access to see if the element
is in bounds, or if it lies outside the array and needs to be handled specially,
even though the vast majority of points in the array are far from the boundary.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/canny/canny}
    \end{center}
    \caption[Canny edge detection benchmarks]{Runtimes for the Canny edge
        detection algorithm, comparing the Accelerate kernels and whole program
        to sequential and parallel implementations. Note the log-log scale.}
    \label{fig:canny}
\end{figure}


\subsection{Stencil merging}

Two of the data parallel stages of the Canny algorithm are the calculation of
the image intensity gradients by convolving the image with the $3\times3$
kernels shown below. This convolution, known as the Sobel operator, is a
discrete differentiation operator that computes an approximation of the
horizontal and vertical image gradients.
%
\begin{equation*}
    \text{Sobel}_x =
        \begin{bmatrix*}[r]
          -1 & 0 & 1 \\
          -2 & 0 & 2 \\
          -1 & 0 & 1
        \end{bmatrix*}
    \qquad
    \text{Sobel}_y =
        \begin{bmatrix*}[r]
           1 &  2 &  1 \\
           0 &  0 &  0 \\
          -1 & -2 & -1
        \end{bmatrix*}
\end{equation*}

There is a significant amount of sharing between these two kernels, in terms of
the coefficients required to compute the derivative at each point, so it could
be beneficial to merge these two stencils. As with all shortcut fusion methods,
the system presented here does not combine multiple passes over the same array
into a single traversal that computes multiple results. We leave this to future
work, for example, in the style of data flow fusion~\cite{Lippmeier:2013vz}.


\section{Radix sort}

The radix sort benchmark implements a simple parallel radix sort algorithm as
described by \citet{Blelloch:1990vl} to sort an array by an integral key. We
compare our implementation of Blelloch's algorithm in Accelerate, shown in
Listing~\ref{lst:radixsort}, to one written in Nikola~\cite{Mainland:2010vj},
which is also an embedded language in Haskell for CUDA programming.%
\footnote{To be more specific, we estimate based on the results presented in the
paper~\cite{Mainland:2010vj} as Nikola no longer compiles with recent versions
of GHC, and the development version to replace it is not yet complete.
Nevertheless, results are informative as the same GPU, a Tesla T10, is used in
both cases.}
For this benchmark the Accelerate code is faster than Nikola, because Nikola is
limited to single kernel programs so must transfer every intermediate result
back to the host. Additionally, we are faster than a sequential radix sort
implementation using \texttt{Data.Vector} for as few as $\sim256$ elements,
whereas Nikola requires a dataset of about 32kB before the additional
parallelism of the GPU outperforms the sequential
version~\cite{Mainland:2010vj}.

\begin{lstlisting}[style=haskell
    ,float
    ,label=lst:radixsort
    ,caption={Radix sort algorithm in Accelerate}]
class Elt e => Radix e where
  passes :: e -> Int                            -- number of passes (bits) required to sort this key type
  radix  :: Exp Int -> Exp e -> Exp Int         -- extract the $n^{th}$ bit of the key

sortBy :: forall a r. (Elt a, Radix r)
       => (Exp a -> Exp r)
       -> Acc (Vector a)
       -> Acc (Vector a)
sortBy rdx arr = foldr1 (>->) (P.map radixPass [0..p-1]) arr    -- loop over bits of the key, low\ldots
  where                                                         -- \ldots bit first to maintain sort order
    p           = passes (undefined :: r)
    deal f x    = let (a,b) = unlift x in (f ==* 0) ? (a,b)

    radixPass k v =
      let k'    = unit (constant k)                             -- to create reusable kernels
          flags = A.map (radix (the k') . rdx) v                -- extract the sort key
          idown = prescanl (+) 0 . A.map (xor 1)        $ flags -- move elements to the beginning\ldots
          iup   = A.map (size v - 1 -) . prescanr (+) 0 $ flags -- \ldots end of the vector
          index = A.zipWith deal flags (A.zip idown iup)
      in
      permute const v (\ix -> index1 (index!ix)) v              -- move elements to new position
\end{lstlisting}

The @radixPass@ function sorts the vector @v@ based on the value of
bit @k@; moving elements with a zero at that bit to the beginning of the
vector and those with a one bit to the end. This simple algorithm requires
$b$ iterations to sort an array of elements whose keys are $b$-bits wide, and
each pass requires multiple traversals of the array.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/radixsort/radixsort}
    \end{center}
    \caption[Radix sort kernel benchmarks]{Kernel runtimes for the radix sort benchmark
        of signed 32-bit integers, comparing the Accelerate version to other
        parallel implementations. Note the log-log scale.}
    \label{fig:radixsort}
\end{figure}

As seen in Figure~\ref{fig:radixsort}, the absolute performance of this simple
algorithm, which sorts the array by a single bit at a time, is quite low.
Implementations of radix sort optimised for the CUDA
architecture~\cite{Satish:2009kx,Merrill:2011bz,ThrustAParallelT:ub}, are
approximately $10\times$ faster because they make efficient use of memory
bandwidth and on-chip shared memory to sort keys by 8-bits at a time. Such
implementations are, essentially, different algorithms than Blelloch's radix
sort algorithm that we have implemented here, but serve to illustrate the
absolute performance of the device. To address this we have implemented a
foreign function interface for Accelerate to take advantage of existing
high-performance libraries. The foreign function interface is orthogonal to the
work presented here of optimising programs written in the Accelerate language.


\section{Sparse-matrix vector multiplication}
\label{sec:smvm}

\marginnote{tk: redo bechmarks}
\newcommand\spyplot[1]{\parbox[c][1.1cm][c]{1.1cm}{\includegraphics[width=1cm]{images/sec-6/smvm/#1}}}

\begin{table}
\centering
\small
\begin{tabu}{X[-1cm]X[2lm]cX[-1cb]rrr} \toprule

\textbf{spyplot}
& \textbf{Description}
& \textbf{Dimensions}
& \textbf{Nonzeros (nnz/row)}
& \multicolumn{1}{c}{\rotatebox{90}{\textbf{CUSP}}}
& \multicolumn{1}{c}{\rotatebox{90}{\textbf{Accelerate}}}
& \multicolumn{1}{c}{\begin{sideways}%
    \parbox{\widthof{\bf Accelerate}}%
    {\centering\textbf{Accelerate no fusion}}%
    \end{sideways}}
\\ \midrule

\spyplot{dense2} % Dense
& Dense matrix in sparse format
& 2K $\times$ 2K
& 4.0M (2K)
& 14.48 & 14.62 & 3.41
\\

\spyplot{pdb1HYS} % Protein
& Protein data bank 1HYS
& 36K $\times$ 36K
& 4.3M (119)
& 13.55 & 13.65 & 0.26
\\

\spyplot{consph} % FEM / Spheres
& FEM concentric spheres
& 83K $\times$ 83K
& 6M (72)
& 12.63 & 9.03 & 4.70
\\

\spyplot{cant} % FEM / Cantilever
& FEM cantilever
& 62K $\times$ 62K
& 4M (65)
& 11.98 & 7.96 & 4.41
\\

\spyplot{pwtk} % Wind Tunnel
& Pressurised wind tunnel
& 128K $\times$ 128K
& 11.6M (53)
& 11.98 & 7.33 & 4.62
\\

\spyplot{rma10} % FEM/Harbour
& 3D CFD of Charleston harbour
& 47K $\times$ 47K
& 2.37M (50)
& 9.42 & 6.14 & 0.13
\\

\spyplot{qcd5_4} % QCD
& Quark propagators (QCD/LGT)
& 49K $\times$ 49K
& 1.9M (39)
& 7.79 & 4.66 & 0.13
\\

\spyplot{shipsec1} % FEM/Ship
& FEM ship section/detail
& 141K $\times$ 141K
& 3.98 (28)
& 12.28 & 6.60 & 4.47
\\

\spyplot{mac_econ_fwd500} % Economics
& Macroeconomics model
& 207K $\times$ 207K
& 1.27M (6)
& 4.59 & 0.90 & 1.06
\\

\spyplot{mc2dephi} % Epidemiology
& 2D Markov model of epidemic
& 526K $\times$ 526K
& 1.27M (4)
& 6.42 & 0.59 & 0.91
\\

\spyplot{cop20k_A} % FEM/Accelerator
& Accelerator cavity design
& 121K $\times$ 121K
& 2.62M (22)
& 5.41 & 3.08 & 2.92
\\

\spyplot{scircuit} % Circuit
& Motorola circuit simulation
& 171K $\times$ 171K
& 959k (6)
& 3.56 & 0.82 & 1.08
\\

\spyplot{webbase-1M} % Webbase
& Web connectivity matrix
& 1M $\times$ 1M
& 3.1M (3)
& 2.11 & 0.47 & 0.74
\\

\spyplot{rail4284} % LP
& Railways set cover constraint matrix
& 4K $\times$ 1.1M
& 11.3M (2825)
& 5.22 & 5.04 & 2.41
\\

\bottomrule
\end{tabu}
\caption[Sparse-matrix vector multiplication benchmarks]{Overview of sparse
matrices tested and results of the benchmark. Measurements are in GFLOPS/s
(higher is better).}
\label{tab:smvm_summary}
\end{table}


This benchmark considers the multiplication of sparse matrices in compressed row
format (CSR)~\cite{Chatterjee:1990vj} with a dense vector. This matrix format
consists of an array of the non-zero elements paired with their column index,
together with a segment descriptor recording the number of non-zeros in each
row. The corresponding Accelerate code is shown in Listing~\ref{lst:smvm}.
Table~\ref{tab:smvm_summary} compares Accelerate to the CUSP
library~\cite{Bell:2008wc,Bell:2009bl}; a special purpose library for sparse
matrix operations. Using a 14 matrix corpus derived from a variety of
application domains~\cite{Williams:2009cy}, we compare against CUSP for
compressed row format matrices.

\begin{lstlisting}[style=haskell
    ,float
    ,label=lst:smvm
    ,caption={Sparse-matrix vector multiplication in Accelerate}]
type SparseVector e = Vector (Int, e)                   -- column index and value
type SparseMatrix e = (Segments Int, SparseVector e)    -- length of each row

smvm :: (Elt a, IsNum a) => Acc (SparseMatrix a) -> Acc (Vector a) -> Acc (Vector a)
smvm smat vec
  = let (segd, svec)    = unlift smat
        (inds, vals)    = A.unzip svec
        vecVals         = gather inds vec
        products        = A.zipWith (*) vecVals vals
    in
    foldSeg (+) 0 products segd
\end{lstlisting}

Compared to our previous work~\cite{Chakravarty:2011fr} the fusion
transformation compresses the program into a single segmented reduction. As
predicted, the corresponding reduction in memory bandwidth means that Accelerate
is on par with the CUSP library for several of the test matrices. In a balanced
machine SMVM should be limited by memory throughput, and a dense matrix in
sparse format should provide an upper bound on performance because loops are
long running and accesses to the source vector are contiguous and have high
re-use. We see that Accelerate with array fusion not only achieves this expected
performance limit, but is also slightly faster than the CUSP implementation.

\subsection{Segment startup}

To maximise global memory throughput the skeleton code ensures that the vector
\marginnote{this is very rough}
read of each matrix row is coalesced and aligned to the warp boundary. Since
Accelerate does not require the starting element of the reduction to be a
neutral element, threads must also ensure they initialise their local sum from
the input array (\ref{sec:non-neutral_starting_elements}).
Listing~\ref{lst:smvm_cuda} shows the generated CUDA code for the first
sequential phase of the cascaded tree-reduction algorithm (see
\S\ref{sec:algorithm_cascading}). Figure~tk illustrates the different cases that
must be considered.

\begin{lstlisting}[style=cuda
    ,float
    ,firstnumber=44
    ,label=lst:smvm_cuda
    ,caption={Generated CUDA code for sparse-matrix vector multiplication}]
if (num_elements > warpSize) {
    ix = start - (start & warpSize - 1) + thread_lane;                  (@* \label{lst:smvm_cuda_boundary} *@)
    if (ix >= start) {                                                  (@* \label{lst:smvm_cuda_warp_start} *@)
        const Int64 v3 = ix;
        const int v4 = toIndex(shIn2, shape(v3));
        const int v5 = toIndex(shIn1, shape((Int64) arrIn2_a1[v4]));
        const int v6 = toIndex(shIn2, shape(v3));

        y0 = arrIn1_a0[v5] * arrIn2_a0[v6];
    }
    if (ix + warpSize < end) {                                          (@* \label{lst:smvm_cuda_warp_next} *@)
        const Int64 v3 = ix + warpSize;
        const int v4 = toIndex(shIn2, shape(v3));
        const int v5 = toIndex(shIn1, shape((Int64) arrIn2_a1[v4]));
        const int v6 = toIndex(shIn2, shape(v3));

        x0 = arrIn1_a0[v5] * arrIn2_a0[v6];
        if (ix >= start) {                                              (@* \label{lst:smvm_cuda_branch} *@)
            y0 = x0 + y0;
        } else {
            y0 = x0;
        }
    }
    for (ix += 2 * warpSize; ix < end; ix += warpSize) {                (@* \label{lst:smvm_cuda_loop} *@)
        const Int64 v3 = ix;
        const int v4 = toIndex(shIn2, shape(v3));
        const int v5 = toIndex(shIn1, shape((Int64) arrIn2_a1[v4]));
        const int v6 = toIndex(shIn2, shape(v3));

        x0 = arrIn1_a0[v5] * arrIn2_a0[v6];
        y0 = x0 + y0;
    }
} else if (start + thread_lane < end) {                                 (@* \label{lst:smvm_cuda_unaligned} *@)
    const Int64 v3 = start + thread_lane;
    const int v4 = toIndex(shIn2, shape(v3));
    const int v5 = toIndex(shIn1, shape((Int64) arrIn2_a1[v4]));
    const int v6 = toIndex(shIn2, shape(v3));

    y0 = arrIn1_a0[v5] * arrIn2_a0[v6];
}
\end{lstlisting}

To initialise the warp-aligned read the location of the warp boundary before the
start of the source vector is calculated (line~\ref{lst:smvm_cuda_boundary}),
offset by this thread's warp lane index. If the thread lies within the source
vector its starting value can be initialised
(line~\ref{lst:smvm_cuda_warp_start}). Reading elements from the second warp
(line~\ref{lst:smvm_cuda_warp_next}) is aligned and coalesced to the warp
boundary so completes in a single global memory transfer, but requires a
divergent branch (line~\ref{lst:smvm_cuda_branch}) depending on whether this is
the first value read or not. Once all threads have initialised their local sum,
the serial reduction phase completes (line~\ref{lst:smvm_cuda_loop}), which is
the same as we saw in the dot product example (Listing~\ref{lst:dotp_cuda}). If
there is less than a warp's worth of data to be reduced, the elements are simply
read unaligned (line~\ref{lst:smvm_cuda_unaligned}). If we do not do this, the
first threads of the warp may not have been initialised, which is a requirement
for the second cooperative tree-reduction phase of the algorithm, as it combines
values for indices $\left[0,n\right)$ into index zero.

Matrices such as FEM/Spheres, with few non-zero elements per row ($\lesssim 2
\times \text{warp size} = 64$) exhibit a drop in performance relative to CUSP,
because this extra startup cost necessary to align global memory reads to the
warp boundary can not be amortised over the row length.

Matrices such as Epidemiology, with large vectors and few non-zeros per row,
exhibit low flop:byte ratio and are poorly suited to the CSR format, with all
implementations performing well below peak. This highlights the nature of sparse
computations and the reason the CUSP library supports several algorithms and
matrix formats.

% This may be related to the way the skeleton code ensures that
% the vector read of each row is coalesced and aligned to the warp boundary to
% maximise global memory throughput, but is then not able to amortize this extra
% startup cost over the row length.
% 
% Since Accelerate does not require the starting element of the reduction to be a
% neutral element (see \S\ref{sec:non-neutral_starting_elements})
% 
% The regression relative to
% our previous result will be investigated. Nevertheless,

\section{Discussion}
% \subsection{Related Work}
% \subsection{Future Work}

% \begin{itemize}
%     \item performance \& analysis
%     \begin{itemize}
%       \item What is an optimisation? Describe the metrics considered.
%         \begin{itemize}
%           \item wall-clock time, parallel speedup??
%           \item instruction count
%           \item memory traffic (load/store/coalescing)
%           \item memory size (heap, register count, shared memory)
%           \item program size (number of parallel steps)
%           \item code size (generated binary, compilation time c.f. code complexity)
%         \end{itemize}
%     \end{itemize}
%
%     \item expressiveness / example programs
%     \item difficulties
%     \begin{itemize}
%         \item code generation
%         \item unintentional nesting
%     \end{itemize}
% \end{itemize}

