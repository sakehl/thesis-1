%
% analysis, example programs
%

\chapter{Results}
\label{ch:results}
% \epigraph{Dionysus [is] the Master of Illusions, who could make a vine grow out
% of a ship's plank, and in general enable his votaries to see the world as the
% world's not.}%
% {\textsc{---e.\ r.\ dodds}\\\textit{The Greeks and the Irrational}}

% \epigraph{If the immutable appears recast, then you yourself have been
% transformed.}%
% {\textsc{---r.\ scott bakker}\\\textit{The Judging Eye}}

\epigraph{Logic merely enables one to be wrong with authority}%
{\textsc{---the doctor}} %\\\textit{The Wheel in Space}}


The previous chapters discussed the design, implementation and optimisation of
the Accelerate language and CUDA backend. This chapter analyses the performance
of the implementation. Benchmarks were conducted on a single Tesla T10 processor
(compute capability 1.3, 30 multiprocessors = 240 cores at 1.3GHz, 4GB RAM)
backed by two quad core Xeon E5405 CPUs (64-bit, 2GHz, 8GB RAM) running
GNU/Linux (Ubuntu 12.04 LTS, CUDA 5.0).


\section{Runtime overheads}

Runtime program optimisation, code generation, kernel loading, data transfer,
and so on can contribute significant overhead to short lived GPU computations.
Accelerate mitigates these overheads via caching and memoisation. For example,
the first time a particular expression is executed it is compiled to CUDA code,
which is reused for subsequent invocations. This section analyses those
overheads, so that they can be factored out later when we discuss the effects of
the program optimisations on kernel runtimes of the benchmark programs.


\subsection{Program optimisation}

% tk: performance and benchmarks for the fusion pass
%   - actual runtime
%   - tick counters for the final version and old one with extend/cunctation
%     separation


The Floyd-Warshall algorithm for finding the shortest paths in a directed graph,
discussed in Section~\ref{sec:floyd_warshall}, was found to cause serious
problems with the original implementation of the optimisations discussed in this
thesis. Thus, changes were made to ensure that the complexity of the
optimisations are not exponential in the number of terms in the program. In
particular, the Floyd-Warshall program consists of a sequence of let bindings to
producer operations, so the handling of let bindings
(\S\ref{sec:binder_elimination}) and the representation of producers that
separates the delayed array representation from the captured environment terms
(\S\ref{sec:representing_producers}) was found to be crucial.
Table~\ref{tab:floyd_warshal_opt} shows the time to optimise the Floyd-Warshall
program with and without these changes of the optimisation pipeline.


\begin{table}
    \centering
    \begin{tabu}{rrrr}
\toprule
\textbf{k}
    & \multicolumn{1}{c}{Final (ms)}
    & \multicolumn{1}{c}{no split opt (ms)}
    & \multicolumn{1}{c}{no binder opt (ms)}
    \\

\midrule

1   &
    &
    &
    \\

2   &
    &
    &
    \\

4   &
    &
    &
    \\

8   &
    &
    &
    \\
\bottomrule

    \end{tabu}
    \caption{Optimisation time for the Floyd-Warshall algorithm}
    \label{tab:floyd_warshal_opt}
\end{table}


\subsection{Memory management \& data transfer}

Figure~\ref{fig:data_transfer} shows the time to transfer Accelerate arrays to
and from the device. Observe that the data transfer time can account for a
significant portion of the overall execution time for some benchmarks.
% The measured times allow the Accelerate runtime system to avoid allocating data
% on every transfer to the device.

% tk: some graphs of memory transfer times or bandwith vs. array size
% tk: importance of using weak pointers, so data transfers ``escape'' the black
%     box of evaluating the AST --> hashcat.
% tk: importance of the nursery for reducing calls to malloc/free

\subsection{Code generation \& compilation}

Table~\ref{tab:compilation_times} lists the code generation and compilation
times for each of the benchmark programs considered in this chapter. Compilation
times are measured by saving the Accelerated generated code to file and
compiling separately with the @nvcc@ command line tool. Times are the average of
tk evaluations.

\begin{table}
    \centering
    \begin{tabu}{lrr}
\toprule
\textbf{Benchmark}
    & \multicolumn{1}{c}{Code generation (ms)}
    & \multicolumn{1}{c}{Compilation (ms)}
    \\
    \midrule

Dot Product
    &
    &
    \\

Black-Scholes
    &
    &
    \\

Mandelbrot
    &
    &
    \\

N-Body
    &
    &
    \\

Fluid Flow
    &
    &
    \\

Canny
    &
    &
    \\

Radix sort
    &
    &
    \\

SMVM
    &
    &
    \\

Hashcat
    &
    &
    \\

Ray
    &
    &
    \\

Floyd-Warshall
    &
    &
    \\

K-Means
    &
    &
    \\

\bottomrule

    \end{tabu}
    \caption{Code generation and compilation times for the benchmark programs}
    \label{tab:compilation_times}
\end{table}

% tk: measure \& tabulate some code generation / compilation times
% tk: persistent cache, perhaps just a mention?


\section{Dot product}
\label{sec:dotp}

Dot product, or scalar product, is an algebraic operation that takes two
equal-length sequences of numbers and returns a single number by computing the
sum of the products of the corresponding entries in the two sequences. Dot
product can be defined in Accelerate using the code shown in
Listing~\ref{lst:dotp}, and seen previously.

\begin{lstlisting}[style=haskell_float
    ,label=lst:dotp
    ,caption={Vector dot-product}]
dotp :: Acc (Vector Float) -> Acc (Vector Float) -> Acc (Scalar Float)
dotp xs ys = A.fold (+) 0                       -- sum result of\ldots
           $ A.zipWith (*) xs ys                -- \ldots element-wise multiplying inputs
\end{lstlisting}

Figure~\ref{fig:dotp} show the result of running this code, compared to several
other sequential and parallel implementations. The \texttt{Data.Vector} baseline
is sequential code produced by \index{fusion!stream}stream
fusion~\cite{Coutts:2007kp}, running on the host CPU. The \texttt{Repa} version
runs in parallel on all eight cores of the host CPU, using the fusion method of
\index{fusion!delayed arrays}delayed arrays~\cite{Keller:2010er}. The
\texttt{NDP2GPU}~\cite{Bergstrom:2012bi} version compiles NESL
code~\cite{Blelloch:1995ut} down to CUDA. The performance of NDP2GPU suffers
because it uses the legacy NESL compiler for the front-end, which introduces
redundant administrative operations that are not strictly needed when evaluating
a dot product.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/dotp/dotp}
    \end{center}
    \caption[Vector dot product kernel benchmarks]{Kernel runtimes for vector
        dot product, in Accelerate with and without optimisations, compared to
        other parallel GPU and CPU implementations. Note the log-log scale.}
    \label{fig:dotp}
\end{figure}

Without optimisations the Accelerate version executes in approximately twice the
time of the CUBLAS version. Since the individual aggregate operations consist of
only a single arithmetic operation each, the problem can not be a lack of
sharing.


\subsection{Too many kernels}

The slow-down in the unoptimised version is due to Accelerate generating one GPU
kernel function for each aggregate operation in the source program. The use of
two separate kernels requires the intermediate array produced by @zipWith@ to be
constructed in GPU memory before being immediately read back by @fold@. In
contrast, the CUBLAS version uses only a single kernel. As this is a simple
memory bound benchmark, we would expect the lack of fusion to double the
runtime, which is what we observe.

The fused version of dot product combines the aggregate computations by
embedding a function of type @(sh -> a)@ into the reduction, represented
here as the second argument to the constructor @Delayed@. This scalar
function does the work of element wise multiplying the two input vectors, and is
performed on-the-fly as part of the reduction kernel instead of requiring an
intermediate array. See Section~\ref{sec:array_fusion} for more information.
Applied to two manifest arrays, the following embedded program will be executed:

\begin{lstlisting}[style=haskell]
let a0 = use (Array ...) in
let a1 = use (Array ...) in
fold (\x0 x1 -> x0 + x1) 0.0
     (Delayed (intersect (shape a0) (shape a1))   -- extent of the input array
              (\x0 -> (a0!x0) * (a1!x0)))         -- function to generate a value at each index
\end{lstlisting}

While the fused Accelerate version is $30\times$ faster than the sequential
version executed on the CPU, it is still approximately $20\%$ slower than the
\marginnote{tk:check this}
hand-written CUBLAS version. As dot product is a computationally simple task,
any additional overheads in the implementation will be significant. To see why
this version is still slightly slower, we continue by analysing the generated
code to inspect it for sources of overhead.


% \subsection{Duplicate loop counters}
%
% The fused dot product operation will only perform the element wise
% multiplication of the two input arrays in the first step of the tree reduction
% (\S\ref{sec:parallel_reduction}). This occurs in the phase of the cascaded
% algorithm when individual threads sequentially sum multiple elements. After
% embedding the fused producer, the following CUDA code is generated for the inner
% loop of this step:
% %
% % pookie is the bestest
% % bubbaboo ish silly
% % kekekee
% % i can't put smiley faces in cause weird things happen
% % hehe^{this is my thesis
% % please like it
% % i worked really hard
% % and writed a lots
% % }<++>
% %
% %
% \begin{lstlisting}[style=cuda
%     ,firstnumber=18
%     ,label=lst:dotp_cuda
%     ,caption={Generated CUDA code for the first step of fused dot product}]
% for (ix += gridSize; ix < shapeSize; ix += gridSize) {
%     const Int64 v2 = ix;
%     const int v3 = toIndex(shIn0, shape(v2));
%     const int v4 = toIndex(shIn1, shape(v2));
%
%     x0 = arrIn0_a0[v3] * arrIn1_a0[v4];
%     y0 = x0 + y0;
% }
% \end{lstlisting}
% %
% We have four loop counters: @ix@, @v2@, @v3@ and @v4@ ---
% two for the source arrays and two to convert between the multidimensional and
% linear representations. These counters contain the same value and are
% incremented in lockstep. In addition to the superfluous arithmetic, the
% duplication of counters unnecessarily increases register pressure.
%
% \marginnote{this was unexpected}
% The corresponding section of PTX~\cite{NVIDIA:2012vj} code is for this loop is
% shown in Listing~\ref{lst:dotp_ptx}. To retrieve the data from the first input
% array, the input array pointer is retrieved (line~\ref{lst:dotp_ptx_ldparam}),
% the offset stored in register @rd16@ added to it
% (line~\ref{lst:dotp_ptx_add}), and then the value read from global memory at
% this address (line~\ref{lst:dotp_ptx_ldglobal}). Happily, we note that
% retrieving the second input element \emph{also} uses the offset stored in
% register @rd16@ (line~\ref{lst:dotp_ptx_ldglobal2}).
% %
%
% \begin{lstlisting}[style=ptx
%     ,float
%     ,firstnumber=103
%     ,label=lst:dotp_ptx
%     ,caption={[PTX code for the first step of fused dot product]
%         PTX code for the first step of fused dot product (sm13)}]
%  //  18          for (ix += gridSize; ix < shapeSize; ix += gridSize) {
%         cvt.u32.u16     %r10, %nctaid.x;
%         mul.lo.u32      %r11, %r10, %r1;
%         add.s32         %r12, %r11, %r7;
%         mov.s32         %r13, %r12;
%         setp.le.s32     %p2, %r8, %r12;
%         `%p2 bra        $Lt_0_13058;
%         cvt.s64.s32     %rd12, %r12;
%         cvt.s64.u32     %rd13, %r11;
% $Lt_0_14082:
%  //<loop> Loop body line 18, nesting depth: 1, estimated iterations: unknown
%         .loc    16      24      0
%  //  20              const int v3 = toIndex(shIn0, shape(v2));
%  //  21              const int v4 = toIndex(shIn1, shape(v2));
%  //  22
%  //  23              x0 = arrIn0\_a0[v3] * arrIn1\_a0[v4];
%  //  24              y0 = x0 + y0;
%         cvt.s32.s64     %rd14, %rd12;                           (@* \label{lst:dotp_ptx_cvt1} *@)
%         cvt.s32.s64     %r14, %rd14;
%         cvt.s64.s32     %rd15, %r14;                            (@* \label{lst:dotp_ptx_cvt3} *@)
%         mul.wide.s32    %rd16, %r14, 4;
%         .loc    16      17      0
%         ld.param.u64    %rd9, [__cudaparm_foldAll_arrIn0_a0];   (@* \label{lst:dotp_ptx_ldparam} *@)
%         .loc    16      24      0
%         add.u64         %rd17, %rd16, %rd9;                     (@* \label{lst:dotp_ptx_add} *@)
%         ld.global.f32   %f4, [%rd17+0];                         (@* \label{lst:dotp_ptx_ldglobal} *@)
%         .loc    16      17      0
%         ld.param.u64    %rd8, [__cudaparm_foldAll_arrIn1_a0];
%         .loc    16      24      0
%         add.u64         %rd18, %rd16, %rd8;
%         ld.global.f32   %f5, [%rd18+0];                         (@* \label{lst:dotp_ptx_ldglobal2} *@)
%         mad.f32         %f3, %f4, %f5, %f3;
%         add.s32         %r13, %r13, %r11;
%         add.s64         %rd12, %rd12, %rd13;
%         setp.gt.s32     %p3, %r8, %r13;
%         `%p3 bra        $Lt_0_14082;
%         bra.uni         $Lt_0_13058;
% $Lt_0_13314:
%         mov.f32         %f3, %f6;
% $Lt_0_13058:
%         .loc    16      27      0
%  //  25          }
% \end{lstlisting}
%
% In this case the CUDA compiler was able to coalesce our four counters into a
% single counter. This is because our definition of @toIndex@ specialised for
% one-dimensional indices @DIM1@ does \emph{not} do bounds checking:
% %
% \begin{lstlisting}[style=cuda]
% template <>
% static __inline__ __device__ Ix toIndex(const DIM1 sh, const DIM1 ix)
% {
%     return ix;
% }
% \end{lstlisting}
% %
% Since we do not check that the current index @ix@ is within bounds of the
% array shape @sh@, the compiler can see that the definitions of @v3@
% and @v4@ in Listing~\ref{lst:dotp_cuda} are identical. Similarly
% one-dimensional indices are just integers, and so the one-dimensional instance
% of @shape@ is the identity function. For higher dimensional shapes,
% however, this is not the case and @toIndex@ and @shape@ do real work
% to their arguments. As the shape @shIn0@ and @shIn1@ are inputs
% arguments to the kernel, the compiler will not be able to determine they are
% equivalent and so the counters would not be combined.
%
% Why don't we perform bounds checks in @toIndex@? Implementing exceptions in
% a massively parallel architecture is difficult, and support for throwing
% exceptions from kernel functions was only recently added for devices of compute
% capability 2.0 and later~\cite{NVIDIA:2012wf}. % [\SB.15]


\subsection{64-bit Arithmetic}

The host architecture that the Haskell program executes on is likely to be a
64-bit processor. If this is the case, then manipulating machine word sized
types such as @Int@ in embedded code must generate instructions to manipulate
64-bit integers in the GPU\@. On the other hand, CUDA devices are at their core
32-bit processors, and are thus optimised for 32-bit arithmetic. For example, our
Tesla GPU with compute capability 1.3 has a throughput of eight 32-bit
floating-point add, multiply, or multiply-add operations per clock cycle per
multiprocessor, but only a single operation per cycle per multiprocessor of the
64-bit equivalents of these~\cite{NVIDIA:2012wf}. % [\S5.4.1]
Additionally, while not explicitly stated, it appears that 64-bit integer
instructions are not natively supported on any architectures, and require
multiple clock cycles per instruction per multiprocessor.

The delayed producer function that is embedded into the consumer @fold@ kernel
corresponds to the operation @zipWith (*)@, and has concrete type
@(Exp (Z:.Int) -> Exp Float)@. Since this is executed on a 64-bit host machine,
the indexing operations must generate embedded code for 64-bit integers. See
Section~\ref{sec:instantiating_skeletons} for more information on code
generation, which produces the following CUDA code:
%
\begin{lstlisting}[style=haskell]
const Int64 v1 = ({ assert(ix >= 0 && ix < min(shIn1_0, shIn0_0)); ix; });
y0 = arrIn1_0[v1] * arrIn0_0[v1];
\end{lstlisting}
%
This is translated by the CUDA compiler into the following PTX assembly
code~\cite{NVIDIA:2012vj}, which is the lowest level representation accessible
to the developer:
%
\begin{lstlisting}[style=ptx]
//  13      const Int64 v1 = ({ assert(ix >= 0 && ix < min(shIn1_0, shIn0_0)); ix; });
        cvt.s64.s32     %rd5, %r5;
        mov.s64         %rd6, %rd5;
//  14
//  15      y0 = arrIn1_0[v1] * arrIn0_0[v1];
        mov.s64         %rd7, %rd6;
        mul.lo.u64      %rd8, %rd7, 4;
        ld.param.u64    %rd9, [__cudaparm_foldAll_arrIn1_0];
        ld.param.u64    %rd10, [__cudaparm_foldAll_arrIn0_0];
        add.u64         %rd11, %rd8, %rd9;
        ld.global.f32   %f1, [%rd11+0];
        add.u64         %rd12, %rd8, %rd10;
        ld.global.f32   %f2, [%rd12+0];
        mul.f32         %f3, %f1, %f2;
\end{lstlisting}
%
The first instruction converts the loop variable @ix@ --- which was not shown
but is declared with the C @int@ type --- to a 64-bit integer using the
@cvt.s64.s32@ instruction. The @assert@ function does not contribute anything in
this case as the kernel was not compiled with debugging enabled. Instructions in
PTX assembly are appended with the type of their operands, where @s64@ and @u64@
represent signed and unsigned 64-bit integers respectively, so it is clear that
the instructions in this fragment are manipulating 64-bit values.

In this instance, 64-bit arithmetic was introduced through the use of a
one-dimensional shape index, which has Haskell type @(Z :. Int)@ and corresponds
to a 64-bit wide integer on our host machine. Adding support for non-@Int@ shape
dimensions is left for future work.


\subsection{Non-neutral starting elements}
\label{sec:non-neutral_starting_elements}

In order to support efficient parallel execution, the @fold@ function in
Accelerate requires the combination function to be an associative operator.
However, we do not require the starting value to be a neutral element of the
combination function. For example, @fold (+) 42@ is valid in Accelerate, even
though @42@ is not the neutral element of addition.

While convenient for the user, this feature complicates the implementation. In
particular, threads can not initialise their local sum with the neutral element
during the first sequential reduction phase, and during the second cooperative
reduction step must be sure to only include elements from threads that were
properly initialised from the input array. See
Section~\ref{sec:parallel_reduction} for more information on the implementation
of parallel reductions in CUDA\@. Both of these restrictions necessitate
additional bounds checks, which increases overhead from ancillary instructions
that are not loads, stores, or arithmetic for the core computation. As the
summation reduction has low arithmetic intensity, which is the limiting factor
in the performance of this kernel~\cite{Harris:2007te}, additional bounds checks
further reduce performance.

It is left for future work to have operations such as @fold@ and @scan@ observe
when the combination function and starting element form a monoid, and provide
optimised kernels that avoid these extra administrative instructions. This is
the approach taken by, for example, Thrust~\cite{ThrustAParallelT:ub}.


\subsection{Kernel specialisation}

To further reduce instruction overhead, it is possible to completely unroll the
reduction by specialising the kernel for a specific block size. In CUDA this can
be achieved through the use of C++ templates. Branches referring to the template
parameter will be evaluated at compile time, resulting in a very efficient inner
loop.
%
\begin{lstlisting}[style=cuda]
template <unsigned int blockSize> __global__ void reduce(...) {
    ...
    if (blockSize > 512) {
    }
    if (blockSize > 256) {
    }
    ...
\end{lstlisting}

This technique has shown to produce significant gains in
practice~\cite{Harris:2007te}, although requires compiling a separate kernel for
each thread block size we wish to specialise for. Accelerate can achieve this
kind of kernel specialisation because the CUDA code is generated at program
runtime. However, since compilation also occurs at program runtime, this adds
significant additional runtime overhead. Since compiled kernels are cached and
reused, if we know that the reduction will be computed many times, the extra
compilation overhead can be amortized by the improved kernel performance, and
thus may be worthwhile. See Section~\ref{sec:dynamic_compilation} for more
information on the mechanism of compilation and code memoisation. Implementing
kernel specialisations, and evaluating their effectiveness, is left to future
work.


\section{Black-Scholes option pricing}
\label{sec:blackscholes}

The Black-Scholes algorithm is a partial differential equation for modelling the
evolution of a European-style stock option price under certain assumptions. The
corresponding Accelerate program is shown in Listing~\ref{lst:blackscholes}.
Given a vector of triples of the underlying stock price, strike price, and time
to maturity in years, the Black-Scholes formula computes the price of a call and
put option. The function @callput@ evaluates the Black-Scholes formula for
a single triple, and @blackscholes@ maps it over a vector of triples such
that all individual applications of the formula are executed in parallel.

\begin{lstlisting}[style=haskell_float
    ,float
    ,label=lst:blackscholes
    ,caption={Black-Scholes option pricing}]
horner :: Num a => [a] -> a -> a
horner coeff x =
  let madd a b  = a + x*b
  in
  x * foldr1 madd coeff

cnd' :: Floating a => a -> a
cnd' d =
  let poly      = horner coeff
      coeff     = [0.31938153, -0.356563782, 1.781477937, -1.821255978, 1.330274429]
      rsqrt2pi  = 0.39894228040143267793994605993438
      k         = 1.0 / (1.0 + 0.2316419 * abs d)
  in
  rsqrt2pi * exp (-0.5*d*d) * poly k

blackscholes :: Acc (Vector (Float, Float, Float)) -> Acc (Vector (Float, Float))
blackscholes = A.map callput
  where
  callput x =
    let (price, strike, years) = A.unlift x
        r       = A.constant riskfree
        v       = A.constant volatility
        v_sqrtT = v * sqrt years
        d1      = (log (price / strike) + (r + 0.5 * v * v) * years) / v_sqrtT
        d2      = d1 - v_sqrtT
        cnd d   = let c = cnd' d in d >* 0 ? (1.0 - c, c)
        cndD1   = cnd d1
        cndD2   = cnd d2
        x_expRT = strike * exp (-r * years)
    in
    A.lift ( price * cndD1 - x_expRT * cndD2                    -- call price
           , x_expRT * (1.0 - cndD2) - price * (1.0 - cndD1))   -- put price
\end{lstlisting}

Figure~\ref{fig:blackscholes} shows the result of running this code compared to
the implementation that ships with the CUDA SDK. Without optimisations, the
Accelerate version is almost twenty times slower than the equivalent
implementation in CUDA\@. As @blackscholes@ includes only one collective array
operation, the problem can not be a lack of fusion.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/black-scholes/black-scholes}
    \end{center}
    \caption[Black-Scholes kernel benchmarks]{Kernel runtimes for Black-Scholes
        options pricing, in Accelerate with and without optimisations, compared
        to a hand-written CUDA version. Note the log-log scale.}
    \label{fig:blackscholes}
\end{figure}

\subsection{Too little sharing}

The function @callput@ from Listing~\ref{lst:blackscholes} includes a
significant amount of sharing: the helper functions @cnd'@ and hence
@horner@ are used twice --- for @d1@ and @d2@ --- and its
argument @d@ is used multiple times in the body. Furthermore, the
conditional expression @d >* 0 ? (1 - c, c)@ results in a branch that,
without sharing, results in a growing number of predicated instructions that
leads to a large penalty on the SIMD architecture of the GPU\@.

Without sharing the generated code requires 5156 instructions to calculate 128
options (2 thread blocks of 64 threads each) and results in significant warp
divergence which serialises portions of the execution. With sharing recovery the
generated code requires 628 instructions (one thread block of 128 threads) and
is actually slightly faster than the reference CUDA version because the latter
contains a common subexpression that was not spotted by the programmer and not
eliminated by the CUDA compiler. The common subexpression performs a single
multiplication. The CUDA version executes 652 instructions (128 threads per
block).


\section{Mandelbrot fractal}
\label{sec:mandelbrot}

The Mandelbrot set is generated by sampling values $c$ in the complex plane and
determining whether under iteration of the complex quadratic polynomial:
\[
z_{n+1} = z_{n}^{2} + c
\]
that the magnitude of $z$ (written $\left| z_{n} \right|$) remains bounded
however large $n$ gets. Images of the Mandelbrot set are created such that each
pixel corresponds to a point $c$ in the complex plane, and its colour depends on
the number of iterations $n$ before the relation diverges, where $z_{0} = c$.
The set of points forming the boundary of this relation forms the distinctive
and easily recognisable fractal shape shown in Figure~\ref{fig:mandelbrot}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/mandelbrot/mandelbrot}
    \end{center}
    \caption[The Mandelbrot fractal]{Image of a Mandelbrot set with a
        continuously coloured environment. Each pixel corresponds to a point $c$
        in the complex plane, and its colour depends the number of iterations
        $n$ before the relation diverges. Centre coordinate
        $\left( -0.5+0i \right)$, horizontal width 3.2.}
    \label{fig:mandelbrot}
\end{figure}

\begin{table}
\centering
\small
\begin{tabu}{lrrrr}
\toprule

                        & \multicolumn{1}{c}{\textbf{Time}}
                        & \multicolumn{1}{c}{\textbf{Bandwidth}}
                        & \multicolumn{1}{c}{\textbf{Step}}
                        & \multicolumn{1}{c}{\textbf{Cumulative}} \\

\textbf{Benchmark}      & \multicolumn{1}{c}{\textbf{(ms)}}
                        & \multicolumn{1}{c}{\textbf{(GB/s)}}
                        & \multicolumn{1}{c}{\textbf{Speedup}}
                        & \multicolumn{1}{c}{\textbf{Speedup}} \\\midrule

Accelerate              &
                        &
                        &
                        & \\

Accelerate (+sharing)   &
                        &
                        &
                        & \\

Accelerate (+fusion)    &
                        &
                        &
                        & \\

Accelerate (+loop recovery)
                        &
                        &
                        &
                        & \\

Accelerate (+iteration)
                        &
                        &
                        &
                        & \\

CUDA (limit)            & 14.0
                        & 0.55
                        &
                        & \\

CUDA (avg)              & 5.0
                        & 1.53
                        & 2.79$\times$
                        & 2.79$\times$ \\

CPU (avg)               & 1389
                        & 0.006
                        &
                        & \\

\bottomrule
\end{tabu}
\caption[Mandelbrot fractal kernel benchmarks]{Mandelbrot fractal benchmarks in
    Accelerate with and without optimisations, compared to a hand written CUDA
    version. The benchmark computes the Mandelbrot set shown in
    Figure~\ref{fig:mandelbrot}, which has centre coordinates $-0.7 + 0i$ and
    width $3.067$. The CUDA (limit) benchmark computes every pixel to the
    maximum iteration count.}
\label{tab:mandelbrot}
\end{table}

Table~\ref{tab:mandelbrot} shows the results of the Mandelbrot program. As
expected, without fusion the performance of this benchmark is poor because
storing each step of the iteration saturates the memory bus, to the point that
reducing arithmetic intensity with sharing recovery provides no improvement.


\subsection{Fixed unrolling}

In order to meet the restrictions of what can be efficiently executed on
specialised hardware such as GPUs, Accelerate did not directly support any form
of iteration or recursion. To implement the recurrence relation we instead
define each step of computing $z_{n+1}$ given $z_n$ and $c$ as a collective
operation, and apply the operation a fixed number of times. The trick then is to
keep a pair @(z, i)@ for every point on the complex plane, where @i@ is the
iteration at which the point @z@ diverged. The code implementing this is shown
in Listing~\ref{lst:mandelbrot}.

\begin{lstlisting}[style=haskell_float
    ,label=lst:mandelbrot
    ,caption={Mandelbrot set generator, using fixed unrolling}]
mandelbrot
    :: forall a. (Elt a, IsFloating a)
    => Int
    -> Int
    -> Int
    -> Acc (Scalar (View a))
    -> Acc (Array DIM2 Int32)
mandelbrot screenX screenY depth view
  = P.snd . A.unzip
  $ P.foldr ($) zs0                                                                    -- (1)
  $ P.take depth (repeat step)
  where
    -- The view plane
    (xmin,ymin,xmax,ymax)     = unlift (the view)
    sizex                     = xmax - xmin
    sizey                     = ymax - ymin
    viewx                     = constant (P.fromIntegral screenX)
    viewy                     = constant (P.fromIntegral screenY)

    step :: Acc (Array DIM2 (Complex a, Int32))
         -> Acc (Array DIM2 (Complex a, Int32))
    step = A.zipWith iter cs

    iter :: Exp (Complex a) -> Exp (Complex a, Int32) -> Exp (Complex a, Int32)        -- (2)
    iter c zi = next (A.fst zi) (A.snd zi)
     where
      next :: Exp (Complex a) -> Exp Int32 -> Exp (Complex a, Int32)
      next z i =
        let z' = c + z*z
        in (dot z' >* 4) ? ( zi , lift (z', i+1) )                                     -- (3)

    dot :: Exp (Complex a) -> Exp a
    dot c = let r :+ i = unlift c
            in  r*r + i*i

    -- initial conditions for a given pixel in the window, translated to the
    -- corresponding point in the complex plane
    cs  = A.generate (constant $ Z :. screenY :. screenX) initial
    zs0 = A.map (\c -> lift (c, constant 0)) cs

    initial :: Exp DIM2 -> Exp (Complex a)
    initial ix = lift ( (xmin + (x * sizex) / viewx) :+ (ymin + (y * sizey) / viewy) )
      where
        pr = unindex2 ix
        x  = A.fromIntegral (A.snd pr :: Exp Int)
        y  = A.fromIntegral (A.fst pr :: Exp Int)
\end{lstlisting}

\begin{enumerate}
\item The function @step@ advances the entire complex plane by one iteration of
    the recurrence relation. This function is repeated @depth@ number of times,
    and the sequence combined by folding together with function application
    @($)@. This effectively unrolls the loop to @depth@ iterations.

\item The function @iter@ computes the next value in the sequence for a single
    point on the complex plane. If the point has diverged, we return the
    iteration count at which divergence occurred, otherwise the new value @z'@
    is returned and the iteration count @i@ is increased.

\item The conditional test is performed by the @(?)@ operator. Recall that GPUs are
    designed to do the same thing to lots of different data at the same time,
    whereas we want to do something different depending on whether or not a
    particular point has diverged. While we can not avoid having some kind of
    conditional in the code, we ensure there is only a bounded amount of
    divergence by having just one conditional per iteration, and a fixed number
    of iterations.

\end{enumerate}

Array fusion applied to the program in Listing~\ref{lst:mandelbrot} results in
the @depth@ copies of the function @step@ being combined into a single kernel.
As seen in Table~\ref{tab:mandelbrot}, while fusion improves the performance of
this program, it is still slower than the hand written version.

This slowdown is because the fused code generates a completely unrolled loop,
with @depth@ copies of the function body. Unrolling loops not only increases
instruction counts, but can often increase register lifetimes because of the
scheduling of loads and stores. In particular, stores are pushed down and loads
moved up in the instruction stream, which results in temporary scalar lifetimes
being longer. The unrolled code requires TK registers, resulting in a
multiprocessor occupancy of TK\%. Since the Mandelbrot program is limited by
computation, reducing the number of resident threads has a corresponding
reduction on maximum performance.

% \begin{lstlisting}[style=cuda
%     ,firstnumber=24]
% const float v13 = v10 * v10 - v11 * v11;                // single application of \texttt{step}
% const float v14 = v10 * v11 + v11 * v10;
% const float v15 = v8 + v13;
% const float v16 = v9 + v14;
% const Word8 v17 = v15 * v15 + v16 * v16 > 4.0f;
% const float v18 = v17 ? v10 : v15;
% const float v19 = v17 ? v11 : v16;
% const Int64 v20 = v17 ? v12 : (Int64) 1;
%   // repeats 255 times\ldots
% \end{lstlisting}


\subsection{Loop recovery}

Loop unrolling or loop unwinding is a loop transformation technique that
attempts to optimise a program's execution speed by reducing or eliminating
instructions that control the loop, such as branching and the end-of-loop test.
Loop unrolling does not always improve performance, however, because it may lead
to, for example, increased register usage.

Applying the fusion transformation to the Mandelbrot program shown in
Listing~\ref{lst:mandelbrot} resulted in a single kernel with @depth@ copies of
the function @iter@. In essence, the loop computing the Mandelbrot recurrence
relation has been completely unrolled. Unfortunately, the fused program does not
match the performance of the hand written version, because (a)
The program has a increased register usage; and (b) the conditional test is
still performed at the end of each copy of the loop body, so there is no
reduction in the overall number of instructions and branches executed.

We attempted to reduce these overheads by re-introducing explicit scalar loops
into the generated code. Recovering scalar loops then enables a backend to
generate explicit @for@ loops in the target language, and the compiler then
makes the decision of whether or not to unroll the loop. The following pair of
rewrite rules are used.
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape]
%\bf$\langle$ loop introduction $\rangle$%
    let x =
        let y = e1
        in e2
    in e3
    $\mapsto$
    iterate[2] (\y -> e2) e1            %\rm if \texttt{e2} $\equiv$ \texttt{e3}%

%\bf$\langle$ loop joining $\rangle$%
    let x = iterate[n] (\y -> e2) e1
    in e3
    $\mapsto$
    iterate[n+1] (\y -> e2) e1          %\rm if \texttt{e2} $\equiv$ \texttt{e3}%
\end{lstlisting}

As seen in Table~\ref{tab:mandelbrot}, loop recovery shows a good increase in
performance compared to the fused and unrolled result, but is slower than the
hand written CUDA program because every thread always executes until the
iteration limit, even if it diverges immediately, and must maintain this extra
information of whether or not the thread has diverged yet.


\subsection{Explicit iteration}

Following our most recent published results~\cite{McDonell:2013wi}, we added
explicit value recursion constructs to the source and target languages. Robert
Clifton-Everest implemented the necessary changes to the sharing recovery
algorithm, while I implemented the backend changes for code generation and
execution. The new function @while@ applies the given function, starting with
the initial value, as long as the test function continues to evaluate to @True@.
%
\begin{lstlisting}[style=haskell]
while :: Elt e
      => (Exp e -> Exp Bool)            -- conditional test
      -> (Exp e -> Exp e)               -- function to iterate
      -> Exp e                          -- initial value
      -> Exp e
\end{lstlisting}
%
The Mandelbrot program using explicit iteration is shown in
Listing~\ref{lst:mandelbrot_loop}, where the common parts to the @where@ clause
in Listing~\ref{lst:mandelbrot} have been elided. Note that in contrast to the
implementation based on fixed unrolling, the function will stop executing as
soon as the relation diverges, rather than always continuing to the iteration
limit even if the point has already diverged.

\begin{lstlisting}[style=haskell_float
    ,label=lst:mandelbrot_loop
    ,caption={Mandelbrot set generator, using explict iteration}]
mandelbrot
    :: forall a. (Elt a, IsFloating a)
    => Int
    -> Int
    -> Int
    -> Acc (Scalar (View a))
    -> Acc (Array DIM2 Int32)
mandelbrot screenX screenY depth view =
  generate (constant (Z :. screenY :. screenX))
           (\ix -> let c = initial ix
                   in  A.snd $ A.while (\zi -> A.snd zi <* lIMIT &&* dot (A.fst zi) <* 4)
                                       (\zi -> lift1 (next c) zi)
                                       (lift (c, constant 0)))
  where
    lIMIT = P.fromIntegral depth

    next :: Exp (Complex a) -> (Exp (Complex a), Exp Int32) -> (Exp (Complex a), Exp Int32)
    next c (z, i) = (c + (z * z), i+1)
\end{lstlisting}

With respect to implementation on SIMD architectures such as CUDA, while both
the fixed unrolling and explicit iteration methods have a conditional test that
introduces thread divergence, it is important to note that the behaviour of the
divergence is different for each method. In the fixed unrolling case, both
branches of the conditional must be executed to keep track of whether the
element has already diverged. This results in threads following separate
execution paths, where threads that follow the @True@ branch execute while all
other threads sit idle, then execution switches to the @False@ branch and the
first set of threads sit idle. In contrast, in the explicit iteration case,
threads exit the loop as soon as their point on the complex plane diverges, and
then simply wait for all other threads in their SIMD group to complete their
calculation and catch up.

While the introduction of explicit value recursion required changes to the
source language program, as seen in Table~\ref{tab:mandelbrot}, performance
improves significantly. Additionally, if all threads in the SIMD group diverge
quickly, this can save a significant amount of redundant computation.
Table~\ref{tab:mandelbrot} also shows the difference in execution time between
computing the entire fractal shown in Figure~\ref{fig:mandelbrot}, where all
points diverge at different depths in the sequence (avg), compared to a region
where all threads continue to the iteration limit (limit).


\subsection{Unbalanced workload}

In order to manage the unbalanced workload, the hand-written CUDA version
includes a custom thread-block scheduler. Avoiding thread divergence in CUDA
programs is an active area of research~\cite{Zhang:2010jc}. TK.

% The final problem with the Mandelbrot program is due to the initial formulation
% of the program using a fixed unrolling, as we must handle application of the
% function to elements which have already diverged. As explained, this is to
% ensure that all threads [in a warp] continue doing the same thing, which avoids
% excessive SIMD divergence, but means that the computation always proceeds to the
% iteration limit.
%
% For the Mandelbrot program threads only diverge in the sense that they are
% either still computing the recurrence relation, or the point is not in the set
% and the computation has completed; threads do not diverge to follow separate
% execution paths. The latter type of SIMD divergence is what we really need to
% avoid, because this results in predicated execution: threads taking the
% @true@ path of a branch execute while all other threads sit idle, then
% execution switches to the @false@ branch while the first set of threads
% idle.
%
% The CUDA version is faster because threads exit the loop as soon as their point
% in the complex plane diverges. Although once a thread completes it sits idle
% until all threads in the group complete their calculation, if all the points
% within a group of threads diverge quickly this can save a significant amount of
% work. Table~\ref{tab:mandelbrot} shows the difference in execution time when
% displaying the entire fractal (avg), where points diverge at varying iteration
% depths in the sequence, compared to a region where all thread continue to the
% iteration limit (limit). In order to manage this unbalanced workload, the CUDA
% version additionally includes a custom thread block scheduler.
%
% This test demonstrates that while loop recovery offered a significant
% performance benefit for free, it would be beneficial to expose looping
% constructs in the source language as well, rather than relying on potentially
% fragile post-hoc transformations. Avoiding thread divergence in CUDA programs is
% an active area of research~\cite{Zhang:2010jc}.

\section{N-body gravitational simulation}
\label{sec:nbody}

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/nbody/nbody}
    \end{center}
    \caption[N-body gravitational simulation kernel benchmarks]{Kernel runtimes
        for the $n$-body gravitational simulation, in Accelerate with and
        without optimisations, compared to a hand-written CUDA implementation.
        Note the log-log scale.}
    \label{fig:nbody}
\end{figure}

The $n$-body example simulates the Newtonian gravitational forces on a set of
massive bodies in 3D space, using the na\"ive $\mathcal{O}\left( n^{2} \right)$
algorithm. In a data-parallel setting, the natural way to express this algorithm
is first to compute the forces between every pair of bodies, before adding the
forces applied to each body using a segmented sum. Without fusion this approach
also requires $\mathcal{O}\left( n^{2} \right)$ space for the intermediate array
of forces, which exhausts the memory of our device (4GB!) when using more than
about five thousand bodies. With fusion, the reduction operation consumes each
force value on-the-fly, so that the program only needs $\mathcal{O}\left( n
\right)$ space to store the final force values. The core of the $n$-body
simulation is shown in Listing~\ref{lst:nbody}, where @accel@ calculates
the acceleration between two particles, and @(.+.)@ component-wise sums the
acceleration of a particle along each $x$, $y$ and $z$ axis.

\begin{lstlisting}[style=haskell_float
    ,label=lst:nbody
    ,caption={$N$-body gravitational simulation, using parallel reduction}]
calcAccels :: Exp Float -> Acc (Vector Body) -> Acc (Vector Accel)
calcAccels epsilon bodies
  = let n       = A.size bodies
        cols    = A.replicate (lift $ Z :. n :. All) bodies
        rows    = A.replicate (lift $ Z :. All :. n) bodies
    in
    A.fold (.+.) (vec 0) $ A.zipWith (accel epsilon) rows cols
\end{lstlisting}


\subsection{Sequential iteration}

Even with fusion, the reference CUDA version is over $10\times$ faster. Although
the fused program requires only $\mathcal{O}\left( n \right)$ space, it still
requires all $\mathcal{O}\left( n^2 \right)$ memory accesses, and $n^{2}$
threads to cooperatively compute the interactions.

With the addition of sequential iteration following our most recent
publication~\cite{McDonell:2013wi}, we can emit an alternative implementation.
Rather than computing all interactions between each pair of bodies and reducing
this array in parallel, we express the program as a parallel map of sequential
reductions, as shown in Listing~\ref{lst:nbody_seq}. Although the program still
performs all $n^{2}$ memory accesses, since all threads access the @bodies@
array in sequence, these values can be cached efficiently by the GPU, and the
actual bandwidth requirements to main memory are reduced.

\begin{lstlisting}[style=haskell_float
    ,label=lst:nbody_seq
    ,caption={$N$-body gravitational simulation, using sequential reduction}]
calcAccels :: Exp R -> Acc (Vector PointMass) -> Acc (Vector Accel)
calcAccels epsilon bodies
  = let move body = A.sfoldl (\acc next -> acc .+. accel epsilon body next)
                             (vec 0)
                             (constant Z)
                             bodies
    in
    A.map move bodies
\end{lstlisting}


\subsection{Use of Shared Memory}

In CUDA, the memory hierarchy of the device is made explicit to the programmer,
including a small on-chip shared memory region threads can use to share data,
that is essentially a software managed cache~\cite{NVIDIA:2012wf}.

The program shown in Listing~\ref{lst:nbody} uses the shared memory region to
share computations while computing the parallel reduction. The second
implementation shown in Listing~\ref{lst:nbody_seq} does not use shared memory
at all, but rather relies on hardware caching. Similar to our latter
implementation, the CUDA program is also implemented as a parallel map of
sequential reductions, but explicitly uses the shared memory region to share the
particle mass and position data between threads, rather than rely on hardware
caching. Since the shared memory region is very low latency, the CUDA program
remains faster. Automatic use of shared memory is a separate consideration to
the optimisations discussed in this work, and remains an open research
problem~\cite{Ma:2010ft}.

% This reduces the bandwidth requirement of the program by a factor of the number
% of threads in a block (256) but requires each thread to sum its particle
% interactions sequentially in $\mathcal{O}\left( n \right)$ time. The Accelerate
% version uses shared memory to perform a tree-reduction in $\mathcal{O}\left(
% \log n \right)$ time (\S\ref{sec:parallel_reduction}) but requires all $n^{2}$
% memory transfers. For this bandwidth bound application, making the trade-off of
% parallelism for bandwidth clearly wins. Automatic use of shared memory is a
% separate consideration to the optimisations discussed in this work, and remains
% an open research problem~\cite{Ma:2010ft}.


\section{Fluid Flow}
\label{sec:fluid}

The fluid flow example implements Jos Stam's stable fluid
algorithm~\cite{Stam:1999ey}, which is a fast approximate algorithm intended for
animation and games, rather than accurate engineering simulation. An example
sequence is shown in Figure~\ref{fig:fluid_steps}.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.3\textwidth]{images/sec-6/fluid/fluid-10}
        \includegraphics[width=0.3\textwidth]{images/sec-6/fluid/fluid-50}
        \includegraphics[width=0.3\textwidth]{images/sec-6/fluid/fluid-75}
    \end{center}
    \caption[Example of the fluid flow simulation]{An example showing the
        progression of the fluid flow simulation for a set of initial conditions
        after 10 (left), 50 (centre) and 75 (right) steps.}
    \label{fig:fluid_steps}
\end{figure}

The core of the algorithm is a finite time step simulation on a grid,
implemented as a matrix relaxation involving the discrete Laplace operator
($\nabla^2$). This step, known as the linear solver, is used to diffuse the
density and velocity fields throughout the grid, as well as apply a projection
operator to the velocity field to ensure it conserves mass. The linear solver is
implemented in terms of a stencil convolution, repeatedly computing the
following for each grid element to allow the solution to converge:
\[
u_{i,j}^{''} = \left( u_{i,j} + a \cdot \left( u'_{i-1,j}+u'_{i+1,j}+u'_{i,j-1}+u'_{i,j+1} \right) \right) / c
\]
Here, $u$ is the grid in the previous time step, $u'$ the grid in the current
time step and previous relaxation iteration, and $u''$ the current time step and
iteration. The values $a$ and $c$ are constants chosen by the simulation
parameters.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/fluid/fluid}
    \end{center}
    \caption[Fluid flow simulation kernel benchmarks]{Kernel runtimes for the
        fluid flow simulation, in Accelerate with and without optimisations,
        compared to sequential and parallel CPU implementations. Running the
        Repa program with seven threads on the eight cores was found to be
        faster. Note the log-log scale.}
    \label{fig:fluid}
\end{figure}

Figure~\ref{fig:fluid} compares Accelerate to a parallel implementation written
in Repa and running on the host CPU \cite{Lippmeier:2012gx}. The program is
extremely memory intensive, performing approximately 160 convolutions of the
grid per time step. Given the nature of the computation, we find that the raw
bandwidth available on the CUDA device, which is much greater than that
available to the CPU, results in correspondingly shorter run times. Since the
program consists of a sequence of stencil operations, fusion does not apply to
this program. Sharing recovery has a surprisingly minimal impact because the
implementation of stencils always shares some parts of the computation: namely
access to the grid elements $u_{xy}$, precisely because these operations are so
expensive.

The CUDA SDK sample programs includes a version of Jos Stam's fluid flow
algorithm, but that is computed using Fourier transforms and is intermingled
with OpenGL code for the visualisation, so we can not directly compare to that
implementation.

% \note{tk: example showing performance of \code{map (+1)} versus the equivalent
% of using a stencil: extra addressing arithmetic should be significant. For fluid
% flow, the actual computation time (probably) greatly outweighs the extra
% arithmetic.}
%
% \note{TK: Check the fluid demo that is part of the CUDA SDK, it looks like it
% might implement something similar.}


\section{Canny edge detection}
\label{sec:canny}

The edge detection example applies the Canny algorithm~\cite{Canny:1986et} to
rectangular images of various sizes. Figure~\ref{fig:canny} compares Accelerate
to parallel implementations on the CPU and GPU\@. The overall algorithm consists
of seven distinct phases, the first six of which are naturally data parallel and
are performed on the GPU\@. The last phase uses a recursive algorithm to
``connect'' the pixels that make up the output lines. In our implementation this
phase is performed on the CPU, which requires the image data to be transferred
from the GPU back to the CPU for processing, and accounts for the non-linear
slowdown visible with smaller image sizes. In contrast, OpenCV is able to
perform the final step on the GPU\@. The benchmark ``Accelerate (whole
program)'' includes the time to transfer the image data back to the host for the
final processing step. Also shown are the run times for just the first six data
parallel phases, which exhibit linear speedup and do not include data transfer
to the host.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.45\textwidth]{images/sec-6/canny/lena}
        \includegraphics[width=0.45\textwidth]{images/sec-6/canny/lena-edges}
    \end{center}
    \caption[Example of the Canny edge detection algorithm]{An example of the
        Canny algorithm applied to the Lena standard test image (left) which
        identifies edge pixels in the image (right).}
    \label{fig:lena}
\end{figure}

Neglecting the final phase, note that the data parallel phase is still slightly
slower than in OpenCV. As discussed in
Section~\ref{sec:parallel_stencil}, this is because the stencil kernels
in Accelerate currently make a test for every array access to see if the element
is in bounds, or if it lies outside the array and needs to be handled specially,
even though the vast majority of points in the array are far from the boundary.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/canny/canny}
    \end{center}
    \caption[Canny edge detection benchmarks]{Runtimes for the Canny edge
        detection algorithm, comparing the Accelerate kernels and whole program
        to sequential and parallel implementations. Note the log-log scale.}
    \label{fig:canny}
\end{figure}


\subsection{Stencil merging}

Two of the data parallel stages of the Canny algorithm are the calculation of
the image intensity gradients by convolving the image with the $3\times3$
kernels shown below. This convolution, known as the Sobel operator, is a
discrete differentiation operator that computes an approximation of the
horizontal and vertical image gradients.
%
\begin{equation*}
    \text{Sobel}_x =
        \begin{bmatrix*}[r]
          -1 & 0 & 1 \\
          -2 & 0 & 2 \\
          -1 & 0 & 1
        \end{bmatrix*}
    \qquad
    \text{Sobel}_y =
        \begin{bmatrix*}[r]
           1 &  2 &  1 \\
           0 &  0 &  0 \\
          -1 & -2 & -1
        \end{bmatrix*}
\end{equation*}

There is a significant amount of sharing between these two kernels, in terms of
the coefficients required to compute the derivative at each point. Thus, it
could be beneficial to merge these two stencils into a single kernel, rather
than computing them separately. As with all shortcut fusion methods, the system
presented here does not combine multiple passes over the same array into a
single traversal that computes multiple results. We leave this, as well as other
significant stencil
optimisations~\cite{Henretty:2013wb,Kamil:2006un,Lesniak:2010}, to future work.

% We leave this to future work,
% for example, in the style of data flow fusion~\cite{Lippmeier:2013vz}.


\section{Radix sort}

The radix sort benchmark implements a simple parallel radix sort algorithm as
described by \citet{Blelloch:1990vl} to sort an array by an integral key. We
compare our implementation of Blelloch's algorithm in Accelerate, shown in
Listing~\ref{lst:radixsort}, to one written in Nikola~\cite{Mainland:2010vj},
which is also an embedded language in Haskell for CUDA programming.%
\footnote{To be more specific, we estimate based on the results presented in the
paper~\cite{Mainland:2010vj} as Nikola no longer compiles with recent versions
of GHC, and the development version to replace it is not yet complete.
Nevertheless, results are informative as the same GPU, a Tesla T10, is used in
both cases.}
For this benchmark the Accelerate code is faster than Nikola, because Nikola is
limited to single kernel programs so must transfer every intermediate result
back to the host. Additionally, we are faster than a sequential radix sort
implementation using \texttt{Data.Vector} for as few as $\sim256$ elements,
whereas Nikola requires a dataset of about 32kB before the additional
parallelism of the GPU outperforms the sequential
version~\cite{Mainland:2010vj}.

\begin{lstlisting}[style=haskell_float
    ,label=lst:radixsort
    ,caption={Radix sort algorithm}]
class Elt e => Radix e where
  passes :: e -> Int                            -- number of passes (bits) required to sort this key type
  radix  :: Exp Int -> Exp e -> Exp Int         -- extract the $n^{th}$ bit of the key

sortBy :: forall a r. (Elt a, Radix r)
       => (Exp a -> Exp r)
       -> Acc (Vector a)
       -> Acc (Vector a)
sortBy rdx arr = foldr1 (>->) (P.map radixPass [0..p-1]) arr    -- loop over bits of the key, low\ldots
  where                                                         -- \ldots bit first to maintain sort order
    p           = passes (undefined :: r)
    deal f x    = let (a,b) = unlift x in (f ==* 0) ? (a,b)

    radixPass k v =
      let k'    = unit (constant k)                             -- to create reusable kernels
          flags = A.map (radix (the k') . rdx) v                -- extract the sort key
          idown = prescanl (+) 0 . A.map (xor 1)        $ flags -- move elements to the beginning\ldots
          iup   = A.map (size v - 1 -) . prescanr (+) 0 $ flags -- \ldots end of the vector
          index = A.zipWith deal flags (A.zip idown iup)
      in
      permute const v (\ix -> index1 (index!ix)) v              -- move elements to new position
\end{lstlisting}

The @radixPass@ function sorts the vector @v@ based on the value of
bit @k@; moving elements with a zero at that bit to the beginning of the
vector and those with a one bit to the end. This simple algorithm requires
$b$ iterations to sort an array of elements whose keys are $b$-bits wide, and
each pass requires multiple traversals of the array.

\begin{figure}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{images/sec-6/radixsort/radixsort}
    \end{center}
    \caption[Radix sort kernel benchmarks]{Kernel runtimes for the radix sort benchmark
        of signed 32-bit integers, comparing the Accelerate version to other
        parallel implementations. Note the log-log scale.}
    \label{fig:radixsort}
\end{figure}

As seen in Figure~\ref{fig:radixsort}, the absolute performance of this simple
algorithm, which sorts the array by a single bit at a time, is quite low.
Implementations of radix sort optimised for the CUDA
architecture~\cite{Satish:2009kx,Merrill:2011bz,ThrustAParallelT:ub}, are
approximately $10\times$ faster because they make efficient use of memory
bandwidth and on-chip shared memory to sort keys by 8-bits at a time. Such
implementations are, essentially, different algorithms than Blelloch's radix
sort algorithm that we have implemented here, but serve to illustrate the
absolute performance of the device. To address this we have implemented a
foreign function interface for Accelerate to take advantage of existing
high-performance libraries (\S\ref{sec:ffi}). The foreign function interface is
orthogonal to the work presented here of optimising programs written in the
Accelerate language. Useful future work would be to combine a fast-yet-fixed
foreign implementation with a more generic interface, using a method such
as~\cite{Henglein:2013dd}.


\section{Sparse-matrix vector multiplication}
\label{sec:smvm}

% \marginnote{tk: redo bechmarks}
\newcommand\spyplot[1]{\parbox[c][1.1cm][c]{1.1cm}{\includegraphics[width=1cm]{images/sec-6/smvm/#1}}}

\begin{table}
\centering
\small
\begin{tabu}{X[-1cm]X[2lm]cX[-1cb]rrr} \toprule

\textbf{spyplot}
& \textbf{Description}
& \textbf{Dimensions}
& \textbf{Nonzeros (nnz/row)}
& \multicolumn{1}{c}{\rotatebox{90}{\textbf{CUSP}}}
& \multicolumn{1}{c}{\rotatebox{90}{\textbf{Accelerate}}}
& \multicolumn{1}{c}{\begin{sideways}%
    \parbox{\widthof{\bf Accelerate}}%
    {\centering\textbf{Accelerate no fusion}}%
    \end{sideways}}
\\ \midrule

\spyplot{dense2} % Dense
& Dense matrix in sparse format
& 2K $\times$ 2K
& 4.0M (2K)
& 14.48 & 14.62 & 3.41
\\

\spyplot{pdb1HYS} % Protein
& Protein data bank 1HYS
& 36K $\times$ 36K
& 4.3M (119)
& 13.55 & 13.65 & 0.26
\\

\spyplot{consph} % FEM / Spheres
& FEM concentric spheres
& 83K $\times$ 83K
& 6M (72)
& 12.63 & 9.03 & 4.70
\\

\spyplot{cant} % FEM / Cantilever
& FEM cantilever
& 62K $\times$ 62K
& 4M (65)
& 11.98 & 7.96 & 4.41
\\

\spyplot{pwtk} % Wind Tunnel
& Pressurised wind tunnel
& 128K $\times$ 128K
& 11.6M (53)
& 11.98 & 7.33 & 4.62
\\

\spyplot{rma10} % FEM/Harbour
& 3D CFD of Charleston harbour
& 47K $\times$ 47K
& 2.37M (50)
& 9.42 & 6.14 & 0.13
\\

\spyplot{qcd5_4} % QCD
& Quark propagators (QCD/LGT)
& 49K $\times$ 49K
& 1.9M (39)
& 7.79 & 4.66 & 0.13
\\

\spyplot{shipsec1} % FEM/Ship
& FEM ship section/detail
& 141K $\times$ 141K
& 3.98 (28)
& 12.28 & 6.60 & 4.47
\\

\spyplot{mac_econ_fwd500} % Economics
& Macroeconomics model
& 207K $\times$ 207K
& 1.27M (6)
& 4.59 & 0.90 & 1.06
\\

\spyplot{mc2dephi} % Epidemiology
& 2D Markov model of epidemic
& 526K $\times$ 526K
& 1.27M (4)
& 6.42 & 0.59 & 0.91
\\

\spyplot{cop20k_A} % FEM/Accelerator
& Accelerator cavity design
& 121K $\times$ 121K
& 2.62M (22)
& 5.41 & 3.08 & 2.92
\\

\spyplot{scircuit} % Circuit
& Motorola circuit simulation
& 171K $\times$ 171K
& 959k (6)
& 3.56 & 0.82 & 1.08
\\

\spyplot{webbase-1M} % Webbase
& Web connectivity matrix
& 1M $\times$ 1M
& 3.1M (3)
& 2.11 & 0.47 & 0.74
\\

\spyplot{rail4284} % LP
& Railways set cover constraint matrix
& 4K $\times$ 1.1M
& 11.3M (2825)
& 5.22 & 5.04 & 2.41
\\

\bottomrule
\end{tabu}
\caption[Sparse-matrix vector multiplication benchmarks]{Overview of sparse
matrices tested and results of the benchmark. Measurements are in GFLOPS/s
(higher is better).}
\label{tab:smvm_summary}
\end{table}


This benchmark considers the multiplication of sparse matrices in compressed row
format (CSR)~\cite{Chatterjee:1990vj} with a dense vector. This matrix format
consists of an array of the non-zero elements paired with their column index,
together with a segment descriptor recording the number of non-zeros in each
row. The corresponding Accelerate code is shown in Listing~\ref{lst:smvm}.
Table~\ref{tab:smvm_summary} compares Accelerate to the CUSP
library~\cite{Bell:2008wc,Bell:2009bl}, a special purpose library for sparse
matrix operations. Using a 14 matrix corpus derived from a variety of
application domains~\cite{Williams:2009cy}, we compare against CUSP for
compressed row format matrices.

\begin{lstlisting}[style=haskell
    ,float
    ,label=lst:smvm
    ,caption={Sparse-matrix vector multiplication}]
type SparseVector e = Vector (Int, e)                   -- column index and value
type SparseMatrix e = (Segments Int, SparseVector e)    -- length of each row

smvm :: (Elt a, IsNum a) => Acc (SparseMatrix a) -> Acc (Vector a) -> Acc (Vector a)
smvm smat vec
  = let (segd, svec)    = unlift smat
        (inds, vals)    = A.unzip svec
        vecVals         = gather inds vec
        products        = A.zipWith (*) vecVals vals
    in
    foldSeg (+) 0 products segd
\end{lstlisting}

Compared to our previous work~\cite{Chakravarty:2011fr} the fusion
transformation compresses the program into a single segmented reduction. As
predicted, the corresponding reduction in memory bandwidth means that Accelerate
is on par with the CUSP library for several of the test matrices. In a balanced
machine SMVM should be limited by memory throughput, so a dense matrix in
sparse format should provide an upper bound on performance because loops are
long running and accesses to the source vector are contiguous and have high
re-use. We see that Accelerate with array fusion achieves this expected
performance limit. %, but is also slightly faster than the CUSP implementation.

\subsection{Segment startup}

To maximise global memory throughput the skeleton code ensures that the vector
read of each matrix row is coalesced and aligned to the warp boundary. Combined
with the behaviour that reduction operations in Accelerate do not require a
neutral starting element (see Section~\ref{sec:dotp} for further discussion),
this means that there is significant overhead in initialising the local sum for
each thread at the beginning of the reduction.


% Since Accelerate does not require the starting element of the reduction to be a
% neutral element, threads must also ensure they initialise their local sum from
% the input array (\ref{sec:non-neutral_starting_elements}).
% Listing~\ref{lst:smvm_cuda} shows the generated CUDA code for the first
% sequential phase of the cascaded tree-reduction algorithm (see
% \S\ref{sec:algorithm_cascading}). Figure~tk illustrates the different cases that
% must be considered.
%
% \begin{lstlisting}[style=cuda_float
%     ,firstnumber=44
%     ,label=lst:smvm_cuda
%     ,caption={Generated CUDA code for sparse-matrix vector multiplication}]
% if (num_elements > warpSize) {
%     ix = start - (start & warpSize - 1) + thread_lane;                  (@* \label{lst:smvm_cuda_boundary} *@)
%     if (ix >= start) {                                                  (@* \label{lst:smvm_cuda_warp_start} *@)
%         const Int64 v3 = ix;
%         const int v4 = toIndex(shIn2, shape(v3));
%         const int v5 = toIndex(shIn1, shape((Int64) arrIn2_a1[v4]));
%         const int v6 = toIndex(shIn2, shape(v3));
%
%         y0 = arrIn1_a0[v5] * arrIn2_a0[v6];
%     }
%     if (ix + warpSize < end) {                                          (@* \label{lst:smvm_cuda_warp_next} *@)
%         const Int64 v3 = ix + warpSize;
%         const int v4 = toIndex(shIn2, shape(v3));
%         const int v5 = toIndex(shIn1, shape((Int64) arrIn2_a1[v4]));
%         const int v6 = toIndex(shIn2, shape(v3));
%
%         x0 = arrIn1_a0[v5] * arrIn2_a0[v6];
%         if (ix >= start) {                                              (@* \label{lst:smvm_cuda_branch} *@)
%             y0 = x0 + y0;
%         } else {
%             y0 = x0;
%         }
%     }
%     for (ix += 2 * warpSize; ix < end; ix += warpSize) {                (@* \label{lst:smvm_cuda_loop} *@)
%         const Int64 v3 = ix;
%         const int v4 = toIndex(shIn2, shape(v3));
%         const int v5 = toIndex(shIn1, shape((Int64) arrIn2_a1[v4]));
%         const int v6 = toIndex(shIn2, shape(v3));
%
%         x0 = arrIn1_a0[v5] * arrIn2_a0[v6];
%         y0 = x0 + y0;
%     }
% } else if (start + thread_lane < end) {                                 (@* \label{lst:smvm_cuda_unaligned} *@)
%     const Int64 v3 = start + thread_lane;
%     const int v4 = toIndex(shIn2, shape(v3));
%     const int v5 = toIndex(shIn1, shape((Int64) arrIn2_a1[v4]));
%     const int v6 = toIndex(shIn2, shape(v3));
%
%     y0 = arrIn1_a0[v5] * arrIn2_a0[v6];
% }
% \end{lstlisting}
%
% To initialise the warp-aligned read the location of the warp boundary before the
% start of the source vector is calculated (line~\ref{lst:smvm_cuda_boundary}),
% offset by this thread's warp lane index. If the thread lies within the source
% vector its starting value can be initialised
% (line~\ref{lst:smvm_cuda_warp_start}). Reading elements from the second warp
% (line~\ref{lst:smvm_cuda_warp_next}) is aligned and coalesced to the warp
% boundary so completes in a single global memory transfer, but requires a
% divergent branch (line~\ref{lst:smvm_cuda_branch}) depending on whether this is
% the first value read or not. Once all threads have initialised their local sum,
% the serial reduction phase completes (line~\ref{lst:smvm_cuda_loop}), which is
% the same as we saw in the dot product example (Listing~\ref{lst:dotp_cuda}). If
% there is less than a warp's worth of data to be reduced, the elements are simply
% read unaligned (line~\ref{lst:smvm_cuda_unaligned}). If we do not do this, the
% first threads of the warp may not have been initialised, which is a requirement
% for the second cooperative tree-reduction phase of the algorithm, as it combines
% values for indices $\left[0,n\right)$ into index zero.

Matrices such as FEM/Spheres, with few non-zero elements per row ($\lesssim 2
\times \text{warp size} = 64$) exhibit a drop in performance relative to CUSP,
because this extra startup cost necessary to align global memory reads to the
warp boundary can not be amortised over the row length.
%
Matrices such as Epidemiology, with large vectors and few non-zeros per row,
exhibit low flop:byte ratio and are poorly suited to the CSR format, with all
implementations performing well below peak. This highlights the nature of sparse
computations and the reason the CUSP library supports several algorithms and
matrix formats.

As was found with the dot product benchmark (\S\ref{sec:dotp}), implementing
specialised kernels that can avoid this additional startup cost when the
combining function and starting element form a moniod is expected to improve
performance, but is left to future work.

% This may be related to the way the skeleton code ensures that
% the vector read of each row is coalesced and aligned to the warp boundary to
% maximise global memory throughput, but is then not able to amortize this extra
% startup cost over the row length.
%
% Since Accelerate does not require the starting element of the reduction to be a
% neutral element (see \S\ref{sec:non-neutral_starting_elements})
%
% The regression relative to
% our previous result will be investigated. Nevertheless,

\section{Shortest paths in a graph}
\label{sec:floyd_warshall}

The Floyd-Warshall algorithm finds the lengths of the shortest paths between all
pairs of points in a weighted directed graph. The implementation in Accelerate
is shown in Listing~\ref{lst:floyd_warshall}, and appeared in the book
\emph{Parallel and Concurrent Programming in Haskell}~\cite{Marlow:2013wn}. The
algorithm runs over an adjacency matrix, building the solution bottom-up so that
earlier results are used when computing later ones. Each step of the algorithm
is $O\left(n^2\right)$ in the number of vertices, so the whole algorithm is
$O\left(n^3\right)$.

\begin{lstlisting}[style=haskell_float
    ,label=lst:floyd_warshall
    ,caption={[Floyd-Warshall shortest-paths algorithm]
        Floyd-Warshall shortest-paths algorithm~\cite{Marlow:2013wn}}]
type Weight = Int32
type Graph  = Array DIM2 Weight                                                        -- (1)

shortestPaths :: Graph -> Graph
shortestPaths g0 = run (shortestPathsAcc n (use g0))
  where
    Z :. _ :. n = arrayShape g0

shortestPathsAcc :: Int -> Acc Graph -> Acc Graph
shortestPathsAcc n g0 = foldl1 (.) steps g0                                            -- (6)
  where
    steps :: [ Acc Graph -> Acc Graph ]
    steps =  [ step (unit (constant k)) | k <- [0 .. n-1] ]                            -- (5)

step :: Acc (Scalar Int) -> Acc Graph -> Acc Graph
step k g = generate (shape g) sp                                                       -- (2)
  where
    k' = the k                                                                         -- (3)

    sp :: Exp DIM2 -> Exp Weight
    sp ix = let Z :. i :. j = unlift ix
            in  min (g ! (index2 i j))                                                 -- (4)
                    (g ! (index2 i k') + g ! (index2 k' j))
\end{lstlisting}

\begin{enumerate}
\item The dense adjacency matrix is a two-dimensional array indexed by pairs of
    vertices, where each element is the length of the path between two vertices.

\item The @step@ function computes the lengths of the shortest paths between two
    elements, using only vertices up to @k@, where the previous graph @g@
    contains the lengths of the shortest paths using vertices up to @k - 1@.

\item The iteration number @k@ is a scalar array. Why don't we pass this as a
    plain @Int@? When the program runs, Accelerate converts the program into a
    series of CUDA kernels, each of which must be compiled and loaded onto the
    GPU~(\S\ref{sec:code_generation}). Since compilation can take a while,
    Accelerate remembers the kernels it has seen before and reuses
    them~(\S\ref{sec:dynamic_compilation}). Our goal with @step@ is to make a
    kernel that will be reused, otherwise the overhead of compiling a new kernel
    for each iteration will ruin performance.

    By defining @k@ as a scalar array, we ensure that it is defined
    \emph{outside} the call to @generate@. If we don't do this, a different
    value of @k@ will be embedded directly into each call to the @generate@
    function, which will defeat Accelerate's caching of CUDA kernels.

\item To determine the length of the shortest path between @i@ and @j@, we take
    the minimum of the previous shortest paths from @i@ to @j@, and the path
    that goes from @i@ to @k@ and then from @k@ to @j@.

\item The list of steps, where each step takes a @Graph@ and delivers a new
    @Graph@, is constructed by applying @step@ to each value of @k@ in the
    sequence @0 .. n-1@.

\item Finally, the @shortestPathsAcc@ function connects the sequence of @step@
    calls by left-folding with function composition @(.)@, and passing the
    original graph @g0@ as input to the pipeline.

\end{enumerate}

% This is the best program ebaaaaaa~<3!
% Buy ALLLLLLLLLL~ the things~<3 !
% And give meh ALLLLLLLLLLLLLL~ the monieeeeeees~<3 !
% ;D
%
% - pookie 2014/06/16

Figure~\ref{fig:floyd_warshall} compares the performance of the algorithm in
Accelerate to several parallel CPU implementations.


\section{MD5 password recovery}

The MD5 message-digest algorithm~\cite{Rivest:1992va} is a cryptographic hash
function producing a 128-bit hash value from a variable length message. MD5 has
been used in a wide variety of cryptographic and data integrity applications,
although cryptographers have since found flaws in the algorithm and now
recommend the use of other hash functions for cryptographic
applications.\footnote{\url{http://www.kb.cert.org/vuls/id/836068}}

Figure~\ref{fig:md5_round} illustrates the main MD5 algorithm, whose
implementation in Accelerate is shown in Listing~\ref{lst:md5}. This
implementation only processes a single MD5 round (512-bits of input as
16$\times$32-bit words), and the input message is padded appropriately on the
CPU before being passed to the @md5round@ function. The algorithm operates on a
128-bit state, divided into 4$\times$32-bit words, denoted $A$, $B$, $C$, and
$D$, which are initialised to fixed constants. The algorithm uses the 512-bit
message block to modify the state in four rounds of 16 operations each, where
each round is based on a nonlinear function $F$, modulo addition, and left
rotation. The nonlinear mixing operation used by each round are respectively:
%
\begin{align*}
    F(B,C,D) &= (B \wedge C) \vee (\neg B \wedge D) \\
    G(B,C,D) &= (B \wedge D) \vee (C \wedge \neg D) \\
    H(B,C,D) &= B \oplus C \oplus D \\
    I(B,C,D) &= C \oplus (B \vee \neg D)
\end{align*}
%
Where $\oplus$, $\wedge$, $\vee$, and $\neg$ are the bitwise XOR, AND, OR and
NOT operations respectively.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/sec-6/MD5/MD5}
    \caption[A single round of the MD5 hash algorithm]{A single round of the MD5
        hash algorithm. It consists of 64 iterations of this operation, split
        into 4 rounds of 16 iterations each. $F$ is a nonlinear function that is
        different for each round, $M_i$ denotes a 32-bit block of the input
        message, and $K_i$ a 32-bit constant. $\lll_s$ is left rotation by $s$
        bits, and $\boxplus$ addition modulo $2^{32}$. Image from
        \url{http://en.wikipedia.org/wiki/MD5}.}
    \label{fig:md5_round}
\end{figure}


\begin{lstlisting}[style=haskell_float
    ,label=lst:md5
    ,caption={A single round of the MD5 hash algorithm}]
type MD5        = (Word32, Word32, Word32, Word32)      -- output 128-bit digest
type Dictionary = Array DIM2 Word32                     -- input 16$\times$32-bit words per block

md5Round :: Acc Dictionary -> Exp DIM1 -> Exp MD5
md5Round dict (unindex1 -> word)
  = lift
  $ foldl step (a0,b0,c0,d0) [0..64]
  where
    step (a,b,c,d) i
      | i < 16    = shfl ((b .&. c) .|. ((complement b) .&. d))         -- F
      | i < 32    = shfl ((b .&. d) .|. (c .&. (complement d)))         -- G
      | i < 48    = shfl (b `xor` c `xor` d)                            -- H
      | i < 64    = shfl (c `xor` (b .|. (complement d)))               -- I
      | otherwise = (a+a0,b+b0,c+c0,d+d0)
      where
        shfl f = (d, b + ((a + f + k i + get i) `rotateL` r i), b, c)

    get :: Int -> Exp Word32
    get i
      | i < 16    = getWord32le i
      | i < 32    = getWord32le ((5*i + 1) `rem` 16)
      | i < 48    = getWord32le ((3*i + 5) `rem` 16)
      | otherwise = getWord32le ((7*i)     `rem` 16)

    getWord32le :: Int -> Exp Word32
    getWord32le (constant -> i) = dict ! index2 word i

    a0, b0, c0, d0 :: Exp Word32        -- initial values (constants)

    k :: Int -> Exp Word32              -- binary integer part of sines \& cosines (in radians)
    k i = constant (ks P.!! i)          -- (constants)
      where ks = [ ... ]

    r :: Int -> Exp Int                 -- per-round shift amounts
    r i = constant (rs P.!! i)          -- (constants)
      where rs = [ ... ]
\end{lstlisting}

The input to the @md5round@ function consists of a @(Z :. 16 :. n)@ input array
containing @n@ 512-bit input chunks to hash, which are processed by @n@ threads
in parallel. Note that the input array is stored in column major order, so that
accesses to the input @dict@ are coalesced. See \cite{NVIDIA:2012wf} for details
on CUDA memory access rules. In contrast, a CPU prefers the data to be laid out
in row-major order, so that the input chunk is read on a single cache line.
Exploring the differences in memory architectures between CPUs and GPUs, which
focus on task versus data parallelism respectively, is left for future work.

In the MD5 benchmark, each thread computes the hash of a 512-bit input column
and compares it to a supplied hash. If the values match, the input corresponds
to the plain text of the unknown hash. Table~\ref{tab:md5} compares Accelerate
to several other implementations. Note that the Hashcat program is not open
source and provides many additional options not supported by the Accelerate
implementation, so they are not strictly comparable. However, Hashcat is
regarded as the fastest password recovery tool available, so provides a useful
baseline.

\begin{table}
\centering
\small
\begin{tabu}{lr}
\toprule
\textbf{Benchmark} & \multicolumn{1}{c}{\textbf{Rate (GHash/sec)}} \\

\midrule
Accelerate      & \\
Repa (-N8)      & \\
Hashcat (CPU)   & \\
Hashcat (GPU)   & \\

\bottomrule
\end{tabu}
\caption{MD5 password recovery benchmarks}
\label{tab:md5}
\end{table}


\section{K-Means clustering}

In the $K$-means problem, the goal is to partition a set of observations into
$k$ clusters, in which each observation belongs to the cluster with the nearest
mean. Finding an optimal solution to the problem is NP-hard, however there exist
efficient heuristic algorithms that converge quickly to a local optimum, and in
practice give good results. The most well known heuristic technique is Lloyd's
algorithm, which finds a solution by iteratively improving on an initial guess.
The algorithm takes as a parameter the number of clusters, makes an initial
guess at the centre of each cluster, and proceeds as follows:
%
\begin{enumerate}
    \item Assign each point to the cluster to which it is closest. This yields
        the new set of clusters.

    \item Compute the centroid of each cluster.

    \item Repeat steps (1) and (2) until the cluster locations stabilise.
        Processing is also stopped after some chosen maximum number of
        iterations, as sometimes the algorithm does not converge.
\end{enumerate}
%
The initial guess can be constructed by randomly assigning each point in the
data set to a cluster and then finding the centroids of those clusters. The core
of the algorithm is shown in Listing~\ref{lst:kmeans}, which computes the new
cluster locations. To complete the algorithm, the function @makeNewClusters@ is
applied repeatedly until convergence, or some maximum iteration limit is
reached. While the algorithm works for any number of dimensions, the
implementation shown here is for two dimensional points only.

\begin{lstlisting}[style=haskell_float
    ,label=lst:kmeans
    ,caption={$K$-means clustering for 2D points}]
type Point a    = (a, a)
type Id         = Word32
type Cluster a  = (Id, (a, a))
type PointSum a = (Word32, (a, a))

distance :: (Elt a, IsNum a) => Exp (Point a) -> Exp (Point a) -> Exp a
distance u v = ...

findClosestCluster
    :: forall a. (Elt a, IsFloating a, RealFloat a)
    => Acc (Vector (Cluster a))
    -> Acc (Vector (Point a))
    -> Acc (Vector Id)
findClosestCluster clusters points =
  A.map (\p -> A.fst $ A.sfoldl (nearest p) z (constant Z) clusters) points
  where
    z = constant (-1, inf)

    nearest :: Exp (Point a) -> Exp (Id, a) -> Exp (Cluster a) -> Exp (Id, a)
    nearest p st c =
      let d  = A.snd st
          d' = distance p (centroidOfCluster c)
      in
      d' <* d ? ( lift (idOfCluster c, d') , st )

makeNewClusters
    :: forall a. (Elt a, IsFloating a, RealFloat a)
    => Acc (Vector (Point a))
    -> Acc (Vector (Cluster a))
    -> Acc (Vector (Cluster a))
makeNewClusters points clusters
  = pointSumToCluster
  . makePointSum
  . findClosestCluster clusters
  $ points
  where
    npts        = size points
    nclusters   = size clusters

    pointSumToCluster :: Acc (Vector (PointSum a)) -> Acc (Vector (Cluster a))
    pointSumToCluster ps =
      A.generate (A.shape ps)
                 (\ix -> lift (A.fromIntegral (unindex1 ix), average (ps ! ix)))

    makePointSum :: Acc (Vector Id) -> Acc (Vector (PointSum a))
    makePointSum = A.fold1 addPointSum . pointSum

    pointSum :: Acc (Vector Id) -> Acc (Array DIM2 (PointSum a))
    pointSum nearest =
      A.generate (lift (Z :. nclusters :. npts))
                 (\ix -> let Z:.i:.j = unlift ix    :: Z :. Exp Int :. Exp Int
                             near    = nearest ! index1 j
                             yes     = lift (constant 1, points ! index1 j)
                             no      = constant (0, (0,0))
                         in
                         near ==* A.fromIntegral i ? ( yes, no ))

    average :: Exp (PointSum a) -> Exp (Point a)
    average ps = ...            -- average of the $x$- and $y$-coordinates

    addPointSum :: Exp (PointSum a) -> Exp (PointSum a) -> Exp (PointSum a)
    addPointSum x y = ...       -- component-wise addition
\end{lstlisting}

Figure~\ref{fig:kmeans} compares the performance of the implementation shown
here to several other CPU implementations.

\section{Discussion}
% \subsection{Related Work}
% \subsection{Future Work}

% \begin{itemize}
%     \item performance \& analysis
%     \begin{itemize}
%       \item What is an optimisation? Describe the metrics considered.
%         \begin{itemize}
%           \item wall-clock time, parallel speedup??
%           \item instruction count
%           \item memory traffic (load/store/coalescing)
%           \item memory size (heap, register count, shared memory)
%           \item program size (number of parallel steps)
%           \item code size (generated binary, compilation time c.f. code complexity)
%         \end{itemize}
%     \end{itemize}
%
%     \item expressiveness / example programs
%     \item difficulties
%     \begin{itemize}
%         \item code generation
%         \item unintentional nesting
%     \end{itemize}
% \end{itemize}

