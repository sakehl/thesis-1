
\chapter{Implementation}
\epigraph{The trouble with opportunity is that it always comes disguised as hard work.}%
{\textsc{---herbert v.\ prochnow}}


%\begin{itemize}
%\item implementing parallel algorithms (20\%, 1 week)
%    \begin{itemize}
%        \item fold
%        \item scan
%        \item permute (atomic operations)
%        \item segmented/rank polymorphic operations
%    \end{itemize}
%
%\item skeleton-based code generation (2 weeks)
%    \begin{itemize}
%        \item device capabilities (free array variables, \code{umul24},
%            \code{shfl}, atomic operations, shared memory)
%        \item rank polymorphic operations
%    \end{itemize}
%
%\item runtime system (3 weeks)
%    \begin{itemize}
%        \item launching kernels (occupancy calculator)
%        \item caching compiled kernels (live \& persistent caches)
%        \item memory management (escaping the EDSL evaluator; weak pointers vs.
%            reference counting)
%        \item executing kernels (annotated AST)
%    \end{itemize}
%
%\end{itemize}

% \begin{itemize}
%\item Accelerate frontend
%    \begin{itemize}
%        \item reification
%            \begin{itemize}
%                \item smart constructors for type classes
%                \item explicit dictionaries (polymorphism)
%                \item environments
%                \item HOAS vs. de Bruijn
%                \item representation types
%            \end{itemize}
%        \item surface \& internal (core) languages
%            \begin{itemize}
%                \item representing different constructs in surface/core
%                \item surface nested $\rightarrow$ core flat ?? (fuuuuture)
%            \end{itemize}
%        \item sharing observation ( --> sec 5? not my work )
%    \end{itemize}
%
%\item Accelerate CUDA backend
%    \begin{itemize}
%        \item code generation
%            \begin{itemize}
%                \item architecture sensitive JIT cross-compiler
%                \item future work: types, Haskell compile time
%            \end{itemize}
%        \item external compilation
%            \begin{itemize}
%                \item annotating AST nodes
%            \end{itemize}
%        \item memory management
%            \begin{itemize}
%                \item weak pointers \& weak hash tables
%                \item advantages to alternatives
%            \end{itemize}
%        \item execution
%            \begin{itemize}
%                \item occupancy analysis
%                \item multi-pass kernels
%            \end{itemize}
%        \item performance
%            \begin{itemize}
%                \item amortizing overheads (how to quantise this?)
%                \item of generated code
%                \item runtime overheads
%                \item w.r.t. CUDA memory subsystem (theoretical performance)
%                \item examples! spot the infelicities! (dot product)
%            \end{itemize}
%    \end{itemize}
%\end{itemize}

%Compilation has five phases:
%\begin{itemize}
%    \item Lexing
%    \item Parsing
%    \item Semantic analysis
%    \item Optimisation
%    \item Code Generation
%\end{itemize}

The previous chapter described the design of the Accelerate language, an
embedded DSL of operations over arrays, rich enough to express some interesting
real-world problems. This chapter details the architecture and implementation of
a backend for parallel execution on CUDA hardware.


\section{Parallel Algorithms in CUDA}

The CUDA\index{CUDA} architecture is a parallel computing platform and
programming model created by NVIDIA and implemented by the graphics processing
units (GPUs)\index{GPU|see {graphics processing unit}}\index{graphics processing
unit} that they produce. Using CUDA, the GPU becomes accessible not just for
graphics applications, but for computational tasks much like a CPU\@. Unlike
CPUs, however, GPUs have a parallel throughput architecture that emphasises
executing many concurrent threads slowly, rather than executing a single thread
very quickly.
% Unlike CPU cores, instructions are issued in order and there is no
% branch prediction and no speculative execution.

The CUDA hardware architecture is built around a scalable array of multithreaded
\emph{streaming multiprocessors} (SMs).\index{SM|see {streaming
multiprocessor}}\index{streaming multiprocessor} When a CUDA program on the host
CPU invokes a \indexe{kernel} --- a function which executes on the GPU --- a
\emph{grid} of thread \emph{blocks} is
% \index{thread!grid} \index{thread!block}
enumerated and distributed to the multiprocessors for execution, using a
\emph{single-instruction, multiple thread} (SIMT) model.\index{SIMT|see {single
instruction multiple thread}}\index{single instruction multiple thread} The
multiprocessor creates, manages, schedules, and executes threads in groups of 32
parallel threads call a \indexe{warp}. All threads in a warp share a single
program address counter, so divergent threads within a warp are handled via
predicated execution; for example the positive and negative cases of a branch
are executed in sequence, with threads not on the current execution path made
inactive (disabled). In essence, streaming multiprocessors replace the complex
instruction scheduling logic of a CPU which increases single threaded
performance --- namely out-of-order execution, branch prediction and speculative
execution --- with many additional ALUs (arithmetic logic units, often referred
to as CUDA cores) all executing the same instruction sequence in parallel in
order to increase total instruction throughput. Details of the CUDA architecture
can be found in the CUDA C programming guide~\cite{NVIDIA:2012wf}; we present
only those features required for the discussion.

Some collective operations such as \code{map} have an obvious mapping to the
highly parallel CUDA architecture, so we elide discussion of their
implementation. Other collective operations such as \code{scan}, where the value
of each element depends on the value of the last, may at first seem to not admit
a parallel interpretation at all. This section outlines the implementation of
this second kind of collective operation, where efficient parallel
implementation in CUDA may not be obvious.


\subsection{Reduction}
\label{sec:parallel_reduction}

Reductions are a common and important data-parallel primitive.
Figure~\ref{fig:tree_reduction} illustrates the basic strategy of a tree
reduction: within each thread block a tree-based approach is used to reduce
elements to a single value, and multiple thread blocks each process a portion of
the array in parallel. There is no global synchronisation primitive in CUDA,
because (a) it would be expensive to build in hardware for large numbers of
multiprocessors, and (b) would be difficult for programmers to use without
introducing sources of deadlock. Instead, kernel launches serve as a global
synchronisation point, so thread blocks instead communicate their partial
results by committing them to memory, and the kernel is launched recursively
until the final reduction is computed.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=\textwidth]{images/sec-4/tree-reduction}
    \end{center}
    \caption[A parallel tree reduction]{Illustration of a tree reduction,
        performed in two steps. In the first step 8 thread blocks in parallel
        reduce 8 elements to a single value. The thread blocks synchronise by
        writing their result to memory, and the kernel is called recursively
        until the final result is computed.}
    \label{fig:tree_reduction}
\end{figure}

For a fused dot product kernel, only the first line of reductions at level zero
perform the element wise multiplication. All subsequent reductions in that
kernel, as well as the recursive steps at level one and beyond, are pure
reductions. Thus, the fused operation requires compilation of two separate
kernels. % We focus on the level zero kernel, which is more interesting.

\subsubsection{Parallel Reduction Complexity}
\label{sec:parallel_reduction_complexity}

If each thread combines two elements at a time, then a vector of length $N$ will
\marginnote{tk: for power-of-2 n, add upper/lower bounds symbols}
be reduced in $\mathcal{O}\left( \log N \right)$ parallel steps. Each step $S$
does $\sfrac{N}{S^2}$ independent operations, so the \emph{step
complexity}\index{complexity!step} of the algorithm is:
\[
\mathcal{O}\left( \log N \right)
\]
For $N=2^{D}$, the algorithm thus performs $\sum_{S=1}^{D}2^{D-S} = N - 1$
operations. This means that the \emph{work complexity}\index{complexity!work} of
the algorithm is:
\[
\mathcal{O}\left( N \right)
\]
and so does not perform more work than a sequential algorithm. For $P$ threads
running physically in parallel on $P$ processors, the \emph{time
complexity}\index{complexity!time} is $\mathcal{O}\left( \sfrac{N}{P} + \log N
\right)$. In a thread block $N = P$, so the time complexity is:
\[
\mathcal{O}\left( \log N \right)
\]
Compare this to a sequential reduction, which has a time complexity of
$\mathcal{O}\left( N \right)$.

\subsubsection{Algorithm Cascading}
\label{sec:algorithm_cascading}

The \emph{cost} of a parallel algorithm is the number of processors $\times$
time complexity. This implies that the cost of the algorithm is
$\mathcal{O}\left( N \log N \right)$, which is \emph{not} cost efficient.

Brent's theorem~\cite{Chatterjee:2009vh} suggests that instead of each thread
summing two elements, \emph{algorithm cascading} can be used to combine a
sequential and parallel reduction. Each thread does $\mathcal{O}\left( \log N
\right)$ sequential work, which reduces the cost of the algorithm to
$\mathcal{O}\left( \sfrac{N}{\log N} \log N \right)$, or rather:
\[
\mathcal{O}\left( N \right)
\]
while keeping the work complexity $\mathcal{O}\left( N \right)$ and step
complexity $\mathcal{O}\left( \log N \right)$.

This suggests, for example, that a block of 256 threads should sum a total of
2048 elements. In practice it is beneficial to do even more sequential work per
thread, since this reduces the number of levels in the recursive tree reduction
and provides better latency hiding.


\subsubsection{Mapping to CUDA Threads}

Reduction of a one dimensional array uses multiple thread blocks to
cooperatively reduce the array, as described above. The number of thread blocks
is limited to the maximum number of blocks that can be simultaneously resident
on the current device, which requires that blocks each do a larger amount of
sequential work before being the cooperative reduction phase. This limits the
total kernel startup cost associated with launching thread blocks. Since the
maximum number of resident blocks is typically much less than the number of
threads in a block, the reduction will typically require at most two kernel
invocations.\footnote{Accelerate selects the thread block size in order to
maximise thread occupancy. As this depends on both the specific GPU being used
as well as the user function the array is reduced with, an upper bound of two
parallel steps can not be guaranteed.}

Higher-dimensional reductions reduce the array along the innermost dimension
only. This is similar to a segmented fold, except that the segment descriptor is
not necessary since the length of every segment is the same and can be
determined from the array shape. Instead of thread blocks cooperatively reducing
each segment sequentially, each segment is reduced by a single thread block
which operates independently of all other thread blocks. A consequence of this
is that proper device utilisation depends on the shape of the array and not
simply the total number of elements in the array. For example, reduction of an
array with shape \code{(Z :. 1 :. n)} will use a single thread block, no matter
how large \code{n} is. This simplifies the implementation but is clearly not
always ideal.\footnote{The number of multiprocessors on a device various between
architecture generation and performance of a given card. For example, a Tesla
T10 processor (compute compatability 1.3) has 240 cores split over 30
multiprocessors, while a Kepler K20X processor (compute capability 3.5) has 2688
cores split over only 14 multiprocessors. A given architecture generation will
have the same number of cores per multiprocessor, and lower performance
processors in a generation are produced by incorporating (or activating) fewer
multiprocessors, thereby reducing the total core count.}

A segmented reduction uses a single warp to reduce each segment. Investigation
of whether it is be better to utilise a thread block per segment instead, or
convert the multidimensional reduction to this warp per segment strategy, is
left for future work.


\subsection{Scan}

Parallel scan and segmented scan algorithms are a crucial building block for a
great many data-parallel algorithms~\cite{Blelloch:1990ts,Chatterjee:1990vj}.
They also form the basis for efficiently mapping nested data-parallel languages
such as NESL~\cite{Blelloch:1995ut,Blelloch:1996jx} on to flat data-parallel
machines. Because of their fundamental importance to many algorithms,
implementing efficient scan operations in CUDA has received much
attention~\cite{Sengupta:2007tc,Dotsenko:2008fo,Harris:2012fy}.

A basic description (in pictures) of how the scan and segmented scan algorithms
work.


\subsection{Permute}

The \code{permute} collective operation defines a forward permutation $f$ as an
index mapping from one array $X$ onto the result array $Y$, which we can write
as  $f : X \rightarrow Y$. Implementation of the permute function in Accelerate
is complicated because we have not placed any particular restrictions on $f$,
namely:
%
\begin{enumerate}
    \item $f$ is not surjective: the range of $f$ may not cover the codomain
        $Y$. For every $x$ in $X$, $f\left( x \right)$ need not yield every
        index $y$ in $Y$. This means that the result array must first be
        initialised with a set of default values.

    \item $f$ is not injective: distinct elements of the domain may
        map to the same element in the codomain. For all $x$ and $x'$ in $X$, if
        $f\left( x \right) = f\left( x' \right)$, we may have that $x \ne x'$.
        This means we require an associative combination function to combine
        elements from the domain that map to the same index in the codomain.

    \item $f$ is partial: elements of the domain may be ignored, and thus do
        not map to elements in the codomain.
\end{enumerate}

That the permutation function admits partial functions in the index mapping is
not particularly challenges for the implementation, and indeed is useful for
implementing the \code{filter} operation, shown in Listing~\ref{lst:filter}. The
special value \code{ignore} is used to drop elements of the input vector that do
not satisfy the predicate, by not mapping those indices to an index in the
codomain.
%
\begin{lstlisting}[style=haskell
    ,label=lst:filter
    ,caption={[Filter in Accelerate] Filtering returns only those elements of a
    vector which satisfy a predicate. This operation is included as part of
    Accelerate's standard prelude.}]
filter :: Elt a => (Exp a -> Exp Bool) -> Acc (Vector a) -> Acc (Vector a)
filter p vec
  = let flags            = map (boolToInt . p) vec
        (targetIdx, len) = scanl' (+) 0 flags
        defaults         = backpermute (index1 $ the len) id vec
    in
    permute const defaults (\ix -> flags!ix ==* 0 ? (ignore, index1 $ targetIdx!ix)) vec
\end{lstlisting}

On the other hand, because we can not prove that \code{filter} is surjective, we
require the result vector to be first initialised with default values. Since we
do not know anything about the element type \code{a}, the only recourse is to
copy elements from the input array \code{vec}. This is doubly wasteful, because
we must first execute the \code{backpermute} kernel to compute the
\code{defaults} array, and then copy those values into the results vector before
executing the \code{permute} kernel, even though we know the initialised values
will be completely overwritten.

At a second example, Listing~\ref{lst:histogram} demonstrates the computation of
a simple ten bin histogram from a vector of floating point elements in the range
$\left[ 0, 100 \right)$. In this case the permutation function is neither
surjective nor injective, as some bins may contain no elements and thus take the
default value, while other bins may contain multiple elements which need to be
combined (accumulated) correctly.
%
\begin{lstlisting}[style=haskell
    ,label=lst:histogram
    ,caption={[Simple histogram in Accelerate] A simple histogram written in
    Accelerate. We assume the input vector contains elements in the range
    $\left[0,100\right)$ and accumulate into ten equally sized bins.}]
histogram :: Acc (Vector Float) -> Acc (Vector Int)
histogram vec
  = let bins      = 10
        zeros     = fill (constant (Z :. bins)) 0
        ones      = fill (shape vec)            1
    in
    permute (+) zeros (\ix -> index1 (A.floor ((vec ! ix) / P.fromIntegral bins))) ones
\end{lstlisting}

Conceptually speaking, every result of the permutation is applied at the same
time in a single parallel step. If multiple CUDA threads attempt a non-atomic
write to the same memory location at the same time, the writes will be
serialised but the thread which performs the final write is undefined, and so
the final value at that memory slot is undefined~\cite{NVIDIA:2012wf}.

To support non-injective permutation functions, which is required to evaluate
the histogram program correctly, the atomic compare-and-swap operation is used
to implement write combining.\footnote{The atomic compare-and-swap operation on
32-bit values is only available for devices of compute capability 1.1 and
higher, and for 64-bit values on devices of compute capability 1.2 and higher.}
For a combination function \code{f} on elements of type \code{T}, the following
atomically combines a value from the source array \code{x} with the value of the
result array \code{y}.

\begin{lstlisting}[style=cuda]
    T x         = source[i];
    T y, old_y  = result[j];
    do {
        y       = old_y;
        old_y   = atomicCAS(&result[j], y, f x y);
    while (y != old_y);
\end{lstlisting}

Any atomic operation can be implemented in terms of compare-and-swap. Types
consisting of more than one primitive type, such as tuples, apply the procedure
to each component separately. Utilising more specific atomic operations such as
\code{atomicAdd} when available is left to future work.

