%
% Optimising data-parallel programs --> contribution!!?
%

\chapter{Optimisation}
\epigraph{Relax. As usual, I will bore you with the details.}%
{\textsc{---chris lee}}

% \begin{itemize}
%     \item parallel array fusion (AIM)
%     \item existing fusion techniques
%         \begin{itemize}
%             \item 81: wadler deforestation
%             \item 93: gill foldr/build
%             \item 07: coutts stream fusion
%             \item 08: keller repa delayed arrays
%             \item brief mention of other techniques and how they are not
%                 applicable; imperative loops, category-theory stuff
%             \item limitations; why they are not applicable w.r.t. parallelism
%         \end{itemize}
% 
%     \item manipulating richly typed terms
%         \begin{itemize}
%             \item basic outline
%             \item preserve types vis-\`a-vis correctness
%         \end{itemize}
% 
%     \item simultaneous substitution
%     \item typed equality
%         \begin{itemize}
%             \item \url{http://stackoverflow.com/questions/13423961/how-to-derive-eq-for-a-gadt-with-a-non-kinded-phantom-type-parameter/13431026#13431026}
%         \end{itemize}
% 
%     \item shrinking / dead code elimination
% 
%     \item simplifier
%         \begin{itemize}
%             \item ``opportunistic'' CSE
%             \item loop recovery (mandelbrot)
%             \item constant folding, branch pruning
%             \item correctness of constant propagation, algebraic rearrangements
%             \item various examples here of C vs. idiomatic C
%         \end{itemize}
% 
%     \item FUSION
%         \begin{itemize}
%             \item reiterate design requirements, constraints
%             \item explain basic idea
%             \item environment manipulations
%             \item enumerate/describe the special cases
%                 \begin{itemize}
%                     \item zipWith
%                     \item slice, reshape (lost bounds checks)
%                     \item combining / floating lets
%                     \item consumers
%                 \end{itemize}
%             \item fusion rules as inferences
%         \end{itemize}
% 
%     \item correctness of the transform
% 
% \end{itemize}

The previous chapter discussed the architecture of the Accelerate language
embedding and execution on CUDA hardware. Through a set of benchmarks, we
identify the two most pressing performance limitations: operator fusion and data
sharing. This chapter describes the method used to overcome these issues,
focusing primarily on the approach to operator fusion in the context of the
stratified Accelerate language and skeleton-based implementation of the CUDA
backend.


\section{Manipulating Terms}
\label{sec:manipulating_terms}

The Accelerate core language is richly typed, maintaining full type information
of the embedded language in the term tree. In order to apply transformations to
these terms while maintaining the correctness of the program as encoded in its
type, we require methods to manipulate these richly typed terms in a type- and
scope-preserving manner. This section describes the basic operations required
for manipulating terms, upon which the optimisation algorithms are built.

% \subsubsection{Preserving Types vis-\`a-vis Correctness}

\subsection{Equality}

If we want to test whether two terms are equal, for example to determine if two
subexpressions are equivalent and can be shared (\S\ref{sec:cse}), we
immediately run into the problem that terms are existentially typed:
%
\begin{lstlisting}[style=haskell]
Exp s == Exp t = ??
\end{lstlisting}
%
There is no reason that \texttt{s} and \texttt{t} should be expressions of the
same type. In some instances we might not care, and as such can define standard
heterogeneous equality:
%
\begin{lstlisting}[style=haskell]
instance Eq (OpenExp env aenv t) where
    (==) = heq
    where
        heq :: OpenExp env aenv a -> OpenExp env aenv b -> Bool
        heq = ...
\end{lstlisting}
%
where we do not care about the result type of each expression, and only require
that the environment type, re size, of free scalar and array variables are the
same so that we can test equality of variable indices.

However, we often \emph{do} care about the specific types of our existentially
quantified terms. Consider the case of defining equality for \texttt{Let}
bindings. Recall:
%
\begin{lstlisting}[style=haskell]
data PreOpenExp acc env aenv t where

  -- Local binding of a scalar expression
  Let   :: (Elt bnd_t, Elt body_t)
        => PreOpenExp acc env          aenv bnd_t       -- bound term
        -> PreOpenExp acc (env, bnd_t) aenv body_t      -- body/scope of binding
        -> PreOpenExp acc env          aenv body_t
\end{lstlisting}
%
To apply \texttt{heq} to the body expression, we need to know something about
the type of the bound terms to ensure that the scalar environments are the same,
namely \lstinline[mathescape]{a $\sim$ bnd_t $\sim$ b}.\footnotemark\ To do
this we must equip terms with a runtime witness to the existentially quantified
type. Our reified dictionaries will allow us to do exactly this, so we can
define heterogeneous equality for reified dictionaries:
%
\footnotetext{Alternatively we can use \texttt{gcast} to provide a type-safe
cast, but this quickly becomes unwieldy and is a little unsatisfactory.}
%
\begin{lstlisting}[style=haskell]
heqIntegralType :: IntegralType s -> IntegralType t -> Bool
heqIntegralType (TypeInt _)  (TypeInt _)  = True
heqIntegralType (TypeWord _) (TypeWord _) = True
  ...
heqIntegralType _            _            = False
\end{lstlisting}
%
However, doing this is perfectly useless as it only gives us a value of type
\texttt{Bool}, with no idea what that value means or to what its truth might
entitle us. The type checker does not gain any useful knowledge about the types
the dictionaries \texttt{a} and \texttt{b} witness simply by knowing that
\lstinline{heqIntegralType a b = True}. A boolean is a bit uninformative.

Instead, we can write essentially the same test, but in the positive case
deliver some \emph{evidence} that the types are equal:
%
\begin{lstlisting}[style=haskell]
data s :=: t where
  REFL :: s :=: s

matchIntegralType :: IntegralType s -> IntegralType t -> Maybe (s :=: t)
matchIntegralType (TypeInt _)  (TypeInt _)  = Just REFL
matchIntegralType (TypeWord _) (TypeWord _) = Just REFL
  ...
matchIntegralType _            _            = Nothing
\end{lstlisting}
%
Matching on \lstinline{Just REFL} will inject the knowledge into the type
checker that the types \texttt{s} and \texttt{t} are the same. Now with our
evidence-producing heterogeneous equality test for reified dictionary families,
we can compare two terms and gain type-level knowledge when they witness the
same type-level values. These witnesses for existentially quantified types then
allow us to test for equality \emph{homogeneously}, ensuring that positive
results from singleton tests give the bonus of unifying types for other tests:
%
\begin{lstlisting}[style=haskell]
matchOpenExp :: OpenExp env aenv s -> OpenExp env aenv t -> Maybe (s :=: t)
matchOpenExp (Let x1 e1) (Let x2 e2)
  | Just REFL <- matchOpenExp x1 x2     -- if the bound expressions match
  , Just REFL <- matchOpenExp e1 e2     -- then the environment of the body term will also match
  = Just REFL

matchOpenExp ...
\end{lstlisting}

\subsubsection{Congruence}

As we are interested in the facility of matching terms for the purposes of
optimisations such as common subexpression elimination, it is beneficial to
define not equality but \emph{congruence} of terms. Two nodes are congruent if:
%
\begin{itemize}
    \item The nodes label constants and the constants are equal; or
    \item They have the same operator and the nodes are congruent.
\end{itemize}
%
The crux of the latter point refers to commutative relations, such as scalar
addition, where the arguments to the operator yield the same result when the
order of the arguments is reversed. When checking equality of primitive
applications, we discriminate binary functions whose arguments commute, and
return those arguments in a stable ordering by comparing a hash of each of the
sub-terms.
%
% TLM: maybe elide this snippet.
%
\begin{lstlisting}[style=haskell]
commutes :: PrimFun (a -> r) -> OpenExp env aenv a -> Maybe (OpenExp env aenv a)
commutes f x = case f of
  PrimAdd _     -> Just (swizzle x)
    ...         -> Nothing
  where
    swizzle :: OpenExp env aenv (a,a) -> OpenExp env aenv (a,a)
    swizzle exp@(Tuple (NilTup `SnocTup` a `SnocTup` b))
      | hashOpenExp a > hashOpenExp b   = Tuple (NilTup `SnocTup` b `SnocTup` a)
      | otherwise                       = exp
\end{lstlisting}
%
% where ordering is determined by applying a hash function to the term tree.


\subsection{Simultaneous Substitution}
\label{sec:substitution}

In order to do things like renaming and substitution we require a value-level
substitution algorithm for the richly typed terms. The implementation follows
the method of \citet{McBride:2006,McBride:2005}, where it is seen that renaming
and substitution are both instances of a \emph{single} traversal operation,
pushing functions from variables to ``stuff'' through terms, for a suitable
notion of stuff.

The trick is to push a \emph{type-preserving} but \emph{environment changing}
operation structurally through terms:
%
\begin{lstlisting}[style=haskell]
v :: forall t'. Idx env t' -> f env' t'
\end{lstlisting}
%
Where the operation differs is in the image of variables: renaming maps
variables to variables and substitution maps variables to terms. We then lift
this to an operation which traverses terms, with appropriate lifting to push
under a lambda, and rebuilding the term after applying \texttt{v} to the
variables:
%
\begin{lstlisting}[style=haskell,mathescape]
rebuild :: Syntactic f
        => (forall t'. Idx env t' -> f env' t')
        -> Term env  t
        -> Term env' t
\end{lstlisting}

The \texttt{Syntactic} class\footnote{Since Accelerate is a stratified language
there are separate implementations of this toolkit on syntactic elements for the
scalar and collective operations, but the details are identical. The discussion
uses the generic \texttt{Term} to mean either of these operations.} tells us
everything we need to know about \texttt{f} --- our notion of ``stuff'' --- in
order to rebuild terms: a mapping in from variables, a mapping out to terms, and
a weakening map which extends the context. Renaming will instantiate \texttt{f}
with \texttt{Idx}, whereas for substitutions we may choose \texttt{Term}
instead.
%
\begin{lstlisting}[style=haskell]
class Syntactic f where
    varIn   :: Idx env t -> f env t             -- variables embed in f
    termOut :: f env t   -> Term env t          -- f embeds in terms
    weaken  :: f env t   -> f (env, s) t        -- f allows weakening

instance Syntactic Idx
instance Syntactic Term
\end{lstlisting}

A key component of this toolkit, \indexe{weakening} describes an operation we
usually take for granted: every time we learn a new word, old sentences still
make sense; if a conclusion is justified by a hypothesis, it is still justified
if we add more hypotheses; a term remains in scope if we bind new (fresh)
variables. Weakening is the process of shifting things from one scope to a
larger scope in which new things have become meaningful, but no old things have
vanished. When we use a named representation (or HOAS\index{HOAS}) we get
weakening for free, but in the de Bruijn\index{de Bruijn} representation
weakening takes work: we need to shift all the variable references to make room
for new bindings. To do this we need to explain how to shift the \texttt{v}
operation into an extended environment; saying what to do with the new variable
and how to account for it on the output side.
%
\begin{lstlisting}[style=haskell]
shift :: Syntactic f
      => (forall t'. Idx env t' -> f env' t')
      -> Idx (env,  s) t
      -> f   (env', s) t
\end{lstlisting}
%
Overall, the crucial functionality of simultaneous substitution is to propagate
a class of operations on variables closed under shifting, which is what
\texttt{Syntactic} and \texttt{rebuild} offer.

The question, then, is how to write the function \texttt{v} which we push
through terms? This is the final piece of equipment that essentially takes our
simultaneous substitution algorithm and specialises it to one-at-a-time
substitution. Let's say we have a term with one free variable, and we want to
replace that variable with a term. The following takes a replacement for the top
variable and yields a simultaneous substitution that eliminates
\texttt{ZeroIdx}:
%
\begin{lstlisting}[style=haskell,caption={A simultaneous substitution to inline terms}]
inline :: Term (env, s) t -> Term env s -> Term env t
inline body bnd = rebuild (subTop bnd) body
  where
    subTop :: Term env s -> Idx (env, s) t -> Term env t
    subTop s ZeroIdx      = s
    subTop _ (SuccIdx ix) = Var ix
\end{lstlisting}
%
The demonic $\forall$ --- which is to say that the quantifier is in a position
which gives us obligation, not opportunity --- of the operator \texttt{v} forces
us to respect type: when pattern matching detects the variable we care about,
happily we discover that it has the type we must respect. The demon is not so
free to mess with us as one might at first fear.

% Typing rules; each rule types the general usage of a symbol, below the
% line, in terms of the parameters above the line.??


\section{Expression Optimisation}

Now that we have the mechanisms to correctly analyse and manipulate terms, we
can consider optimisations that may be valuable in improving the quality of the
core program. It should be noted that ``optimisation'' is a misnomer; only
rarely does applying optimisations to a program result in object code whose
performance is optimal, by any measure. Rather, the goal is to \emph{improve}
the performance of the object code generated by a backend, although it is
entirely possible that they may decrease it or make no difference at all. As
with many interesting problems in [computer] science, in most cases it is
formally undecidable whether a particular optimisation improves, or at least
does not worsen, performance.

In general, we would like to be as aggressive as possible in improving code, but
not at the expense of making it incorrect. Optimisations are applied repeatedly
until no further changes can me made, or an iteration limit is reached.

% \begin{itemize}
%     \item{Importance of Individual Optimisations}
%     \item{Order and Repetition of Optimisations}
% \end{itemize}

\subsection{Simplification}

Functional language compilers often perform optimisations. To avoid speculative
optimisations that can blow up code size, we might wish to use only
optimisations guaranteed to make the program smaller: these include
dead-variable elimination, constant folding, and a restricted $\beta$-reduction
rule that only inlines functions that are called just once. This leads to a
simplification system guaranteed not to lead to code blowup or nonterminating
compilation.


\subsubsection{Shrinking}

The \emph{shrinking reduction} arises as a restriction of $\beta$-reduction
(i.e. inlining) to cases where the bound variable is used zero (dead-code
elimination) or one (linear inlining) times. As well as reducing binding
overhead, the shrinking reduction exposes opportunities for further optimisation
such as more aggressive inlining, constant folding and common sub-expression
elimination.

The difficulty with implementing the shrinking reduction is that dead-code
elimination at one redex can expose further shrinking reductions at completely
different portions of the term, so attempts at writing a straightforward
compositional algorithm fail. The current implementation uses a na\"ive
algorithm that re-traverses the whole reduct whenever a redex is reduced,
although more efficient imperative algorithms exist
\cite{Appel:1997gs,Benton:2004ua,Kennedy:2007cb}.

The only interesting case of the \texttt{shrink} function is that of a
$\beta$-redex where the number of uses is less than or equal to one. The
implementation, outlined below, uses the de Bruijn manipulation techniques
developed in \autoref{sec:manipulating_terms}. This also suffices to eliminate
dead code, as Accelerate does not have separate forms for \texttt{let}-bindings
and lambda abstractions.
%
\begin{lstlisting}[style=Haskell,caption={The shrinking reduction}]
usesOf :: Idx env s -> OpenExp env aenv t -> Int
usesOf idx (Var idx')
    | Just REFL <- matchIdx idx idx'    = 1
    | otherwise                         = 0
usesOf idx (Let bnd body)               = usesOf idx bnd + usesOf (SuccIdx idx) body
usesOf ...

shrink :: OpenExp env aenv t -> OpenExp env aenv t
shrink (Let bnd body)
    | usesOf ZeroIdx bnd' <= 1          = shrink (inline body' bnd')
    | otherwise                         = Let bnd' body'
    where
        bnd'  = shrink bnd
        body' = shrink body
shrink ...
\end{lstlisting}


\subsubsection{Constant-Expression Evaluation}

\emph{Constant-expression evaluation}, or \emph{constant folding}, refers to the
evaluation at compile time of expressions whose operands are known to be
constant. Essentially, constant-expression evaluation involves determining that
all the operands in an expression are constant valued and performing the
evaluation of the expression at compile time, replacing the expression by this
result.

The applicability of the transformation depends on the type of the expression
under consideration. For Boolean values, the optimisation is always applicable.
For integers, it is almost always applicable, with the exceptions being cases
that would produce run-time exceptions if they were executed, such as division
by zero and underflow or overflow. For floating-point values, the situation is
even more complicated; the compiler's floating point arithmetic must match that
of the processor being compiled for, otherwise floating-point operations may
produce different results. Furthermore, the IEEE-754 standard specifies many
types of exceptions and exceptional values --- including infinities, NaNs and
denormalised values --- all of which should be taken into account.


\paragraph{Constant Folding}

The current implementation performs constant folding of scalar expressions for
all primitive functions and all element types. However, no explicit checks for
under/overflows are made, nor for invalid or exceptional values. For example,
the following rewrite is always applied:
%
% As Accelerate programs are just-in-time compiled and thus the optimisation is
% applied at program runtime, I consider these checks to be of lower priority than
% they would be in a traditional offline compiler.
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape]
$\mathbf{\langle constant\ folding\rangle}$
    PrimAdd ($\mathrm{\langle elided\ type\ info \rangle}$)
      `PrimApp`
      Tuple (NilTup `SnocTup` Const x `SnocTup` Const y) $\mapsto$ Const (x+y)
\end{lstlisting}
%
The attentive reader will note that it is straightforward to choose a positive
non-zero floating-point value \texttt{eps} such that \lstinline{eps + 1.0 > 1.0}
is \texttt{False}. Issues arising from simplification of floating-point
expressions are currently ignored.

% Constant folding is also used to eliminate branches when the value of the
% predicate can be determined.


\paragraph{Constant Propagation}

The effectiveness of constant folding can be increased by combining it with
other data-flow optimisations, particularly constant propagation. When
determining whether the arguments to a primitive function application are
constants, we consider \texttt{let}-bound constants and constant-valued tuple
components in addition to literal constants as material for constant folding.


\paragraph{Algebraic Simplification}

\emph{Algebraic simplifications} use algebraic properties of operators, or
particular operator/operand combinations to simplify expressions. The most
obvious algebraic simplifications involve combining a binary operator with an
operand that is the algebraic identity of that operator, or with an operand that
always yields a constant, independent of the other value of the operand. For
example, for a term \texttt{x} the following are always true:
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape]
$\mathbf{\langle algebraic\ simplification \rangle}$
    x + 0 = 0 + x = x - 0 = x
    0 - x = -x
    x * 1 = 1 * x = x / 1 = x
    x * 0 = 0 * x = 0
\end{lstlisting}
%
Similarly, there are simplifications that apply to unary operators and
combinations of unary and binary operators. Some operations can also be viewed
as strength reductions, that is, replacing an operator by one that is faster to
compute, such as:
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape]
$\mathbf{\langle strength\ reduction \rangle}$
    x $\uparrow$ 2 = x * x
    2 * x = x + x
\end{lstlisting}
%
Likewise, multiplications by small constants can frequently be computed faster
by sequences of shifts and adds and/or subtractions. These techniques are often
more effective if applied during code generation rather than optimisation, so
strength reductions are not currently applied.


\paragraph{Algebraic Reassociation}

\emph{Reassociation} refers to using specific algebraic properties --- namely
associativity, commutativity and distributivity --- to divide an expression into
parts that are constant and variable.
%To further increase opportunities for constant folding, algebraic reassociations
%are applied.
This is particularly relevant when only one operand of a binary
operator is identified as being constant valued. For example, the expression
\lstinline{x + 1 + 2} would only be simplified if the second addition occurred
first. For the case of commutative binary operators where only one operand is
constant valued, the constant expression is applied as the first operand. The
previous term would then be rewritten as:
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape]
$\mathbf{\langle algebraic\ reassociation \rangle}$
    x + 1 + 2 $\mapsto$ 1 + x + 2
              $\mapsto$ 1 + 2 + x
              $\mapsto$ 3 + x
\end{lstlisting}


\paragraph{Summary}

Constant folding and algebraic simplifications are applied to scalar and array
term. The list of algebraic simplifications presented here and currently
implemented is not necessarily exhaustive, and adding additional simplifications
might be eased through the use of a rewrite rule system expressed in terms of
the source language, rather than by direct manipulation of de Bruijn terms.
Furthermore, it may be important to assess the validity of constant folding
based on the type of the expression, particularly for floating point
terms. Both these concerns are left for future work. An example of the current
implementation of constant folding is shown below.

\begin{lstlisting}[style=Haskell,mathescape,caption={Example of constant expression evaluation}]
f :: Exp Float -> Exp Float
f x = let a = lift (constant 30, x)
          b = 9 - fst a / 5
          c = b * b * 4
          d = c >* pi + 10 ? (c - 15, x)
      in x * d * (60 / fst a)
    $\mapsto$
    42.0 * x
\end{lstlisting}


\subsection{Redundancy Elimination}

The optimisations in this section deal with the elimination of redundant
computations. These procedures almost always improve the performance of the code
they are applied to.

\subsubsection{Sharing Observation}

As described in section~\derp, operations of the embedded language do not
directly issue computations; instead, they build term trees that represent
the embedded computation. These term trees use \indexe{higher-order abstract
syntax} (HOAS)\index{HOAS|see{higher-order abstract syntax}}
to embed function-valued scalar expressions as well as typeclass overloading to
reflect arithmetic expressions.

The HOAS representation, while convenient for the human reader, is awkward for
program transformations as it complicates looking under lambdas --- i.e.,
inspecting and manipulating the bodies of function abstractions. The frontend
converts HOAS terms into a \emph{nameless de Bruijn representation} \index{de
Bruijn} in the style of \citet{Altenkirch:2003kz}, using GADTs \cite{Jones:2006}
and type families \cite{Chakravarty:2005a,Schrijvers:2008} to preserve the
embedded program's type information. The type-preserving HOAS to de Bruijn
conversion is described elsewhere \cite{Atkey:2009,Chakravarty:2009uo}.

Together with the conversion to a nameless representation, we recover the
sharing introduced by \texttt{let}-bound subterms in the embedded program. For
example, consider:
%
\begin{lstlisting}[style=Haskell]
let ys = map f xs
in  zipWith g ys ys
\end{lstlisting}
%
If we do not take care, the expression will be inefficiently translated as:
%
\begin{lstlisting}[style=Haskell]
zipWith g (map f xs) (map f xs)
\end{lstlisting}
%
To recover sharing, we user a variant of Gill's technique \cite{Gill:2009}; in
contrast to this work, we preserve types and produce a nested term with minimal
flattening, instead of a graph. Sharing recovery proceeds in two phases:

\paragraph{Phase 1: Build the occurrence map}

A top-down traversal of the AST that computes a map from AST nodes to the number
of occurrences of that node in the overall program; an occurrence count of two
or more indicates sharing. During computation of the occurrences, the tree is
annotated with the \indexe{stable name} \cite{Jones:2000} of that node, and all
but the first occurrence of shared subtrees are pruned and replaced with
variable bindings. Note that to avoid completely unfolding the embedded
expression, we do not descend into subterms that have been encountered
previously. Hence the complexity of sharing observation is proportional to the
number of nodes in the tree \emph{with} sharing.

As it is based on stable names, this phase of the algorithm is impure and,
strictly speaking, because of this it is not deterministic. The stable name
API does not guarantee completeness: for two stable names \lstinline{sn1} and
\lstinline{sn2}, if \lstinline{sn1 == sn2} then the two stable names were
created from the same object. However, the reverse is not necessarily true; if
the two stable names are not equal the objects they come from may still be
equal. This means that we may fail to discover some sharing, but sharing does
not affect the denotational meaning of the program, and hence we do not
compromise denotational correctness.


\paragraph{Phase 2: Determine scope and inject sharing information}

A bottom-up traversal that determines the scope for every binding to be
introduced to share a subterm. It uses the occurrence map to determine, for
every shared subtree, the meet of all the shared subtree occurrences --- the
lowest AST node at which the binding for the subterm can be placed.

\paragraph{Phase 3: Convert to de Bruijn form}

Finally, the AST with environment layout and sharing information can be
converted into de Bruijn form while incorporating sharing by introducing
appropriate \texttt{let} bindings.

A completed treatment of the sharing recovery algorithm appears in \derp.


\subsubsection{Common-Subexpression Elimination}
\label{sec:cse}

\emph{Common-subexpression elimination} finds computations that are performed at
least twice on a given execution path and eliminates the second and later
occurrences, replacing them with uses of saved values. The current
implementation performs a simplified version of common subexpression, where we
look for expressions of the form:
%
\begin{lstlisting}[style=Haskell,mathescape,numbers=none]
$\mathbf{\langle common\ subexpression\ elimination\rangle}$ let x = e1 in [x/e1]e2
\end{lstlisting}
%
and replace all occurrences of \texttt{e1} in \texttt{e2} with \texttt{x}. This
is not complete full redundancy analysis, but is good enough to catch some
cases.\footnote{\url{http://hackage.haskell.org/trac/ghc/ticket/701}}

While it may seem that common-subexpression elimination is always worthwhile, as
it reduces the number of arithmetic operations performed, this is not
necessarily advantageous. The simplest case is which it may not be desirable is
if it causes a register to be occupied for a long time in order to hold the
shared expression's value, and hence reduces the number of registers available
for other uses. Even worse is if the value has to be spilled to memory because
there are insufficient registers available. We sidestep this tricky and
target-dependent issue by, for now, simply ignoring it.


% \subsubsection{Loop-Invariant Code Motion}


% 
% \subsection{Beta reduction}
% 
% Beta reduction captures the idea of function application, and is defined in
% terms of substitution of terms:
% %
% \begin{lstlisting}[style=haskell,numbers=none,mathescape]
% $\mathbf{\langle \beta-reduction \rangle}$ (\x -> body) y $\mapsto$ body[y/x]
% \end{lstlisting}
% %
% Note that since there is no requirement that \texttt{y} is an atom (variable
% or literal), the replacement implies duplicated work if \texttt{x} occurs
% multiple times in the body.
% % To avoid this, the transformation can be applied in stages, first
% % \texttt{let}-binding the argument expression.
% Beta reduction is particularly effective in exposing other transformations.
% 
% 
% \subsection{\texttt{let} elimination}
% \subsubsection{Inlining}
% \label{sec:inlining}
% 
% Inlining replaces occurrences of \texttt{let}-bound variables by its binding
% expression, to define the binding expression to local context information and
% thus increase the possibility of other transformations being applied.
% 
% 
% \subsubsection{Dead code removal}
% \subsubsection{\texttt{let} floating}
% \subsubsection{Loop recovery}
% 
% \subsection{Constant folding}
% \subsubsection{Branch elimination}
% \subsubsection{Tuple elimination}
% 
% \section{Global Transformations}
% \subsection{Producers}
% \subsection{Consumers}

\section{Introduction to Fusion}

Fusion or deforestation is a term used to describe techniques for having a
compiler automatically eliminate intermediate data structures in a computation.
For example, to compute the sum of squares of all integers from one to a given
number in Haskell~\cite{Haskell:1998}, I could write:
%
\begin{lstlisting}[style=haskell]
sum_of_squares :: Int -> Int
sum_of_squares n
    = sum                       -- add all numbers in the list
    $ map (\x -> x * x)         -- traverse list doubling each element
    $ enumFromTo 1 n            -- generate list of numbers [1..n]
\end{lstlisting}
%
but while the meaning of the program is clear, it is inefficient, as this code
produces two intermediate lists of numbers which requires $O(n)$ space and
associated memory pressure to manipulate. Instead, one could write the program
as a single tail-recursive loop as such:
%
\begin{lstlisting}[style=haskell]
sum_of_squares :: Int -> Int
sum_of_squares n = go 1 0
  where
    go i acc | i > n     = acc                   -- return final tally
             | otherwise = go (i+1) (acc + i*i)  -- add to accumulator and step to next element
\end{lstlisting}
%
The second program is much more efficient than the first, because it does not
involve the production of any intermediate lists and executes is constant space.
Unfortunately, the clarity of the original program has been lost. What we
\emph{really} want is to write the first program, and have the compiler
\emph{automatically} transform it into the second, or something morally
equivalent.

This example also demonstrates a subtle behavioural tendency of optimising
program transformations: while the second (target) program does not produce any
intermediate data structures as desired, we can no longer interpret the program
as a sequence of combinators. This observation is critical if the combinators
represent \emph{collective} operations as they do in Accelerate: while the
second program compiles to an efficient scalar loop, its \emph{parallel}
interpretation has been lost.


\subsection{Related Work}

The desire to program in a functional style while still achieving the
performance level of an imperative language has been around for as long as
functional programming itself. This section briefly summarises some of the key
milestones in the history of deforestation as a compiler optimisation that
influences this work.

\subsubsection{Deforestation}

% blugh blurgh
Inspired by earlier work in \texttt{fold/unfold} methods (for example
\citet{Burstall:1977kl}), Philip Wadler set out to develop a technique that
would \emph{automatically} remove intermediate structures from functional
programs, a technique he (later) dubbed \indexe{deforestation}
\cite{Wadler:1981hy,Wadler:1990ix}.

The method identifies a core set of list operators that can be used to express a
large class of computations: \texttt{map}, \texttt{reduce} and
\texttt{generate} (the latter being similar in spirit to \texttt{foldr} and
\texttt{unfoldr} respectively), together with a collection of rewrite rules that
apply to combinations of these operators. These rules identify specific patterns
of list operators that create then consume intermediate lists, then merges these
operations to avoid the intermediary.

\paragraph{Advantages}
\begin{itemize}
    \item Automatic: does not require programmer input, and therefore suitable
        for integration to a compiler.

    \item Source-to-source: all transformations take valid programs in the
        source language and output (faster) valid programs in the source
        language. This makes the output of deforestation easy to integrate as
        part of a compiler pipeline.

    \item Simple: each rule is easy to understand and obviously correct.
\end{itemize}

\paragraph{Disadvantages}
\begin{itemize}
    \item Each optimisation requires a separate transformation rule. If we want
        to add new list operators, we need to add a new rule for each
        combination of the new operator with each existing operator. This does
        not scale.

    \item Limited: many computations can not be expressed in terms of these
        three operations, so the framework can not eliminate intermediate lists
        in these case. \marginnote{check this, c.f. adv of foldr/build}
\end{itemize}


\subsubsection{\texttt{foldr/build}}

Building upon the original deforestation algorithm, Andy
Gill's doctoral thesis introduces another approach to deforestation known as
\indexe{foldr/build fusion} \cite{Gill:1996tf,Gill:1993}.
% This techniques performs a range of optimisations as broad as previous
% approaches the listless transformer \cite{Wadler:1984ia,Wadler:2005iw} while
% addressing their drawbacks.
Like the deforestation algorithm, \texttt{foldr/build} fusion is a rule-based
source-to-source transformation. In fact, it is based on a single rule:
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape,caption={The \texttt{foldr/build} transformation}]
$\mathbf{\langle foldr/build\ fusion\rangle}$ forall g k z. foldr k z (build g) $\mapsto$ g k z

build :: (forall b. (a -> b -> b) -> b -> b) -> [a]
foldr :: (a -> b -> b) -> b -> [a] -> b
\end{lstlisting}

The idea is to think of \texttt{build} as a function that constructs a list, and
\texttt{foldr} as a list consumer. Because of this uniform treatment of lists as
\emph{complete objects}, rather than being composed of many individual
constructor cells, we can have a single \texttt{foldr/build} rule, which is
analogous to standard case reduction rules, but instead of removing a single
constructor removes the \emph{entire data structure}. This method became the
first non-trivial deforestation system to be included as an active part of a
production quality functional language compiler.

\paragraph{Advantages}
\begin{itemize}
    \item Simple, automatic, source-to-source transformation.

%    \item No limitation on inputs: \texttt{foldr/build} fusion operates over the
%        entire Haskell language. When a list is produced without using
%        \texttt{build} or consumed without using \texttt{foldr}, the
%        deforestation scheme is not hampered and simply leaves the list intact.

    \item Single rule: unlike the original deforestation algorithm, a single
        rule suffices. When new list operations are added, so long as they cane
        be expressed in terms of \texttt{foldr} and \texttt{build}, the
        transformation will apply; there is no need to consider all combinations
        with the existing operations.
\end{itemize}

\paragraph{Disadvantages}
\begin{itemize}
    \item The transformation can not handle functions that use accumulators,
        such as \texttt{foldl}, or consume multiple inputs, such as
        \texttt{zip}.

    \item Operations that do not produce lists using \texttt{build} or consume
        them using \texttt{foldr} will not have the intermediate list
        eliminated.
\end{itemize}


\subsubsection{Stream Fusion}

Following Gill's work, a whole cottage industry of new deforestation approaches
appeared with the goal of covering the cases left out by \texttt{foldr/build}.
Most of that work was theoretical, a lot of it was based on category theory, and
almost none of it had any impact in practice. Unlike those systems of the prior
two decades, \indexe{stream fusion} \cite{Coutts:2007} succeeded in this goal and
at the same time stayed simple and useful in practice.

Stream fusion has a similar structure to \texttt{foldr/build}: a single rule
eliminates matching pairs of constructor and destructor, and operations need to
be expressed in terms of these constructors:
%
\begin{lstlisting}[style=Haskell,numbers=none,mathescape,caption={The \emph{stream fusion} transformation}]
$\mathbf{\langle stream/unstream\ fusion\rangle}$ forall s. stream (unstream s) $\mapsto$ s

stream   :: [a] -> Stream a
unstream :: Stream a -> [a]

data Stream a = exists s. Stream (s -> Step a s) s
data Step a s = Done
              | Yield a s           -- produce a value and new state
              | Skip s              -- update the state without producing a value
\end{lstlisting}

Streams have some abstract state and a \emph{stepper} function that advances the
stream from one state to the next, possibly yielding a new value. The
\texttt{stream} constructor \texttt{Yield}s values from its input until it runs
out, and the \texttt{unstream} destructor build a list by consuming its input
stream until encountering \texttt{Done}, discarding \texttt{Skip}s along the
way. Applying the rule to keep the computation in the non-recursive
\texttt{Stream} formulation allows the compiler to aggressively apply standard
optimisation techniques.

\paragraph{Advantages}
\begin{itemize}
    \item Preserves the nice properties of \texttt{foldr/build} fusion.
    \item Adds supports operations with accumulators and multiple inputs.
\end{itemize}

% \paragraph{Disadvantages}
%
% \begin{itemize}
%     \item None, really\ldots? Requires a highly optimising compiler?
% \end{itemize}


\subsubsection{Delayed Arrays}

The previous fusion transformations are based on the idea of expressing
computations in terms of a builder and consumer function, and then having the
compiler identify and remove adjacent constructor/destructor pairs. In contrast,
the Repa \cite{Keller:2010} library uses a functional representation of
\indexe{delayed arrays} that instead \emph{avoids} creating unnecessary
intermediate structures, rather than relying on a subsequent fusion
transformation to remove them.

\begin{lstlisting}[style=Haskell,numbers=none,mathescape,caption={Repa-1 style array definition}]
data Array sh e = Manifest sh (Vector e)    -- unboxed data
                | Delayed  sh (sh -> e)     -- array shape and indexing function
\end{lstlisting}

The key principle of the design is to avoid generating an explicit
(\texttt{Manifest}) representation of intermediate arrays that can instead be
represented as the original array combined with an index-space transformation
function.

% Repa retains the functional programming style by exposing an interface of
% collective operations over arrays --- such as maps, folds, and permutations ---
% instead of the imperative style of reading and writing individual array
% elements.

\paragraph{Advantages}
\begin{itemize}
    \item Explicitly avoids intermediate structures by design, rather than
        relying on potentially fragile post hoc compiler transformations.
\end{itemize}

\paragraph{Disadvantages}
\begin{itemize}
    \item Requires the user to explicitly state when arrays should be computed
        (made manifest), else performance suffers due to redundant computation.
\end{itemize}


\subsection{Aims for Accelerate array fusion}

From this brief survey of existing work, we can state some aims for a fusion
transformation that is to be developed for the Accelerate language.

\begin{enumerate}
    \item The transformation must retain the combinator-based interpretation of
        the program, which is necessary to support execution on parallel
        architectures such as CUDA.

    \item Fuse everything: The transformation should cover all Accelerate
        operations, and not be limited to common special cases such as
        \texttt{map/map} and \texttt{map/fold}.

    \item Source-to-source: The fusion transformation should take valid source
        (or core) programs and return valid programs in the same representation.
        This is important for integration into existing phases of the compilation
        pipeline and multiple-backend architecture. Tangentially, this forces
        adherence to the first requirement.

    \item Have a constant number of rewrite rules: Adding new terms to the core
        language should not require adding new rules for each combination of the
        new operator to the existing operations. Furthermore, adding new
        combinators should be relatively easy to integrate into the fusion
        schema.
\end{enumerate}

We shall return to this list when we discuss the results generated by the
optimisation pass. For now, we turn the discussion towards implementation.



% \subsection{Design}
% 
% basic outline?
% 
% \begin{lstlisting}[style=Haskell,numbers=none,mathescape]
% data DelayedAcc aenv a where
%   Done  :: Arrays a
%         => Extend aenv aenv'
%         -> PreOpenAcc OpenAcc aenv' a   -- a sub-term
%         -> DelayedAcc         aenv  a
% 
%   Step  :: (Elt a, Elt b, Shape sh, Shape sh')
%         => Extend aenv aenv'
%         -> PreExp OpenAcc aenv' sh'
%         -> PreFun OpenAcc aenv' (sh' -> sh)
%         -> PreFun OpenAcc aenv' (a   -> b)
%         -> Idx            aenv' (Array sh  a)
%         -> DelayedAcc     aenv  (Array sh' b)
% 
%   Yield :: (Shape sh, Elt a)
%         => Extend aenv aenv'
%         -> PreExp OpenAcc aenv' sh
%         -> PreFun OpenAcc aenv' (sh -> a)
%         -> DelayedAcc     aenv  (Array sh a)
% \end{lstlisting}

\section{The Transformations Interacting}

This section follows a few examples to discuss the effect of applying the
transformations described in the preceding sections. Many of these motivating
examples have shown up in real applications. The effects usually involve a
combination of many transformations and so give an idea of how the
transformations interact with one another.

\subsection{Repeated Evaluations}
% \subsection{Error tests eliminated}
\subsection{Confluence and Termination}

The set of transformations can be seen as a set of term rewriting rules. We
would like this set of transformations to be:
%
\begin{itemize}
    \item Correct: The transformed code retains the same semantics as the
        original code. This correctness is proved in
        section~\derp.

    \item Improving efficiency: The transformed code should require less time
        and/or fewer resources to execute than the original. We return to this
        topic in the analysis of benchmarks in chapter~\ref{ch:results}.
\end{itemize}
%
In addition, it would be of significant practical advantage if the
transformations were:
%
\begin{itemize}
    \item Confluent: When more than one transformation is applicable to a given
        term, applying the transformations in any ordering should yield the same
        result. This is important to ensure that opportunities to apply
        transformations are not lost, or worse code is generated, due to
        the choice of applying one transformation before another.

    \item Terminating: We reach a point when no transformation is applicable so
        the simplification process stops. One must be careful that one
        transformation [sequence] can not generate terms that can be transformed
        back to the original; that no transformation undoes the work of another.
\end{itemize}

Do some work dis/proving these two things, then write about it here.


\section{Compiling the \derp\ program}

\section{Conclusion}

% \section{Fusion / Global optimisation}
% \textcolor{red}{[insert catchy name]}
% 
% Inference rules for forcing\ldots

% \inference[force 1: ]
% {\Gamma |- \textsc{Done}\ \Gamma'\ a}
% {\Gamma |> \Gamma' |- a}
% 
% \inference[force 2: ]
% {\Gamma |- \textsc{Yield}\ \Gamma'\ sh\ f}
% {\Gamma |> \Gamma' |- \mathrm{Generate}\ sh\ f}
% 
% \inference[force 3a: ]
% {\Gamma |- \textsc{Step}\ \Gamma'\ sh\ ix\ f\ a & sh \equiv \mathrm{shape}\ a &
% \mathrm{identity}\ f & \mathrm{identity}\ ix}
% {\Gamma |> \Gamma' |- \mathrm{Avar}\ a}
% 
% \inference[force 3b: ]
% {\Gamma |- \textsc{Step}\ \Gamma'\ sh\ ix\ f\ a & sh \equiv \mathrm{shape}\ a & \mathrm{identity}\ ix}
% {\Gamma |> \Gamma' |- \mathrm{Map}\ f\ a}
% 
% \inference[force 3c: ]
% {\Gamma |- \textsc{Step}\ \Gamma'\ sh\ ix\ f\ a & \mathrm{identity}\ f}
% {\Gamma |> \Gamma' |- \mathrm{Backpermute}\ sh\ ix\ a}
% 
% \inference[force 3d: ]
% {\Gamma |- \textsc{Step}\ \Gamma'\ sh\ ix\ f\ a}
% {\Gamma |> \Gamma' |- \mathrm{Transform}\ sh\ ix\ f\ a}

% \section{Simplification / Local optimisation}

